{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"dev Textile Cheat Sheet 2015-10-20 Image Resize 2015-10-27 Screenshots Screencasts Animated Gifs 2015-11-01 ffmpeg notes 2015-11-05 Unix-y notes 2017-05-14 lattice reduction 2015-11-30 GCode Conversion 2016-09-19 Git Rename Master 2016-09-21 MkDocs Quickstart 2016-09-28 GPG Notes 2017-02-09 Git Notes 2017-02-10 Shannon Entropy 2017-06-12 Enabling Server HTTPS 2017-06-12 BGZF Example 2017-06-12 C Project Template 2017-08-05 Project Organization 2017-08-05 Kelly Criterion 2017-08-18 File Naming Conventions 2017-08-18 Command Line Option Loose Standard 2017-12-25 PCB Notes 2018-02-03 Coding Style 2018-02-17 Energy Consumption Stats 2017-09-21 Simple Sum 2018-05-25 Fisher Yates Shuffle 2018-06-13 Halting Problem 2018-06-13 Assorted Small Probability Problems 2018-06-29 Probability Notes 2018-08-04 Amdahls Law 2018-09-03 Is It Really Open 2019-01-10 Number Theory Notes 2019-01-10 Arbitrary Binary Functions 2019-01-10 Empirical Laws 2019-03-18 Food CO2 Water 2019-04-02 SSH Recipes 2019-09-24 Future Predictions 2021-04-27 Diophantine Approximation 2020-05-22 GCode Common 2020-09-09 Misc Math 2021-08-30 Socio Economic Definitions 2020-11-02 Bitcoin Moon Math 2017-12-06 Energy Discussion 2021-04-24 AWK Cheatsheet 2021-10-01 Littlewood Polynomials Notes 2022-01-20 Useful Tables 2022-08-09 Belief Propagation 2022-08-16 Probability Defintions 2023-03-24 Ribbon Tile Puzzle (spoilers) 2023-04-02 Kullback-Leibler Divergence 2023-08-22 Statistical Mechanics for Computer Scientists 2023-11-12","title":"Home"},{"location":"index.html#dev","text":"Textile Cheat Sheet 2015-10-20 Image Resize 2015-10-27 Screenshots Screencasts Animated Gifs 2015-11-01 ffmpeg notes 2015-11-05 Unix-y notes 2017-05-14 lattice reduction 2015-11-30 GCode Conversion 2016-09-19 Git Rename Master 2016-09-21 MkDocs Quickstart 2016-09-28 GPG Notes 2017-02-09 Git Notes 2017-02-10 Shannon Entropy 2017-06-12 Enabling Server HTTPS 2017-06-12 BGZF Example 2017-06-12 C Project Template 2017-08-05 Project Organization 2017-08-05 Kelly Criterion 2017-08-18 File Naming Conventions 2017-08-18 Command Line Option Loose Standard 2017-12-25 PCB Notes 2018-02-03 Coding Style 2018-02-17 Energy Consumption Stats 2017-09-21 Simple Sum 2018-05-25 Fisher Yates Shuffle 2018-06-13 Halting Problem 2018-06-13 Assorted Small Probability Problems 2018-06-29 Probability Notes 2018-08-04 Amdahls Law 2018-09-03 Is It Really Open 2019-01-10 Number Theory Notes 2019-01-10 Arbitrary Binary Functions 2019-01-10 Empirical Laws 2019-03-18 Food CO2 Water 2019-04-02 SSH Recipes 2019-09-24 Future Predictions 2021-04-27 Diophantine Approximation 2020-05-22 GCode Common 2020-09-09 Misc Math 2021-08-30 Socio Economic Definitions 2020-11-02 Bitcoin Moon Math 2017-12-06 Energy Discussion 2021-04-24 AWK Cheatsheet 2021-10-01 Littlewood Polynomials Notes 2022-01-20 Useful Tables 2022-08-09 Belief Propagation 2022-08-16 Probability Defintions 2023-03-24 Ribbon Tile Puzzle (spoilers) 2023-04-02 Kullback-Leibler Divergence 2023-08-22 Statistical Mechanics for Computer Scientists 2023-11-12","title":"dev"},{"location":"3SAT-Computer.html","text":"3SAT Computer This is a work in progress. This is a short text on how to make a \"3SAT Computer\". For some reason most people have lost the understanding of the connection between NP-Complete problems and Turing machine decidability. In some very broad sense, NP-Complete problems can be seen to be a finite restatement of the Halting Problem. I believe this was well understood by Stephen Cook and others, which can be seen in the first line of the summary in Cook's original paper: It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be \u201creduced\u201d to the problem of determining whether a given propositional formula is a tautology. The Halting Problem concerns itself with a Turing machine running on an infinite tape and only whether a program will halt in finite time whereas NP-Complete problems concern themselves with whether a Turing machine will halt in polynomial time. By necessity, a Turing machine that runs in polynomial time will have a tape that is finite, though the length might depend on the runtime bound. To construct a \"3SAT Computer\", it suffices to show some basic operations and how they can be encoded into 3SAT. Preliminaries and Notation Single bit variables will be denoted by $x_{k}$. Multi bit variables will be denoted by $x(s) {k} = ($x {s-1,k}, x_{s-2,k}, /cdots, x_{0,k}$, where $s$ is the number of bits. A useful heuristic will be to understand that the clause $(x_0 + x_1 + x_2)$ is false only when $x_0=0$, $x_1=0$ and $x_2=0$. One way to look at the clause is to say that the clause $(x_0+x_1+x_2)$ is only true if the 'forbidden' configuration of $x_0=0$, $x_1=0$ and $x_2=0$ doesn't happen. For an arbitrary boolean function on three variables, we can take the Karnaugh map on false entries and then negate the subsequent expression to get the formula in conjunctive normal form (CNF). For example: $$ \\begin{array} {|cc|c|} \\hline f(x_0,x_1,x_2) & & x_1 \\ & & \\begin{array}{cc} 0 & 1\\end{array} \\ \\hline x_0,x_1 & \\begin{array}{ccc} 0 0 & \\ 0 1 & \\ 1 1 & \\ 1 0 & \\ \\end{array} & \\begin{array}{cc} 1 & 0 \\ 0 & 0 \\ 0 & 0 \\ 0 & 0 \\ \\end{array} \\ \\hline \\end{array} $$ One realization is: $$ [ x_0 + x_1 + x_0^{'} \\cdot x_1 ]^{'} = (x_0^{'})\\cdot(x_1^{'})(x_0 + x_1^{'}) $$ Which can easily be verified to match the original example function. To reduce CNF to 3CNF, we can add auxiliary variables, $y$, to chain the clauses together. $$ \\begin{array}{ll} & (x_0 + x_1 + x_2 + x_3 + \\cdots + x_{n-1}) \\ = & (x_0 + x_1 + y_0) \\cdot (y_0^{'} + x_2 + y_1) \\cdot (y_1^{'} + x_3 + y_2) \\cdots \\ & (y_{n-4}^{'} + x_{n-2} + x_{n-1}) \\end{array} $$ check this and prove Variable Assignment First we start with a simple single bit variable assignment: $$ x_1 = x_0 $$ This corresponds to the truth table: $$ \\begin{array} {|cc|c|} \\hline x_1 = x_0 & & x_1 \\ & & \\begin{array}{cc} 0 & 1\\end{array} \\ \\hline x_0 & \\begin{array}{ccc} 0 & \\ 1 & \\ \\end{array} & \\begin{array}{cc} 1 & 0 \\ 0 & 1 \\ \\end{array} \\ \\hline \\end{array} $$ That is, the expression is only true when both $x_0$ and $x_1$ have the same value. This corresponds to the CNF expression: $$ (x_0^{'} + x_1) \\cdot (x_0 + x_1^{'}) $$ For $s$ bit variables, $x(s)_1 = x(s)_0$, the assignment then becomes: $$ \\begin{array}{l} ( x(s-1)_0^{'} + x(s-1)_1) \\cdot (x(s-1)_0 + x(s-1)_1{'}) \\cdot \\ (x(s-2)_0^{'} + x(s-2)_1) \\cdot (x(s-2)_0 + x(s-2)_1{'}) \\cdots \\ (x(1)_0^{'} + x(1)_1) \\cdot (x(1)_0 + x(1)_1{'}) \\cdot \\ (x(0)_0^{'} + x(0)_1) \\cdot (x(0)_0 + x(0)_1{'}) \\end{array} $$ Addition Consider the two 1-bit variables $x_0$ and $x_1$. To find their sum, without carry, the following truth table represents the operation: $$ \\begin{array} {|cc|c|} \\hline x_a = x_0 + x_1 & & x_a \\ & & \\begin{array}{cc} 0 & 1\\end{array} \\ \\hline x_1 x_0 & \\begin{array}{ccc} 0 0 & \\ 0 1 & \\ 1 1 & \\ 1 0 & \\ \\end{array} & \\begin{array}{cc} 1 & 0 \\ 0 & 1 \\ 1 & 0 \\ 0 & 1 \\ \\end{array} \\ \\hline \\end{array} $$ It's CNF: $$ \\begin{array}{l} (x_0 + x_1 + x_a^{'}) \\cdot \\ (x_0 + x_1^{'} + x_a) \\cdot \\ (x_0^{'} + x_1^{'} + x_a^{'}) \\cdot \\ (x_0^{'} + x_1 + x_a) \\cdot \\ \\end{array} $$ Similarly for the carry: $$ \\begin{array} {|cc|c|} \\hline x_c = \\text{carry}( x_0 + x_1 ) & & x_c \\ & & \\begin{array}{cc} 0 & 1\\end{array} \\ \\hline x_1 x_0 & \\begin{array}{ccc} 0 0 & \\ 0 1 & \\ 1 1 & \\ 1 0 & \\ \\end{array} & \\begin{array}{cc} 1 & 0 \\ 1 & 0 \\ 0 & 1 \\ 1 & 0 \\ \\end{array} \\ \\hline \\end{array} $$ It's CNF: $$ \\begin{array}{l} (x_1 + x_c) \\cdot \\ (x_0^{'} + x_1^{'} + x_c^{'}) \\cdot \\ (x_0^{'} + x_1 + x_c) \\end{array} $$ $s$ bit addition with carry can be performed in a similar fashion: To get something more functional, we can do addition with carry: $$ \\begin{array} {|cc|c|} \\hline y_c = \\text{carry}( x_0 + x_1 + x_c ) \\ y_a = x_0 \\oplus x_1 \\oplus x_c & & y_c, y_a \\ & & \\begin{array}{cc} 0 & 1 & \\end{array} \\ \\hline x_c x_1 x_0 & \\begin{array}{ccc} 0 0 0 & \\ 0 0 1 & \\ 0 1 1 & \\ 0 1 0 & \\ 1 1 0 & \\ 1 1 1 & \\ 1 0 1 & \\ 1 0 0 & \\ \\end{array} & \\begin{array}{cc} 1 1 & 0 0 \\ 1 0 & 0 1 \\ 0 1 & 1 0 \\ 1 0 & 0 1 \\ 0 1 & 1 0 \\\\ 0 0 & 1 1 \\\\ 0 1 & 1 0 \\\\ 1 0 & 0 1 \\\\ \\end{array} \\ \\hline \\end{array} $$ $$ \\begin{array}{l} (x_c + x_1 + x_0 + y_c^{'}) \\cdot (x_c + x_1 + x_0 + y_a^{'}) \\cdot \\ (x_c + x_1 + x_0^{'} + y_c^{'}) \\cdot (x_c + x_1 + x_0^{'} + y_a) \\cdot \\ (x_c + x_1^{'} + x_0^{'} + y_c) \\cdot (x_c + x_1^{'} + x_0^{'} + y_a^{'}) \\cdot \\ (x_c + x_1^{'} + x_0 + y_c^{'}) \\cdot (x_c + x_1^{'} + x_0 + y_a) \\cdot \\ (x_c^{'} + x_1^{'} + x_0 + y_c) \\cdot (x_c^{'} + x_1^{'} + x_0 + y_a^{'}) \\cdot \\ (x_c^{'} + x_1^{'} + x_0^{'} + y_c) \\cdot (x_c^{'} + x_1^{'} + x_0^{'} + y_a) \\cdot \\ (x_c^{'} + x_1 + x_0^{'} + y_c) \\cdot (x_c^{'} + x_1 + x_0^{'} + y_a^{'}) \\cdot \\ (x_c^{'} + x_1 + x_0 + y_c^{'}) \\cdot (x_c^{'} + x_1 + x_0 + y_a) \\cdot \\ \\end{array} $$ We can now construct our $s$ bit addition with $y(s)_a$ as our addition result, without carry, and $y(s+1)_c$ as our auxiliary carry storage: $$ \\begin{array}{c} \\end{array} $$ ... Subtraction can be done in a similar manner but with two's complement. Multiplication Multiplication turns out to be harder as the efficient algorithm is involved. \"Schoolchild\" multiplication is $O(n^2)$ and can be used if need be. Control Flow First we talk about if statements: $$ \\begin{array} {|cc|c|} \\hline \\text{if } (x_d) \\to \\text{expr}(x) & & x_d \\ & & \\begin{array}{cc} 0 & 1\\end{array} \\ \\hline \\text{expr}(x) & \\begin{array}{ccc} 0 & \\ 1 & \\end{array} & \\begin{array}{cc} 1 & 0 \\ 1 & 1 \\ \\end{array} \\ \\hline \\end{array} $$ If $\\text{expr}(x)$ is in CNF, this amounts to adding the negated variable $x_d$ to every clause. $$ (x_d^{'} + \\text{expr}(x)) $$ When $x_d^{'} is 0, the resulting expression is rendered moot as each clause is not set to true from the assignment of $x_d=0$. When $x_d=1$ the only satisfying instance will necessarily have $\\text{expr}(x)$ be true. Bringing it Together Once we have the basics of arithmetic, variable assignment and control flow, we can now construct a language of sorts. for and while loops can be simulated by bounding repetition. A break like construction can be simulated by having an auxiliary variable set to true if some condition is met on the inside loop. Once we have if statements, the goto can effectively be simulated by adding an \"instruction pointer\" variable. We can wrap every unit of code with an if statement that only gets called if the instruction pointer variable is set appropriately. goto s are then effectively an assignment to the instruction pointer. This construction is inefficient in the sense that it adds \"code blocks\" that are duplicated but whose only difference is the instruction pointer but still stays within the polynomial bound on instance size that's needed to keep it within NP. Since we know the basic constructions of variable assignment, basic arithmetic and conditionals are Turing machine equivalent, we have effectively shown that SAT and 3SAT, by the normalization method above, can be used to simulate a polynomial time bounded Turing machine. The reason may have been lost in the details but one of the motivations was to give some justified belief, even if empirical, of why we believe $\\text{NP} \\ne \\text{P}$. Since we know the Halting Problem is undecidable, we might think that the finite restatement the halting problem, in terms of polynomial Turing machine decidability, is also hard. References \"The Complexity of Theorem-Proving Procedures\" by Stephen A. Cook 2020-05-11","title":"3SAT Computer"},{"location":"3SAT-Computer.html#3sat-computer","text":"This is a work in progress. This is a short text on how to make a \"3SAT Computer\". For some reason most people have lost the understanding of the connection between NP-Complete problems and Turing machine decidability. In some very broad sense, NP-Complete problems can be seen to be a finite restatement of the Halting Problem. I believe this was well understood by Stephen Cook and others, which can be seen in the first line of the summary in Cook's original paper: It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be \u201creduced\u201d to the problem of determining whether a given propositional formula is a tautology. The Halting Problem concerns itself with a Turing machine running on an infinite tape and only whether a program will halt in finite time whereas NP-Complete problems concern themselves with whether a Turing machine will halt in polynomial time. By necessity, a Turing machine that runs in polynomial time will have a tape that is finite, though the length might depend on the runtime bound. To construct a \"3SAT Computer\", it suffices to show some basic operations and how they can be encoded into 3SAT.","title":"3SAT Computer"},{"location":"3SAT-Computer.html#preliminaries-and-notation","text":"Single bit variables will be denoted by $x_{k}$. Multi bit variables will be denoted by $x(s) {k} = ($x {s-1,k}, x_{s-2,k}, /cdots, x_{0,k}$, where $s$ is the number of bits. A useful heuristic will be to understand that the clause $(x_0 + x_1 + x_2)$ is false only when $x_0=0$, $x_1=0$ and $x_2=0$. One way to look at the clause is to say that the clause $(x_0+x_1+x_2)$ is only true if the 'forbidden' configuration of $x_0=0$, $x_1=0$ and $x_2=0$ doesn't happen. For an arbitrary boolean function on three variables, we can take the Karnaugh map on false entries and then negate the subsequent expression to get the formula in conjunctive normal form (CNF). For example: $$ \\begin{array} {|cc|c|} \\hline f(x_0,x_1,x_2) & & x_1 \\ & & \\begin{array}{cc} 0 & 1\\end{array} \\ \\hline x_0,x_1 & \\begin{array}{ccc} 0 0 & \\ 0 1 & \\ 1 1 & \\ 1 0 & \\ \\end{array} & \\begin{array}{cc} 1 & 0 \\ 0 & 0 \\ 0 & 0 \\ 0 & 0 \\ \\end{array} \\ \\hline \\end{array} $$ One realization is: $$ [ x_0 + x_1 + x_0^{'} \\cdot x_1 ]^{'} = (x_0^{'})\\cdot(x_1^{'})(x_0 + x_1^{'}) $$ Which can easily be verified to match the original example function. To reduce CNF to 3CNF, we can add auxiliary variables, $y$, to chain the clauses together. $$ \\begin{array}{ll} & (x_0 + x_1 + x_2 + x_3 + \\cdots + x_{n-1}) \\ = & (x_0 + x_1 + y_0) \\cdot (y_0^{'} + x_2 + y_1) \\cdot (y_1^{'} + x_3 + y_2) \\cdots \\ & (y_{n-4}^{'} + x_{n-2} + x_{n-1}) \\end{array} $$ check this and prove","title":"Preliminaries and Notation"},{"location":"3SAT-Computer.html#variable-assignment","text":"First we start with a simple single bit variable assignment: $$ x_1 = x_0 $$ This corresponds to the truth table: $$ \\begin{array} {|cc|c|} \\hline x_1 = x_0 & & x_1 \\ & & \\begin{array}{cc} 0 & 1\\end{array} \\ \\hline x_0 & \\begin{array}{ccc} 0 & \\ 1 & \\ \\end{array} & \\begin{array}{cc} 1 & 0 \\ 0 & 1 \\ \\end{array} \\ \\hline \\end{array} $$ That is, the expression is only true when both $x_0$ and $x_1$ have the same value. This corresponds to the CNF expression: $$ (x_0^{'} + x_1) \\cdot (x_0 + x_1^{'}) $$ For $s$ bit variables, $x(s)_1 = x(s)_0$, the assignment then becomes: $$ \\begin{array}{l} ( x(s-1)_0^{'} + x(s-1)_1) \\cdot (x(s-1)_0 + x(s-1)_1{'}) \\cdot \\ (x(s-2)_0^{'} + x(s-2)_1) \\cdot (x(s-2)_0 + x(s-2)_1{'}) \\cdots \\ (x(1)_0^{'} + x(1)_1) \\cdot (x(1)_0 + x(1)_1{'}) \\cdot \\ (x(0)_0^{'} + x(0)_1) \\cdot (x(0)_0 + x(0)_1{'}) \\end{array} $$","title":"Variable Assignment"},{"location":"3SAT-Computer.html#addition","text":"Consider the two 1-bit variables $x_0$ and $x_1$. To find their sum, without carry, the following truth table represents the operation: $$ \\begin{array} {|cc|c|} \\hline x_a = x_0 + x_1 & & x_a \\ & & \\begin{array}{cc} 0 & 1\\end{array} \\ \\hline x_1 x_0 & \\begin{array}{ccc} 0 0 & \\ 0 1 & \\ 1 1 & \\ 1 0 & \\ \\end{array} & \\begin{array}{cc} 1 & 0 \\ 0 & 1 \\ 1 & 0 \\ 0 & 1 \\ \\end{array} \\ \\hline \\end{array} $$ It's CNF: $$ \\begin{array}{l} (x_0 + x_1 + x_a^{'}) \\cdot \\ (x_0 + x_1^{'} + x_a) \\cdot \\ (x_0^{'} + x_1^{'} + x_a^{'}) \\cdot \\ (x_0^{'} + x_1 + x_a) \\cdot \\ \\end{array} $$ Similarly for the carry: $$ \\begin{array} {|cc|c|} \\hline x_c = \\text{carry}( x_0 + x_1 ) & & x_c \\ & & \\begin{array}{cc} 0 & 1\\end{array} \\ \\hline x_1 x_0 & \\begin{array}{ccc} 0 0 & \\ 0 1 & \\ 1 1 & \\ 1 0 & \\ \\end{array} & \\begin{array}{cc} 1 & 0 \\ 1 & 0 \\ 0 & 1 \\ 1 & 0 \\ \\end{array} \\ \\hline \\end{array} $$ It's CNF: $$ \\begin{array}{l} (x_1 + x_c) \\cdot \\ (x_0^{'} + x_1^{'} + x_c^{'}) \\cdot \\ (x_0^{'} + x_1 + x_c) \\end{array} $$ $s$ bit addition with carry can be performed in a similar fashion: To get something more functional, we can do addition with carry: $$ \\begin{array} {|cc|c|} \\hline y_c = \\text{carry}( x_0 + x_1 + x_c ) \\ y_a = x_0 \\oplus x_1 \\oplus x_c & & y_c, y_a \\ & & \\begin{array}{cc} 0 & 1 & \\end{array} \\ \\hline x_c x_1 x_0 & \\begin{array}{ccc} 0 0 0 & \\ 0 0 1 & \\ 0 1 1 & \\ 0 1 0 & \\ 1 1 0 & \\ 1 1 1 & \\ 1 0 1 & \\ 1 0 0 & \\ \\end{array} & \\begin{array}{cc} 1 1 & 0 0 \\ 1 0 & 0 1 \\ 0 1 & 1 0 \\ 1 0 & 0 1 \\ 0 1 & 1 0 \\\\ 0 0 & 1 1 \\\\ 0 1 & 1 0 \\\\ 1 0 & 0 1 \\\\ \\end{array} \\ \\hline \\end{array} $$ $$ \\begin{array}{l} (x_c + x_1 + x_0 + y_c^{'}) \\cdot (x_c + x_1 + x_0 + y_a^{'}) \\cdot \\ (x_c + x_1 + x_0^{'} + y_c^{'}) \\cdot (x_c + x_1 + x_0^{'} + y_a) \\cdot \\ (x_c + x_1^{'} + x_0^{'} + y_c) \\cdot (x_c + x_1^{'} + x_0^{'} + y_a^{'}) \\cdot \\ (x_c + x_1^{'} + x_0 + y_c^{'}) \\cdot (x_c + x_1^{'} + x_0 + y_a) \\cdot \\ (x_c^{'} + x_1^{'} + x_0 + y_c) \\cdot (x_c^{'} + x_1^{'} + x_0 + y_a^{'}) \\cdot \\ (x_c^{'} + x_1^{'} + x_0^{'} + y_c) \\cdot (x_c^{'} + x_1^{'} + x_0^{'} + y_a) \\cdot \\ (x_c^{'} + x_1 + x_0^{'} + y_c) \\cdot (x_c^{'} + x_1 + x_0^{'} + y_a^{'}) \\cdot \\ (x_c^{'} + x_1 + x_0 + y_c^{'}) \\cdot (x_c^{'} + x_1 + x_0 + y_a) \\cdot \\ \\end{array} $$ We can now construct our $s$ bit addition with $y(s)_a$ as our addition result, without carry, and $y(s+1)_c$ as our auxiliary carry storage: $$ \\begin{array}{c} \\end{array} $$ ... Subtraction can be done in a similar manner but with two's complement.","title":"Addition"},{"location":"3SAT-Computer.html#multiplication","text":"Multiplication turns out to be harder as the efficient algorithm is involved. \"Schoolchild\" multiplication is $O(n^2)$ and can be used if need be.","title":"Multiplication"},{"location":"3SAT-Computer.html#control-flow","text":"First we talk about if statements: $$ \\begin{array} {|cc|c|} \\hline \\text{if } (x_d) \\to \\text{expr}(x) & & x_d \\ & & \\begin{array}{cc} 0 & 1\\end{array} \\ \\hline \\text{expr}(x) & \\begin{array}{ccc} 0 & \\ 1 & \\end{array} & \\begin{array}{cc} 1 & 0 \\ 1 & 1 \\ \\end{array} \\ \\hline \\end{array} $$ If $\\text{expr}(x)$ is in CNF, this amounts to adding the negated variable $x_d$ to every clause. $$ (x_d^{'} + \\text{expr}(x)) $$ When $x_d^{'} is 0, the resulting expression is rendered moot as each clause is not set to true from the assignment of $x_d=0$. When $x_d=1$ the only satisfying instance will necessarily have $\\text{expr}(x)$ be true.","title":"Control Flow"},{"location":"3SAT-Computer.html#bringing-it-together","text":"Once we have the basics of arithmetic, variable assignment and control flow, we can now construct a language of sorts. for and while loops can be simulated by bounding repetition. A break like construction can be simulated by having an auxiliary variable set to true if some condition is met on the inside loop. Once we have if statements, the goto can effectively be simulated by adding an \"instruction pointer\" variable. We can wrap every unit of code with an if statement that only gets called if the instruction pointer variable is set appropriately. goto s are then effectively an assignment to the instruction pointer. This construction is inefficient in the sense that it adds \"code blocks\" that are duplicated but whose only difference is the instruction pointer but still stays within the polynomial bound on instance size that's needed to keep it within NP. Since we know the basic constructions of variable assignment, basic arithmetic and conditionals are Turing machine equivalent, we have effectively shown that SAT and 3SAT, by the normalization method above, can be used to simulate a polynomial time bounded Turing machine. The reason may have been lost in the details but one of the motivations was to give some justified belief, even if empirical, of why we believe $\\text{NP} \\ne \\text{P}$. Since we know the Halting Problem is undecidable, we might think that the finite restatement the halting problem, in terms of polynomial Turing machine decidability, is also hard.","title":"Bringing it Together"},{"location":"3SAT-Computer.html#references","text":"\"The Complexity of Theorem-Proving Procedures\" by Stephen A. Cook","title":"References"},{"location":"3SAT-Computer.html#2020-05-11","text":"","title":"2020-05-11"},{"location":"AWK-Cheatsheet.html","text":"awk Cheatsheet Command Description echo -e \"a b c\\nd e f\" | awk '{ print }' Print each row echo -e \"a b c\\nd e f\" | awk '{ print $0 }' Print each row echo -e \"a b c\\nd e f\" | awk '{ print $1, $2, $3 }' Print each element explicitly for each row echo -e \"a\\tb\\tc\\nd\\te\\tf\" | awk -F '\\t' '{ print $1, $2, $3 }' Print each element explicitly, using tab as a field separator echo -e \"a b c\\nd e f\" | awk '{ print $(NF-2), $(NF-1), $NF }' Print the last three elements in the row echo -e \"a b c\\nd e f\" | awk '{ print NR, NF, $0 }' Print row number, number of columns and the row echo -e \"a b c\\nd e f\" | awk '/ e /{ print NR, $0 }' Match regex and then print the row number and the last three fields echo -e \"a b c\\nd e f\" | awk '$2 == \"e\" { print NR, $0 }' Test to see if the second field is b and print the row number and row echo -e \"a b c\\nd e f\" | awk '$2~/e/{ print $0 }' Test if the second field matches regex and print row echo -e \"a b c\\nd e f\" | awk '{ printf \"ok %s\\n\", $1 }' Use printf to do extra formatting echo -e \"1 2 3\\n4 5 6\" | awk 'BEGIN { print \"begin\" } { tot = tot + $2 } END { print \"end:\", tot, tot/NR }' Calculate the running total and print out the sum and average of the second field ls -la | awk '{ print $1, $2, $3, $4 }' Print out first four columns of ls -la with duplicate separator tokens consolidated into one echo -e \"a\\tb\\tc\\nd\\te\\tf\" | awk 'BEGIN { FS = \"\\t\" } { print NF, $0 }' Change field separator ( FS ) to tab and print each row and their field count echo -e \"a 1\\nb 3\\nc 7\\na 3\" | awk '{ print $1,$2; tot[$1]=tot[$1]+$2; count[$1]=count[$1]+1 } END { for (i in count) { print i, tot[i], count[i] } }' Arrays echo -e \"a 1\\nb 3\\nc 7\\na 3\" | awk '{ print $1,$2; tot[$1]=tot[$1]+$2; count[$1]=count[$1]+1 } END { for (i in count) { if (count[i] > 1) print \">1\", i, tot[i], count[i] ; else if (count[i] < 2) print \"<2\", i, tot[i], count[i] } }' Control flow 2021-10-01","title":"AWK Cheatsheet"},{"location":"AWK-Cheatsheet.html#awk-cheatsheet","text":"Command Description echo -e \"a b c\\nd e f\" | awk '{ print }' Print each row echo -e \"a b c\\nd e f\" | awk '{ print $0 }' Print each row echo -e \"a b c\\nd e f\" | awk '{ print $1, $2, $3 }' Print each element explicitly for each row echo -e \"a\\tb\\tc\\nd\\te\\tf\" | awk -F '\\t' '{ print $1, $2, $3 }' Print each element explicitly, using tab as a field separator echo -e \"a b c\\nd e f\" | awk '{ print $(NF-2), $(NF-1), $NF }' Print the last three elements in the row echo -e \"a b c\\nd e f\" | awk '{ print NR, NF, $0 }' Print row number, number of columns and the row echo -e \"a b c\\nd e f\" | awk '/ e /{ print NR, $0 }' Match regex and then print the row number and the last three fields echo -e \"a b c\\nd e f\" | awk '$2 == \"e\" { print NR, $0 }' Test to see if the second field is b and print the row number and row echo -e \"a b c\\nd e f\" | awk '$2~/e/{ print $0 }' Test if the second field matches regex and print row echo -e \"a b c\\nd e f\" | awk '{ printf \"ok %s\\n\", $1 }' Use printf to do extra formatting echo -e \"1 2 3\\n4 5 6\" | awk 'BEGIN { print \"begin\" } { tot = tot + $2 } END { print \"end:\", tot, tot/NR }' Calculate the running total and print out the sum and average of the second field ls -la | awk '{ print $1, $2, $3, $4 }' Print out first four columns of ls -la with duplicate separator tokens consolidated into one echo -e \"a\\tb\\tc\\nd\\te\\tf\" | awk 'BEGIN { FS = \"\\t\" } { print NF, $0 }' Change field separator ( FS ) to tab and print each row and their field count echo -e \"a 1\\nb 3\\nc 7\\na 3\" | awk '{ print $1,$2; tot[$1]=tot[$1]+$2; count[$1]=count[$1]+1 } END { for (i in count) { print i, tot[i], count[i] } }' Arrays echo -e \"a 1\\nb 3\\nc 7\\na 3\" | awk '{ print $1,$2; tot[$1]=tot[$1]+$2; count[$1]=count[$1]+1 } END { for (i in count) { if (count[i] > 1) print \">1\", i, tot[i], count[i] ; else if (count[i] < 2) print \"<2\", i, tot[i], count[i] } }' Control flow","title":"awk Cheatsheet"},{"location":"AWK-Cheatsheet.html#2021-10-01","text":"","title":"2021-10-01"},{"location":"Amdahls-Law.html","text":"Amdahl's Law $T_s$ - Time for a serial task $p$ - portion of a task that can be parallelized ($ p \\in [0,1]$) $n$ - number of processes that can be used to parallelize $T_{p,n}$ - Time for a parallel task with $p$ and $n$ $F_{p,n} = \\frac{T_s}{T_{p,n}} $ - Speedup factor for $p$ and $n$ $$ T_{p,n} = T_s (1-p) + \\frac{T_s p}{n} \\\\ \\to \\frac{T_{p,n}}{T_s} = 1 - p + \\frac{p}{n} \\\\ \\to \\frac{T_s}{T_{p,n}} = \\frac{1}{ 1 - p + \\frac{p}{n} } \\\\ \\to F_{p,n} = \\frac{1}{ 1 - p + \\frac{p}{n} } $$ For a completely serial task ($p=0$): $$ F_{p,n} = \\frac{1}{ 1 - 0 + \\frac{0}{n} } = 1 $$ For a completely parallel task ($p=1$): $$ F_{p,n} = \\frac{1}{ 1 - 1 + \\frac{1}{n} } = \\frac{1}{ \\frac{1}{n} } = n $$ When $n >> 1$: $$ F_{p,n} = \\frac{1}{ 1 - p + \\frac{p}{n} } = \\frac{n}{ n - n p + p } \\\\ \\approx \\frac{n}{n -n p} = \\frac{1}{1-p} $$ 2018-09-03","title":"Amdahls Law"},{"location":"Amdahls-Law.html#amdahls-law","text":"$T_s$ - Time for a serial task $p$ - portion of a task that can be parallelized ($ p \\in [0,1]$) $n$ - number of processes that can be used to parallelize $T_{p,n}$ - Time for a parallel task with $p$ and $n$ $F_{p,n} = \\frac{T_s}{T_{p,n}} $ - Speedup factor for $p$ and $n$ $$ T_{p,n} = T_s (1-p) + \\frac{T_s p}{n} \\\\ \\to \\frac{T_{p,n}}{T_s} = 1 - p + \\frac{p}{n} \\\\ \\to \\frac{T_s}{T_{p,n}} = \\frac{1}{ 1 - p + \\frac{p}{n} } \\\\ \\to F_{p,n} = \\frac{1}{ 1 - p + \\frac{p}{n} } $$ For a completely serial task ($p=0$): $$ F_{p,n} = \\frac{1}{ 1 - 0 + \\frac{0}{n} } = 1 $$ For a completely parallel task ($p=1$): $$ F_{p,n} = \\frac{1}{ 1 - 1 + \\frac{1}{n} } = \\frac{1}{ \\frac{1}{n} } = n $$ When $n >> 1$: $$ F_{p,n} = \\frac{1}{ 1 - p + \\frac{p}{n} } = \\frac{n}{ n - n p + p } \\\\ \\approx \\frac{n}{n -n p} = \\frac{1}{1-p} $$","title":"Amdahl's Law"},{"location":"Amdahls-Law.html#2018-09-03","text":"","title":"2018-09-03"},{"location":"Analysis-In-A-Nutshell.html","text":"Analysis In A Nutshell Caucy Convergence Test $$ S = \\sum_{k=0}^{\\infty} a_k < \\infty \\ \\iff \\ \\forall \\epsilon > 0, \\exists N \\in \\mathbb{N} \\ \\forall n > N, p \\ge 0 \\ \\ (n,p \\in \\mathbb{N}) \\ | \\sum_{s=0}^{p} a_{n+s} | < \\epsilon $$ 2018-08-29","title":"Analysis In A Nutshell"},{"location":"Analysis-In-A-Nutshell.html#analysis-in-a-nutshell","text":"","title":"Analysis In A Nutshell"},{"location":"Analysis-In-A-Nutshell.html#caucy-convergence-test","text":"$$ S = \\sum_{k=0}^{\\infty} a_k < \\infty \\ \\iff \\ \\forall \\epsilon > 0, \\exists N \\in \\mathbb{N} \\ \\forall n > N, p \\ge 0 \\ \\ (n,p \\in \\mathbb{N}) \\ | \\sum_{s=0}^{p} a_{n+s} | < \\epsilon $$","title":"Caucy Convergence Test"},{"location":"Analysis-In-A-Nutshell.html#2018-08-29","text":"","title":"2018-08-29"},{"location":"Analytic-Number-Theory-Notes.html","text":"Analytic Number Theory Notes $$ \\begin{equation} \\label{eq1} \\begin{split} P &= { p_0, p_1, \\dots, p_{m-1} } &, \\ p_k \\in \\text{Prime} \\\\ N_p(x) &= |{ n : n = \\prod_{k=0}^{m-1} p_k^{\\alpha_k} \\le x, \\alpha_k \\in \\mathbb{N} 0 }| \\\\ & \\prod {k=0}^{m-1} p^{\\alpha_k} \\le x \\\\ \\to & \\ \\ \\sum_{k=0}^{m-1} \\alpha_k \\ln(p_k) \\le \\ln(x) \\\\ \\to & \\ \\ \\alpha_k \\ln(p_k) \\le \\ln(x) \\\\ x \\ge 2 \\\\ \\to & \\ \\ \\alpha_k \\le \\frac{\\ln(x)}{\\ln(p_k)} \\le \\frac{\\ln(x)}{\\ln(2)} \\\\ \\to & \\ \\ \\alpha_k \\le \\lfloor \\frac{\\ln(x) }{\\ln(2)} \\rfloor + 1 \\le \\frac{2 \\ln(x)}{\\ln(2)} \\\\ N_p(x) & \\le | { [\\alpha_0, \\alpha_1, \\dots, \\alpha_{m-1}] : \\alpha_k \\le \\frac{2\\ln(x)}{\\ln(2)}}| \\\\ & \\le \\left( \\frac{2 \\ln(x)}{\\ln(2)} \\right)^m \\end{split} \\end{equation} $$ $$ \\begin{equation} \\label{eq2} \\begin{split} e(n) &= \\begin{cases} 1 & n=1 \\\\ 0 & \\text{otherwise} \\end{cases} \\\\ id(n) &= n \\\\ s(n) &= \\begin{cases} 1 & n =m^2 \\\\ 0 & \\text{otherwise} \\end{cases} \\\\ \\mu(n) &= \\begin{cases} 1 & n=1 \\\\ (-1)^m & n = \\prod_{k=0}^{m-1} p_k \\\\ 0 & \\text{otherwise} \\end{cases} \\\\ \\lambda(n) & = \\begin{cases} 1 & n=1 \\\\ (-1)^s & s = \\sum_{k=0}^{m-1} \\alpha_k, n = \\prod_{k=0}^{m-1} p_k^{\\alpha_k} \\\\ 0 & \\text{otherwise} \\end{cases} \\\\ \\phi(n) &= |{ m : 1 \\le m \\le n, \\gcd(m,n) }| \\\\ d(n) & = \\sum_{d|n} d \\\\ \\omega(n) &= \\begin{cases} m & n = \\prod_{k=0}^{m-1} p_k^{\\alpha_k} \\end{cases} \\\\ \\Omega(n) &= \\begin{cases} \\sum_{k=0}^{m-1} \\alpha_k & n=\\prod_{k=0}^{m-1} p_k^{\\alpha_k} \\end{cases} \\\\ \\Lambda(n) &= \\begin{cases} \\ln(p) & n = p^m \\\\ 0 & \\text{otherwise} \\end{cases} \\end{split} \\end{equation} $$ $$ \\begin{equation} \\label{eq3} \\begin{split} \\sum_{d|n} \\mu(d) &= \\sum_{ < p > } \\mu(\\prod_{k=0}^{m-1} p_k^{\\alpha_k} ) \\\\ &= \\sum_{ < p > } \\mu(\\prod_{k=0}^{m-1} p_k) \\\\ & =\\sum_{k=0}^{m-1} (-1)^k \\binom{m}{k} \\\\ & = (1-1)^m \\\\ & = \\begin{cases} 1 & n = 1 \\\\ 0 & \\text{otherwise} \\end{cases} \\end{split} \\end{equation} $$","title":"Analytic Number Theory Notes"},{"location":"Analytic-Number-Theory-Notes.html#analytic-number-theory-notes","text":"$$ \\begin{equation} \\label{eq1} \\begin{split} P &= { p_0, p_1, \\dots, p_{m-1} } &, \\ p_k \\in \\text{Prime} \\\\ N_p(x) &= |{ n : n = \\prod_{k=0}^{m-1} p_k^{\\alpha_k} \\le x, \\alpha_k \\in \\mathbb{N} 0 }| \\\\ & \\prod {k=0}^{m-1} p^{\\alpha_k} \\le x \\\\ \\to & \\ \\ \\sum_{k=0}^{m-1} \\alpha_k \\ln(p_k) \\le \\ln(x) \\\\ \\to & \\ \\ \\alpha_k \\ln(p_k) \\le \\ln(x) \\\\ x \\ge 2 \\\\ \\to & \\ \\ \\alpha_k \\le \\frac{\\ln(x)}{\\ln(p_k)} \\le \\frac{\\ln(x)}{\\ln(2)} \\\\ \\to & \\ \\ \\alpha_k \\le \\lfloor \\frac{\\ln(x) }{\\ln(2)} \\rfloor + 1 \\le \\frac{2 \\ln(x)}{\\ln(2)} \\\\ N_p(x) & \\le | { [\\alpha_0, \\alpha_1, \\dots, \\alpha_{m-1}] : \\alpha_k \\le \\frac{2\\ln(x)}{\\ln(2)}}| \\\\ & \\le \\left( \\frac{2 \\ln(x)}{\\ln(2)} \\right)^m \\end{split} \\end{equation} $$ $$ \\begin{equation} \\label{eq2} \\begin{split} e(n) &= \\begin{cases} 1 & n=1 \\\\ 0 & \\text{otherwise} \\end{cases} \\\\ id(n) &= n \\\\ s(n) &= \\begin{cases} 1 & n =m^2 \\\\ 0 & \\text{otherwise} \\end{cases} \\\\ \\mu(n) &= \\begin{cases} 1 & n=1 \\\\ (-1)^m & n = \\prod_{k=0}^{m-1} p_k \\\\ 0 & \\text{otherwise} \\end{cases} \\\\ \\lambda(n) & = \\begin{cases} 1 & n=1 \\\\ (-1)^s & s = \\sum_{k=0}^{m-1} \\alpha_k, n = \\prod_{k=0}^{m-1} p_k^{\\alpha_k} \\\\ 0 & \\text{otherwise} \\end{cases} \\\\ \\phi(n) &= |{ m : 1 \\le m \\le n, \\gcd(m,n) }| \\\\ d(n) & = \\sum_{d|n} d \\\\ \\omega(n) &= \\begin{cases} m & n = \\prod_{k=0}^{m-1} p_k^{\\alpha_k} \\end{cases} \\\\ \\Omega(n) &= \\begin{cases} \\sum_{k=0}^{m-1} \\alpha_k & n=\\prod_{k=0}^{m-1} p_k^{\\alpha_k} \\end{cases} \\\\ \\Lambda(n) &= \\begin{cases} \\ln(p) & n = p^m \\\\ 0 & \\text{otherwise} \\end{cases} \\end{split} \\end{equation} $$ $$ \\begin{equation} \\label{eq3} \\begin{split} \\sum_{d|n} \\mu(d) &= \\sum_{ < p > } \\mu(\\prod_{k=0}^{m-1} p_k^{\\alpha_k} ) \\\\ &= \\sum_{ < p > } \\mu(\\prod_{k=0}^{m-1} p_k) \\\\ & =\\sum_{k=0}^{m-1} (-1)^k \\binom{m}{k} \\\\ & = (1-1)^m \\\\ & = \\begin{cases} 1 & n = 1 \\\\ 0 & \\text{otherwise} \\end{cases} \\end{split} \\end{equation} $$","title":"Analytic Number Theory Notes"},{"location":"Arbitrary-Binary-Functions.html","text":"Arbitrary Binary Functions Shannon's counting argument. $$ x \\in \\mathbb{B}^n \\\\ \\\\ f(X): \\{0,1\\}^n \\mapsto \\{0,1\\} $$ There are $2^{2^n}$ different binary functions. Can we approximate this with less than $\\frac{2^n}{c n}$ boolean gates for some constant $c$? The strategy to show we can't is to overcount the number of boolean gates. $$ \\begin{align} m & & \\text{ boolean gates } (\\frac{2^n}{c n}) \\\\ n & & \\text{ inputs } \\end{align} $$ Label each of the $m$ gates as either one of the $n$ inputs or one of the set of ${ \\wedge, \\vee, \\neg }$. Assume a maximum of 2 inputs but that outputs can be traced to as many other gates as needed. We have a total of $(n+3)$ labels for each gate and a maximum of $m^2$ input choices. Each of the $m$ gates has a choice of label and choice of inputs, giving: $$ ((n + 3) m^2)^m $$ $$ \\lg = \\log_2 $$ For $n$ large enough: $$ \\begin{align} \\lg( ((n + 3) m^2)^m ) & = \\frac{2^n}{c n} [ \\lg(n+3) + 2 n - 2 \\lg(c n) ] \\\\ = \\frac{2^n}{c} [ \\frac{\\lg(n+3)}{n} + 2 - \\frac{2 \\lg(c n)}{n} ] & < \\frac{2^n}{c} [ \\frac{2 \\lg(n)}{n} + 2 - \\frac{2 \\lg(c n)}{n} ] \\\\ = \\frac{2^n}{c} [ 2 + \\frac{2}{n} ( \\lg(n) - \\lg(n) - \\lg(c) ) ] & = \\frac{2^n}{c} [ 2 - \\frac{2}{n} \\lg(c) ] \\\\ < 2 \\frac{ 2^n }{ c } & = \\frac{2^n}{ \\frac{c}{2} } \\end{align} $$ For $c>2$: $$ ((n + 3) m^2)^m < 2^{2^n} $$ Even when we overcount, we still can't the count large enough to represent all binary functions. src 2019-01-10","title":"Arbitrary Binary Functions"},{"location":"Arbitrary-Binary-Functions.html#arbitrary-binary-functions","text":"Shannon's counting argument. $$ x \\in \\mathbb{B}^n \\\\ \\\\ f(X): \\{0,1\\}^n \\mapsto \\{0,1\\} $$ There are $2^{2^n}$ different binary functions. Can we approximate this with less than $\\frac{2^n}{c n}$ boolean gates for some constant $c$? The strategy to show we can't is to overcount the number of boolean gates. $$ \\begin{align} m & & \\text{ boolean gates } (\\frac{2^n}{c n}) \\\\ n & & \\text{ inputs } \\end{align} $$ Label each of the $m$ gates as either one of the $n$ inputs or one of the set of ${ \\wedge, \\vee, \\neg }$. Assume a maximum of 2 inputs but that outputs can be traced to as many other gates as needed. We have a total of $(n+3)$ labels for each gate and a maximum of $m^2$ input choices. Each of the $m$ gates has a choice of label and choice of inputs, giving: $$ ((n + 3) m^2)^m $$ $$ \\lg = \\log_2 $$ For $n$ large enough: $$ \\begin{align} \\lg( ((n + 3) m^2)^m ) & = \\frac{2^n}{c n} [ \\lg(n+3) + 2 n - 2 \\lg(c n) ] \\\\ = \\frac{2^n}{c} [ \\frac{\\lg(n+3)}{n} + 2 - \\frac{2 \\lg(c n)}{n} ] & < \\frac{2^n}{c} [ \\frac{2 \\lg(n)}{n} + 2 - \\frac{2 \\lg(c n)}{n} ] \\\\ = \\frac{2^n}{c} [ 2 + \\frac{2}{n} ( \\lg(n) - \\lg(n) - \\lg(c) ) ] & = \\frac{2^n}{c} [ 2 - \\frac{2}{n} \\lg(c) ] \\\\ < 2 \\frac{ 2^n }{ c } & = \\frac{2^n}{ \\frac{c}{2} } \\end{align} $$ For $c>2$: $$ ((n + 3) m^2)^m < 2^{2^n} $$ Even when we overcount, we still can't the count large enough to represent all binary functions. src","title":"Arbitrary Binary Functions"},{"location":"Arbitrary-Binary-Functions.html#2019-01-10","text":"","title":"2019-01-10"},{"location":"Assorted-Small-Probability-Problems.html","text":"Assorted Small Probability Problems Coupon Collector Find the expected time, $T$, to wait to collect $n$ different coupons, each appearing with probability $\\frac{1}{n}$. Assign the random variable $X_t$ to be the time taken to see a new coupon once $t$ have already been collected. $$ X_0 = 1 $$ $$ \\begin{align} E[X_t] & = \\sum_{k=1}^{\\infty} k (\\frac{t}{n})^{k-1} (1 - \\frac{t}{n}) \\\\ & = (1 - \\frac{t}{n}) ( \\sum_{k=0}^{\\infty} k (\\frac{t}{n})^k + \\sum_{k=0}^{\\infty} (\\frac{t}{n})^k ) \\\\ & = (1 - \\frac{t}{n}) ( \\frac{ \\frac{t}{n} }{ (1 - \\frac{t}{n} )^2 } + \\frac{1}{1 - \\frac{t}{n}} ) \\\\ & = \\frac{n}{n-t} \\end{align} $$ $$ E[T] = E[ \\sum_{t=0}^{n-1} X_t ] $$ Since the expectation is independent $$ \\begin{align} E[ \\sum_{t=0}^{n-1} X_t ] & = \\sum_{t=0}^{n-1} E[X_t] \\\\ & = \\frac{n}{n} + \\frac{n}{n-1} + \\frac{n}{n-2} + \\cdots + \\frac{n}{1} \\\\ & = \\sum_{t=0}^{n-1} \\frac{n}{n-t} \\\\ & = n ( \\sum_{t=0}^{n-1} \\frac{1}{n-t} ) \\\\ & = n \\cdot H_n \\end{align} $$ Where $H_n$ is the $n$-th harmonic number. $$ \\begin{align} E[T] & = n \\cdot H_n \\\\ & = n \\log n + \\gamma n + \\frac{1}{2} + O(\\frac{1}{n}) \\\\ & \\approx n \\log n & \\end{align} $$ Birthday Problem $n$ people are assigned a random integer uniformly at random from $[1,m]$. For a given probability $p$, what is the relationship to $n$ and $m$ that at least two people have the same number. $Q_{n,m}$ the probability that no two of $n$ people with $m$ numbers have a number in common $$ \\begin{align} Q_{n,m} & = \\prod_{t=1}^{n} (1 - \\frac{t}{m}) \\\\ & \\le \\prod_{t=1}^{n} e^{-\\frac{t}{m}} \\\\ & = \\exp( -\\sum_{t=1}^{n} \\frac{t}{m} ) \\\\ & = \\exp( -\\frac{ {n+1 \\choose 2 } }{m} ) \\\\ & = \\exp( -\\frac{n (n+1)}{2 m} ) \\end{align} $$ $$ p \\approx 1 - \\exp( -\\frac{n (n+1)}{2 m} ) $$ $$ \\to \\log \\frac{1}{1-p} \\approx \\frac{ n (n+1)}{2 m} $$ For $n >> 1$ we can approximate further: $$ \\begin{align} & \\to \\log \\frac{1}{1-p} \\approx \\frac{ n^2 }{2 m} \\\\ & \\to n \\approx \\sqrt{ 2 m \\log \\frac{1}{1-p} } \\\\ \\end{align} $$ For $m=365$ we get $n \\approx 22.494...$. Best Choice Problem $n$ candidates, with each that can be ranked relative to the others. The candidates file in one at a time and are ranked relative to the candidates already seen. When each candidate is first seen, a decision is made to accept or reject them. Find the optimal stopping point to maximize the likelihood of getting the ideal candidate. One strategy is a \"wait then choose\" strategy, where $r$ of $n$ candidates are considered and then the next candidate chosen that is better than all of the $r$ previously seen. With this in mind, the probability becomes: $$ \\begin{align} P(r) & = \\sum_{k=1}^{n} \\Pr \\{ \\text{ candidate k chosen } \\& \\text{ candidate k is best } \\} \\\\ & = \\sum_{k=1}^{n} \\Pr \\{ \\text{ next best candidate in} \\in [1 \\dots r] | \\text{ candidate k is best } \\} \\\\ & = \\sum_{k=r+1}^{n} \\frac{r}{k-1} \\cdot \\frac{1}{n} \\\\ & = \\frac{r}{n} \\sum_{k=r}^{n-1} \\frac{1}{k} \\\\ & = \\frac{r}{n} ( H_{n-1} - H_{r} ) \\end{align} $$ Consider $$ \\begin{align} f(r) & = \\frac{r}{n} ( H_{n-1} - H_{r} ) \\\\ & \\approx \\frac{r}{n} ( \\ln(n) - \\ln(r) ) \\end{align} $$ If $f(r)$ was continuous and had a single global maximum in the range of $[1 \\dots n]$, we could find the maximum by evaluating the derivative of $f(r)$ at 0. That is: $$ \\begin{align} \\frac{d}{dr} f(r) & \\approx \\frac{d}{dr} ( \\frac{r}{n} ( \\ln(n) - \\ln(r) ) \\\\ & = \\ln(n) - \\ln(r) - 1 \\end{align} $$ $$ \\begin{align} \\to & \\ln(n) - \\ln(r) - 1 & = 0 \\\\ \\to & \\frac{r}{n} = \\frac{1}{e} \\end{align} $$ Meaning, the best strategy is to wait to see $\\frac{n}{e}$ candidates and then take the next best one. Hat Derangement $n$ people each take a random hat out of a bag after throwing them all in. Find the probability of no person getting their own hat back. A crude way to do this is just consider the probability of a person randomly choosing a hat without considering the previously drawn hats. Using this approximation, there are $(1-\\frac{1}{n})$ choices for some other hat, leaving the probability of no person chooses their own hat as: $$ (1 - \\frac{1}{n})^{n} \\to \\frac{1}{e} $$ The more complete solution is to make an inclusion-exclusion argument by counting the number of possibilities. Call $S$ the set of all permutations, and $F_k$ be the set of all permutations that fixes position $k$, then the number of permutations is: $$ \\begin{align} & |S| - |F_0 \\cup F_1 \\cup F_2 \\cup \\dots \\cup F_{n-1}| \\\\ & = |S| - \\sum_{k=0}^{n-1} |F_k| + \\sum_{k=0}^{n-1} \\sum_{k'=k+1}^{n-1} |F_k \\cap F_{k'}| - \\dots + (-1)^{n-1} |F_0 \\cap F_1 \\cap \\dots \\cap F_{n-1}| \\end{align} $$ Symmetry of the sets allows us to consolidate the counts: $$ \\begin{align} & = n! - \\binom{n}{1} (n-1)! + \\binom{n}{2} (n-2)! - \\binom{n}{3} (n-3)! + \\dots + (-1)^{n-1} \\binom{n}{n} 1! \\\\ & = \\sum_{k=0}^{n-1} (-1)^k \\binom{n}{k} (n-k)! \\\\ & = n! ( \\sum_{k=0}^{n} \\frac{ (-1)^k }{ k! } ) \\\\ & \\approx n! e^{-1} \\end{align} $$ Since there are $n!$ total configurations, the probability of a permutation not leaving any position fixed is: $$ \\begin{align} & = \\frac{n! e^{-1} }{n!} \\\\ & = \\frac{1}{e} \\end{align} $$ Labeled Box Problem There are $n$ boxes each assigned randomly a unique number from $1$ to $n$ (inclusive) and $n$ participants each assigned a unique number from $1$ to $n$ (inclusive). Each participant can examine half of the boxes, with the ability to choose later boxes depending on what was seen previously. No participant is allowed to communicate once the process starts. Find a reasonable lower bound on the probability of each participant seeing their own number. By considering the numbers in the boxes as permutations, each participant can start with the box position of the number they've been assigned and continue on, jumping to the subsequent box positions from the revealed number after each box opening. The probability each participant sees their own number in this process is the the chance that the \"box permutation\" has a maximum cycle less than or equal to $\\lfloor \\frac{n}{2} \\rfloor$. The trivial cycle permutation $(1,2,3,\\dots,n)$ has $\\binom{n}{l}$ ways to choose a particular cycle of length $l$. In disjoint cycle notation, it should be clear that for a cycle of length $l$, there are $l!$ different presentations. The remaining elements can be permuted arbitrarily yielding the number of cycles of length $l$: $$ \\begin{align} & \\binom{n}{l} \\cdot l! \\cdot (n-l)! \\\\ & = \\frac{n!}{l} \\end{align} $$ Summing over all cycles of length $l > \\lfloor \\frac{n}{2} \\rfloor$: $$ \\begin{align} & \\sum_{l=\\lfloor \\frac{n}{2} \\rfloor +1} \\frac{n!}{l} \\\\ & = n! \\sum_{l=\\lfloor \\frac{n}{2} \\rfloor + 1} \\frac{1}{l} \\\\ & = n! ( H_n - H_{\\lfloor \\frac{n}{2} \\rfloor + 1} ) \\end{align} $$ The total number of permutations is $n!$, so the resulting probability of having no cycle greater than $\\lfloor \\frac{n}{2} \\rfloor$ is: $$ 1 - (H_{n} - H_{\\lfloor \\frac{n}{2} \\rfloor + 1}) $$ Since $$ \\lim_{n \\to \\infty} (H_n - \\ln n) = \\gamma $$ we get: $$ \\begin{align} & \\lim_{n \\to \\infty} ( 1 - (H_{n} - H_{\\lfloor \\frac{n}{2} \\rfloor + 1} ) ) \\\\ & = 1 - (\\gamma - \\gamma) - \\ln 2 \\\\ & = 1 - \\ln 2 \\end{align} $$ Unfair Coin Given an unfair coin with unknown probability $p$ of landing heads, find a reasonably efficient process that yields $0.5$ probability and estimate the time it takes to 'draw' from the resulting fair distribution. $$ \\Pr \\{ H T \\} = \\Pr \\{ T H \\} = p (1-p) = (1-p) p $$ 2018-06-29","title":"Assorted Small Probability Problems"},{"location":"Assorted-Small-Probability-Problems.html#assorted-small-probability-problems","text":"","title":"Assorted Small Probability Problems"},{"location":"Assorted-Small-Probability-Problems.html#coupon-collector","text":"Find the expected time, $T$, to wait to collect $n$ different coupons, each appearing with probability $\\frac{1}{n}$. Assign the random variable $X_t$ to be the time taken to see a new coupon once $t$ have already been collected. $$ X_0 = 1 $$ $$ \\begin{align} E[X_t] & = \\sum_{k=1}^{\\infty} k (\\frac{t}{n})^{k-1} (1 - \\frac{t}{n}) \\\\ & = (1 - \\frac{t}{n}) ( \\sum_{k=0}^{\\infty} k (\\frac{t}{n})^k + \\sum_{k=0}^{\\infty} (\\frac{t}{n})^k ) \\\\ & = (1 - \\frac{t}{n}) ( \\frac{ \\frac{t}{n} }{ (1 - \\frac{t}{n} )^2 } + \\frac{1}{1 - \\frac{t}{n}} ) \\\\ & = \\frac{n}{n-t} \\end{align} $$ $$ E[T] = E[ \\sum_{t=0}^{n-1} X_t ] $$ Since the expectation is independent $$ \\begin{align} E[ \\sum_{t=0}^{n-1} X_t ] & = \\sum_{t=0}^{n-1} E[X_t] \\\\ & = \\frac{n}{n} + \\frac{n}{n-1} + \\frac{n}{n-2} + \\cdots + \\frac{n}{1} \\\\ & = \\sum_{t=0}^{n-1} \\frac{n}{n-t} \\\\ & = n ( \\sum_{t=0}^{n-1} \\frac{1}{n-t} ) \\\\ & = n \\cdot H_n \\end{align} $$ Where $H_n$ is the $n$-th harmonic number. $$ \\begin{align} E[T] & = n \\cdot H_n \\\\ & = n \\log n + \\gamma n + \\frac{1}{2} + O(\\frac{1}{n}) \\\\ & \\approx n \\log n & \\end{align} $$","title":"Coupon Collector"},{"location":"Assorted-Small-Probability-Problems.html#birthday-problem","text":"$n$ people are assigned a random integer uniformly at random from $[1,m]$. For a given probability $p$, what is the relationship to $n$ and $m$ that at least two people have the same number. $Q_{n,m}$ the probability that no two of $n$ people with $m$ numbers have a number in common $$ \\begin{align} Q_{n,m} & = \\prod_{t=1}^{n} (1 - \\frac{t}{m}) \\\\ & \\le \\prod_{t=1}^{n} e^{-\\frac{t}{m}} \\\\ & = \\exp( -\\sum_{t=1}^{n} \\frac{t}{m} ) \\\\ & = \\exp( -\\frac{ {n+1 \\choose 2 } }{m} ) \\\\ & = \\exp( -\\frac{n (n+1)}{2 m} ) \\end{align} $$ $$ p \\approx 1 - \\exp( -\\frac{n (n+1)}{2 m} ) $$ $$ \\to \\log \\frac{1}{1-p} \\approx \\frac{ n (n+1)}{2 m} $$ For $n >> 1$ we can approximate further: $$ \\begin{align} & \\to \\log \\frac{1}{1-p} \\approx \\frac{ n^2 }{2 m} \\\\ & \\to n \\approx \\sqrt{ 2 m \\log \\frac{1}{1-p} } \\\\ \\end{align} $$ For $m=365$ we get $n \\approx 22.494...$.","title":"Birthday Problem"},{"location":"Assorted-Small-Probability-Problems.html#best-choice-problem","text":"$n$ candidates, with each that can be ranked relative to the others. The candidates file in one at a time and are ranked relative to the candidates already seen. When each candidate is first seen, a decision is made to accept or reject them. Find the optimal stopping point to maximize the likelihood of getting the ideal candidate. One strategy is a \"wait then choose\" strategy, where $r$ of $n$ candidates are considered and then the next candidate chosen that is better than all of the $r$ previously seen. With this in mind, the probability becomes: $$ \\begin{align} P(r) & = \\sum_{k=1}^{n} \\Pr \\{ \\text{ candidate k chosen } \\& \\text{ candidate k is best } \\} \\\\ & = \\sum_{k=1}^{n} \\Pr \\{ \\text{ next best candidate in} \\in [1 \\dots r] | \\text{ candidate k is best } \\} \\\\ & = \\sum_{k=r+1}^{n} \\frac{r}{k-1} \\cdot \\frac{1}{n} \\\\ & = \\frac{r}{n} \\sum_{k=r}^{n-1} \\frac{1}{k} \\\\ & = \\frac{r}{n} ( H_{n-1} - H_{r} ) \\end{align} $$ Consider $$ \\begin{align} f(r) & = \\frac{r}{n} ( H_{n-1} - H_{r} ) \\\\ & \\approx \\frac{r}{n} ( \\ln(n) - \\ln(r) ) \\end{align} $$ If $f(r)$ was continuous and had a single global maximum in the range of $[1 \\dots n]$, we could find the maximum by evaluating the derivative of $f(r)$ at 0. That is: $$ \\begin{align} \\frac{d}{dr} f(r) & \\approx \\frac{d}{dr} ( \\frac{r}{n} ( \\ln(n) - \\ln(r) ) \\\\ & = \\ln(n) - \\ln(r) - 1 \\end{align} $$ $$ \\begin{align} \\to & \\ln(n) - \\ln(r) - 1 & = 0 \\\\ \\to & \\frac{r}{n} = \\frac{1}{e} \\end{align} $$ Meaning, the best strategy is to wait to see $\\frac{n}{e}$ candidates and then take the next best one.","title":"Best Choice Problem"},{"location":"Assorted-Small-Probability-Problems.html#hat-derangement","text":"$n$ people each take a random hat out of a bag after throwing them all in. Find the probability of no person getting their own hat back. A crude way to do this is just consider the probability of a person randomly choosing a hat without considering the previously drawn hats. Using this approximation, there are $(1-\\frac{1}{n})$ choices for some other hat, leaving the probability of no person chooses their own hat as: $$ (1 - \\frac{1}{n})^{n} \\to \\frac{1}{e} $$ The more complete solution is to make an inclusion-exclusion argument by counting the number of possibilities. Call $S$ the set of all permutations, and $F_k$ be the set of all permutations that fixes position $k$, then the number of permutations is: $$ \\begin{align} & |S| - |F_0 \\cup F_1 \\cup F_2 \\cup \\dots \\cup F_{n-1}| \\\\ & = |S| - \\sum_{k=0}^{n-1} |F_k| + \\sum_{k=0}^{n-1} \\sum_{k'=k+1}^{n-1} |F_k \\cap F_{k'}| - \\dots + (-1)^{n-1} |F_0 \\cap F_1 \\cap \\dots \\cap F_{n-1}| \\end{align} $$ Symmetry of the sets allows us to consolidate the counts: $$ \\begin{align} & = n! - \\binom{n}{1} (n-1)! + \\binom{n}{2} (n-2)! - \\binom{n}{3} (n-3)! + \\dots + (-1)^{n-1} \\binom{n}{n} 1! \\\\ & = \\sum_{k=0}^{n-1} (-1)^k \\binom{n}{k} (n-k)! \\\\ & = n! ( \\sum_{k=0}^{n} \\frac{ (-1)^k }{ k! } ) \\\\ & \\approx n! e^{-1} \\end{align} $$ Since there are $n!$ total configurations, the probability of a permutation not leaving any position fixed is: $$ \\begin{align} & = \\frac{n! e^{-1} }{n!} \\\\ & = \\frac{1}{e} \\end{align} $$","title":"Hat Derangement"},{"location":"Assorted-Small-Probability-Problems.html#labeled-box-problem","text":"There are $n$ boxes each assigned randomly a unique number from $1$ to $n$ (inclusive) and $n$ participants each assigned a unique number from $1$ to $n$ (inclusive). Each participant can examine half of the boxes, with the ability to choose later boxes depending on what was seen previously. No participant is allowed to communicate once the process starts. Find a reasonable lower bound on the probability of each participant seeing their own number. By considering the numbers in the boxes as permutations, each participant can start with the box position of the number they've been assigned and continue on, jumping to the subsequent box positions from the revealed number after each box opening. The probability each participant sees their own number in this process is the the chance that the \"box permutation\" has a maximum cycle less than or equal to $\\lfloor \\frac{n}{2} \\rfloor$. The trivial cycle permutation $(1,2,3,\\dots,n)$ has $\\binom{n}{l}$ ways to choose a particular cycle of length $l$. In disjoint cycle notation, it should be clear that for a cycle of length $l$, there are $l!$ different presentations. The remaining elements can be permuted arbitrarily yielding the number of cycles of length $l$: $$ \\begin{align} & \\binom{n}{l} \\cdot l! \\cdot (n-l)! \\\\ & = \\frac{n!}{l} \\end{align} $$ Summing over all cycles of length $l > \\lfloor \\frac{n}{2} \\rfloor$: $$ \\begin{align} & \\sum_{l=\\lfloor \\frac{n}{2} \\rfloor +1} \\frac{n!}{l} \\\\ & = n! \\sum_{l=\\lfloor \\frac{n}{2} \\rfloor + 1} \\frac{1}{l} \\\\ & = n! ( H_n - H_{\\lfloor \\frac{n}{2} \\rfloor + 1} ) \\end{align} $$ The total number of permutations is $n!$, so the resulting probability of having no cycle greater than $\\lfloor \\frac{n}{2} \\rfloor$ is: $$ 1 - (H_{n} - H_{\\lfloor \\frac{n}{2} \\rfloor + 1}) $$ Since $$ \\lim_{n \\to \\infty} (H_n - \\ln n) = \\gamma $$ we get: $$ \\begin{align} & \\lim_{n \\to \\infty} ( 1 - (H_{n} - H_{\\lfloor \\frac{n}{2} \\rfloor + 1} ) ) \\\\ & = 1 - (\\gamma - \\gamma) - \\ln 2 \\\\ & = 1 - \\ln 2 \\end{align} $$","title":"Labeled Box Problem"},{"location":"Assorted-Small-Probability-Problems.html#unfair-coin","text":"Given an unfair coin with unknown probability $p$ of landing heads, find a reasonably efficient process that yields $0.5$ probability and estimate the time it takes to 'draw' from the resulting fair distribution. $$ \\Pr \\{ H T \\} = \\Pr \\{ T H \\} = p (1-p) = (1-p) p $$","title":"Unfair Coin"},{"location":"Assorted-Small-Probability-Problems.html#2018-06-29","text":"","title":"2018-06-29"},{"location":"BGZF-Example.html","text":"BGZF Example BGZIP allows for quick random access to a bgzip file by creating an index. As an example, here is a way to compress a file with bgzip and access a random portion of it: $ ls -la total 685104 drwxrwxr-x 2 abetusk abetusk 4096 Jun 12 17:33 . drwxrwxr-x 5 abetusk abetusk 4096 Jun 12 17:33 .. -rw-rw-r-- 1 abetusk abetusk 701533731 Jun 12 17:33 hu826751.gff $ bgzip -i hu826751.gff $ ls hu826751.gff.gz hu826751.gff.gz.gzi $ bgzip -h Version: 1.4.1 Usage: bgzip [OPTIONS] [FILE] ... Options: -b, --offset INT decompress at virtual file pointer (0-based uncompressed offset) -c, --stdout write on standard output, keep original files unchanged -d, --decompress decompress -f, --force overwrite files without asking -h, --help give this help -i, --index compress and create BGZF index -I, --index-name FILE name of BGZF index file [file.gz.gzi] -r, --reindex (re)index compressed file -g, --rebgzip use an index file to bgzip a file -s, --size INT decompress INT bytes (uncompressed size) -@, --threads INT number of compression threads to use [1] $ time bgzip -b 100000000 -s 100 hu826751.gff.gz 54 . + . alleles C/T;db_xref dbsnp.120:rs11035863;ref_allele C chr11 CGI REF 40509955 40510029 . + . real 0m0.009s user 0m0.004s sys 0m0.004s C Example Here's a C example: #include <stdio.h> #include <stdlib.h> #include <stdint.h> #include \"bgzf.h\" #include <vector> #include <string> int main(int argc, char **argv) { int i, j, k, r; BGZF *bgzfp; std::string ifn, idx_fn; int64_t pos=-1; char buf[1024]; ssize_t s; size_t buflen=1024; if (argc<2) { printf(\"provide bgzip file\\n\"); exit(-1); } ifn = argv[1]; printf(\"loading bgzip file %s\\n\", ifn.c_str()); bgzfp = bgzf_open(ifn.c_str(), \"r\"); if (!bgzfp) { fprintf(stderr, \"error opening file %s\\n\", ifn.c_str()); exit(1); } printf(\"loading index bgzip file %s%s\\n\", ifn.c_str(), \".gzi\"); r = bgzf_index_load(bgzfp, ifn.c_str(), \".gzi\"); printf(\"got %i\\n\", r); r = bgzf_useek(bgzfp, 100000000, SEEK_SET); printf(\"got %i\\n\", r); if (r<0) { perror(\"...\"); exit(-1); } s = bgzf_read(bgzfp, buf, sizeof(char)*buflen); printf(\"...%i\\n\", (int)s); printf(\"---\\n\"); for (i=0; i<s; i++) { printf(\"%c\", buf[i]); } printf(\"\\n---\\n\"); bgzf_close(bgzfp); } To compile: g++ -I $HTSDIR/htslib-1.4.1/htslib -lhts bgzf-example.cpp -o bgzf-example -L $HTSLIB/htslib-1.4.1 -lhts To run: LD_LIBRARY_PATH=$HOME/htslib-1.4.1 ./bgzf-example hu826751.gff.gz Which assumes the hu826751.gff.gz.gzi file is in the same directory as the hu826751.gff.gz file. This assumes htslib is installed under the directory pointed to by the HTSLIB environment variable. See the htslib repo for details on how to download and install. 2017-06-12","title":"BGZF Example"},{"location":"BGZF-Example.html#bgzf-example","text":"BGZIP allows for quick random access to a bgzip file by creating an index. As an example, here is a way to compress a file with bgzip and access a random portion of it: $ ls -la total 685104 drwxrwxr-x 2 abetusk abetusk 4096 Jun 12 17:33 . drwxrwxr-x 5 abetusk abetusk 4096 Jun 12 17:33 .. -rw-rw-r-- 1 abetusk abetusk 701533731 Jun 12 17:33 hu826751.gff $ bgzip -i hu826751.gff $ ls hu826751.gff.gz hu826751.gff.gz.gzi $ bgzip -h Version: 1.4.1 Usage: bgzip [OPTIONS] [FILE] ... Options: -b, --offset INT decompress at virtual file pointer (0-based uncompressed offset) -c, --stdout write on standard output, keep original files unchanged -d, --decompress decompress -f, --force overwrite files without asking -h, --help give this help -i, --index compress and create BGZF index -I, --index-name FILE name of BGZF index file [file.gz.gzi] -r, --reindex (re)index compressed file -g, --rebgzip use an index file to bgzip a file -s, --size INT decompress INT bytes (uncompressed size) -@, --threads INT number of compression threads to use [1] $ time bgzip -b 100000000 -s 100 hu826751.gff.gz 54 . + . alleles C/T;db_xref dbsnp.120:rs11035863;ref_allele C chr11 CGI REF 40509955 40510029 . + . real 0m0.009s user 0m0.004s sys 0m0.004s","title":"BGZF Example"},{"location":"BGZF-Example.html#c-example","text":"Here's a C example: #include <stdio.h> #include <stdlib.h> #include <stdint.h> #include \"bgzf.h\" #include <vector> #include <string> int main(int argc, char **argv) { int i, j, k, r; BGZF *bgzfp; std::string ifn, idx_fn; int64_t pos=-1; char buf[1024]; ssize_t s; size_t buflen=1024; if (argc<2) { printf(\"provide bgzip file\\n\"); exit(-1); } ifn = argv[1]; printf(\"loading bgzip file %s\\n\", ifn.c_str()); bgzfp = bgzf_open(ifn.c_str(), \"r\"); if (!bgzfp) { fprintf(stderr, \"error opening file %s\\n\", ifn.c_str()); exit(1); } printf(\"loading index bgzip file %s%s\\n\", ifn.c_str(), \".gzi\"); r = bgzf_index_load(bgzfp, ifn.c_str(), \".gzi\"); printf(\"got %i\\n\", r); r = bgzf_useek(bgzfp, 100000000, SEEK_SET); printf(\"got %i\\n\", r); if (r<0) { perror(\"...\"); exit(-1); } s = bgzf_read(bgzfp, buf, sizeof(char)*buflen); printf(\"...%i\\n\", (int)s); printf(\"---\\n\"); for (i=0; i<s; i++) { printf(\"%c\", buf[i]); } printf(\"\\n---\\n\"); bgzf_close(bgzfp); } To compile: g++ -I $HTSDIR/htslib-1.4.1/htslib -lhts bgzf-example.cpp -o bgzf-example -L $HTSLIB/htslib-1.4.1 -lhts To run: LD_LIBRARY_PATH=$HOME/htslib-1.4.1 ./bgzf-example hu826751.gff.gz Which assumes the hu826751.gff.gz.gzi file is in the same directory as the hu826751.gff.gz file. This assumes htslib is installed under the directory pointed to by the HTSLIB environment variable. See the htslib repo for details on how to download and install.","title":"C Example"},{"location":"BGZF-Example.html#2017-06-12","text":"","title":"2017-06-12"},{"location":"Belief-Propagation.html","text":"Thanks. Belief Propagation Introduction For a set of random variables, ${ \\bf x } = ( x_0, x_1, x_2, \\cdots, x_{ n-1 } )$, and a given probability distribution over those random variables, $p( { \\bf x } )$, we might like to know what the most likely configuration is. Another name for the most likely configuration is the maximum a posteriori (MAP) assignment of the variables over their domain. Sometimes there is additional structure on the random variables so that the probability function can be decomposed into a product of distributions over the subsets of variables. If the additional structure is encapsulated in a graphical model, connecting variables with an edge and corresponding distribution function, this this can be represented as a Markov Random Field (MRF), with a set of cliques, represented in a graph structure, whose probability distribution function can be written as: $$ \\begin{align} p( { \\bf x } ) & = \\frac{1}{Z} \\prod_{c \\in \\text{clique(} G \\text{)} } \\phi_c ( { \\bf x }_c ) \\end{align} $$ A marginal distribution is the probability of fixing a subset of the random variables over all possible values of the remaining variables. That is: $$ p( { \\bf x } _ s = { \\bf d } _ s ) = \\sum_{ { \\bf x } / x_s } \\text{ } \\prod_{ c \\in \\text{ clique( } G \\text{ ) } } \\phi_c ( { \\bf x } _ c ) $$ Where the term on the right fixes the values of ${ \\bf x }_s$ where appropriate. This problem is NP-Complete in general. If the form of the graphical model is a tree, then the marginals can be computed efficiently with a dynamic programming like algorithm. A dynamic programming like algorithm can be adapted to be used on non-tree like graphs and can be expected to work for graphs that have certain restrictions on their topology or structure. Since the graph now has loops, the independence property that was needed to make the algorithm efficient is violated. The dynamic programming like algorithm is called Belief Propagation (BP), sometimes called Loopy Belief Propagation (LBP) for non-tree like graphs. For non-tree graphs, the term \"probability\" is substituted with \"messages\", as calculations are no longer probability distributions any more, with the underlying algorithm for BP described as a \"message passing\" algorithm. The type of structure on the graphs that lead to proper marginal discovery is not well understood in general and certainly not understood by me. Some clue as to what the structure is comes from the degree of independence of the variables and a somewhat hand-waivy heuristic is that if graphs look locally \"tree-like\" then BP (or LBP) has some expectation of converging to a correct solution. There are three main graph models that belief propagation looks to be run on: Bayesian Network Factor Graph Markov Random Fields (MRF) Markov Random Field Finite Markov Random Fields (MRF) will be considered. $$ G(V,E), \\ \\ \\ |V| = n, \\ \\ \\ x_i \\in D = { d_0, d_1, \\cdots, d_{m-1} } $$ $$ i \\in V \\to g_i(\\cdot) \\in \\mathbb{R} $$ $$ (i,j) \\in E \\to f_{i,j}( \\cdot, \\cdot ) \\in \\mathbb{R} $$ $x_i$ represents the value of vertex $i$. $g_i(x_i)$ is the mapping of vertex $i$ with value $x_i$ $f_{i,j}(x_i,x_j)$ is the mapping of the connected vertex $i$ and $j$ with values $x_i$ and $x_j$, respectively For example, $f_{i,j}(x_i,x_j)$ could be an indicator function that vertex $i$ and $j$ could have values $x_i$ and $x_j$. Belief Propagation on a (discrete) Markov Random Field ::mermaid graph LR u((u)) & v((v)) --> i((i)) -.-> j((j))) $$ \\mu_{i,j}(d) = \\sum_{a \\in D} f_{u,i}(a) \\cdot f_{v,i}(a) \\dots $$ Each vertex, $i$, can be associated with a random variable, $X_i$, taking on (discrete) values chosen from some domain $D = { d_0, d_1, \\cdots, d_{m-1} }$ with a probability distribution function $g_i(\\cdot)$. $$ \\mu_{i,j}^{t+1}(d) = \\sum_{a \\in D} f_{i,j}(a,d) \\cdot g_i(a) \\cdot \\prod_{k \\in N(i) \\text{ \\\\ } j} \\mu_{k,i}^{t}(a) $$ $$ P(X_i = a) \\approx b^t_i(a) \\propto g_i(a) \\cdot \\prod_{k \\in N(i)} \\mu^t_{k,i}(a) $$ $$ \\sum_{b \\in D} \\mu_{i,j}^{t}(d) = 1, \\ \\ \\ \\ \\sum_{a \\in D} b^t_i(a) = 1 $$ The product can be more compactly represented by a function $h^t_{i,j}(\\cdot)$: $$ h^t_{i,j}(a) = g_i(a) \\cdot \\prod_{k \\in N(i) \\text{\\\\} j } \\mu^t_{k,i}(a) $$ $$ \\mu_{i,j}^{t+1}(d) = \\sum_{a \\in D} f_{i,j}(a,d) \\cdot h^t_{i,j}(a) $$ One can recast this as a matrix multiplication: $$ \\begin{bmatrix} f_{i,j}(d_0,d_0) & f_{i,j}(d_1,d_0) & \\cdots & f_{i,j}(d_{m-1},d_0) \\\\ f_{i,j}(d_0,d_1) & f_{i,j}(d_1,d_1) & \\cdots & f_{i,j}(d_{m-1},d_1) \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\\\ f_{i,j}(d_0,d_{m-1}) & f_{i,j}(d_1,d_{m-1}) & \\cdots & f_{i,j}(d_{m-1},d_{m-1}) \\end{bmatrix} \\begin{bmatrix} h_{i,j}^{t}(d_0) \\\\ h_{i,j}^{t}(d_1) \\\\ \\vdots \\\\ h_{i,j}^{t}(d_{m-1}) \\end{bmatrix} = \\begin{bmatrix} \\mu_{i,j}^{t+1}(d_0) \\\\ \\mu_{i,j}^{t+1}(d_1) \\\\ \\vdots \\\\ \\mu_{i,j}^{t+1}(d_{m-1}) \\end{bmatrix} $$ $$ \\to F_{i,j} \\cdot \\vec{h}^t_{i,j} = \\vec{\\mu}^{t+1}_{i,j} $$ Since $h^t_{i,j}(\\cdot)$ has no dependence on $b$, this speeds up a naive calculation by re-using the product results instead of re-calculating them. If the $F_{i,j}$ matrix has low rank, $r < m$, it can be factored into a singular value decomposition (SVD) for performance: $$U \\cdot S \\cdot V = \\begin{bmatrix} \\vec u_0 & \\vec u_1 & \\cdots & \\vec u_{r-1} \\end{bmatrix} \\begin{bmatrix} s_0 & 0 & \\cdots & 0 \\\\ 0 & s_1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & s_{r-1} \\end{bmatrix} \\begin{bmatrix} \\vec{v}^\\dagger_0 \\\\ \\vec{v}^\\dagger_1 \\\\ \\vdots \\\\ \\vec{v}_{r-1}^\\dagger \\end{bmatrix} $$ Where $F_{i,j} = U \\cdot S \\cdot V$. The matrix multiplication that was $O(m^2)$ now becomes two matrix multiplications of order $O(r \\cdot m)$ for a potential speedup of $\\sim \\frac{m}{r}$. Note that if the $F$ matrix has any symmetry, there are potential efficiency gains by exploiting that symmetry. In the above, when $F$ is of low rank, the SVD is exploiting the \"symmetry' of having many zero eigenvalues. There is a well known optimization if the implied function has certain translational symmetry properties, allowing the multiplication to be done via a fast Fourier transform, producing an $O(m \\lg m)$ calculation over the $O(m^2)$. Bethe Free Energy (wip) Belief Propagation (BP) can be thought of as an algorithm that finds a fixed point of the Bethe Free Energy, $F_\\beta$: $$ \\begin{array}{ll} \\phi _ {i,j} (d _ 0, d _ 1 ) = & f ( d _ 0, d _ 1 ) g ( d _ 0 ) g ( d _ 1 ) \\ b _ {i,j} (d _ 0,d _ 1) = & \\alpha \\phi _ {i,j} ( d _ 0, d _ 1 ) \\prod _ { k \\in N(i) / j } \\mu _ {k,i} ( d_0) \\prod _ { l \\in N(j) / i } \\mu _ {l,j} (d _ 1) \\ F _ {\\beta} (f, g) = & \\sum _ {i,j} \\sum _ {d _ 0, d _ 1} b _ {i,j} ( d _ 0, d _ 1 ) [ \\ln(b _ {i,j} (d _ 0, d _ 1 )) - \\ln( \\phi _ {i,j} ( d _ 0 ) ) ] \\ & - \\sum _ {i} (q _ i - 1) \\sum _ d b _ i (d) [ \\ln ( b _ i ( d ) ) - \\ln( g ( d ) ) ] \\end{array} $$ Where $\\alpha$ is a normalization constant, $q_i$ is the degree of the MRF variable at site $i$ and $f$, $g$, $b$ etc. are as above (pair wise function, singleton function, belief, etc.). Imposing the constraints: $$ \\begin{array}{l} \\sum _ d b _ i (d) = 1 \\ \\sum _ { d _ 0 } b _ {i,j} ( d _ 0, d _ 1 ) = b _ j ( d _ 1 ) \\end{array} $$ Allows us to use Lagrange multipliers $\\lambda _ {i,j} ( d )$ for the $\\sum _ { d _ 0 } b _ {i,j} ( d _ 0, d _ 1 ) = b _ j ( d _ 1 )$ constraint, $\\gamma _ i$ for the $\\sum _ d b _ i (d) = 1$ constraint and $\\gamma _ {i,j}$ for the $\\sum _ {d} \\mu _ {i,j} (d) = 1$ constraint: $$ \\begin{array}{ll} L = & F _ {\\beta} \\ & - \\lambda _ {i,j} (d_1) \\left[ \\left( \\sum _ { d _ 0 } b _ {i,j} ( d _ 0, d _ 1 ) \\right) - b _ j ( d _ 1 ) \\right] \\ & - \\gamma _ i \\left[ \\sum _ d b _ i (d) - 1 \\right] \\ & - \\gamma _ {i,j} \\left[ \\sum _ {d} \\mu _ {i,j} (d) - 1 \\right] \\ \\end{array} $$ $$ \\begin{array}{rl} \\frac{ \\partial L }{ \\partial b _ {i,j} ( d _ 0, d _ 1 ) } = & 0 \\ \\to & \\ln b _ {i,j} (d _ 0, d _ 1) = \\ln( \\phi _ {i,j} (d _ 0, d _ 1) ) + \\lambda _ {i,j}(d _ 1) + \\lambda _ {j,i}(d _ 0) + \\gamma _ {i,j} - 1 \\ \\frac{ \\partial L }{ \\partial b _ i (d _ 0) } = & 0 \\ \\to & (q _ i - 1)( \\ln b _ i ( d _ 0 ) + 1) = \\ln g ( d _ 0) + \\sum _ {j \\in N(i) } \\lambda _ {j,i} (d _ 1) + \\gamma _ i \\ \\lambda _ {i,j} ( d _ 1 ) = & \\ln \\prod _ {k \\in N(j) / i } \\mu _ {k,j} ( d _ 1 ) \\ \\to & ??? \\end{array} $$ Sum-Product Belief Propagation $$ G(V,E) $$ $$ s, t \\in V, x_{*} \\in S = { s_0, s_1, \\cdots, s_{n-1} } $$ $$ \\mu_{s \\to t}(x_t) = \\sum_{x_s} [ \\phi_{s t}(x_s,x_t) \\prod_{u \\in N(s) / t} \\mu_{u \\to s} (x_s) ] $$ $$ b_t(x_t) \\propto \\prod_{s \\in N(t)} \\mu_{t \\to s}(x_s) $$ $s,t$ - verticies in the graph ($s,t \\in V$) $x_s, x_t \\in S$ - values at the vertices (one of a discrete set of values from $S = { s_0, s_1, \\cdots, s_{n-1} }$) $N(s)$ - neighbors of vertex $s$ $N(s) / t$ - neighbors of vertex $s$ excluding vertex $t$ $\\phi_{s t}(x_s,x_t)$ - probability/weight of finding value $x_s$ at position $s$ next to value $x_t$ at position $t$ $\\mu_{s \\to t}(x_t)$ - probability/message from position vertex $s$ that value $x_t$ at vertex $t$ is allowed $b_t(x_t)$ - \"belief\" of value $x_t$ at position $t$ (\"belief\" that vertex $t$ holds value $x_t$) A - B - C | | D - E Literature Notes This is a bit outside the scope of this document but I should take some notes on the various \"state of the art\" techniques from about a decade or more ago. Fast Belief Propagation for Early Vision paper talk The basic idea is that for problems with structure, many speedups can be had exploiting the symmetry or simplicity of the label-to-label cost function, locality of labels or other factors. The paper and talk focus on using loopy belief propagation (LBP) for stereo problems. Using linear or quadratic (truncated) label cost functions, you can go from $O(B^2)$ to $O(B)$ by various tricks Using a virtual hierarchy of nodes, you can compute initial messages and beliefs and then propagate those out to the nodes underneath (wholesale) to get faster convergence A point that doesn't really seem to be addressed is that this assumes the labels have locality or \"cohesion\" in that if you find one label somewhere, the chance of finding a similar label nearby is higher. This assumption is obvious if you're doing stereo matching or motion estimation but for general problems this is not the case and it's not clear that this method will do better (and might even perform worse?). Some other random notes: Can do 'checkerboard' updates to get half the memory and twice the speed with similar or the same convergence (guaranteed? empirical?) Other methods that try to make 'superblocks' change the graph and potentially reduce the node and edge count but at the cost of exploding the label state space, which becomes a Cartesian product. Huttenlocher talks about how the superblocks (generalized belief propagation (GBP)?) idea wasn't meant to make it more efficient but was done for other, theoretical, reasons Scalable detection of statistically significant communities and hierarchies, using message passing for modularity talk paper Focused Belief Propagation for Query-Specific Inference paper This builds on an idea of residual belief propagation (RBP) by Elidan et all that updates only one message per time step based on the difference of the messages, called the \"residual\". The idea is that one can weight message updates by a better heuristic than the residual, namely a \"path sensitivity\". The path sensitivity attempts to measure the effect of changing/updating one message on another and then picking a message to update that has maximal path sensitivity. Doing this wholesale and in general is as bad or worse then just running LBP but various heuristics can be used to estimate the path sensitivity which are more efficient than a wholesale recalculation. Other tricks need to be employed in order to make the algorithm \"anytime\", where an \"anytime\" algorithm can be stopped at anytime and still get a good estimate of the answer (maximum a posteriori (MAP) or distribution on end state). Parallel Splash Belief Propagation paper talk The idea is to create a minimum spanning tree (MST) to schedule the BP messages. Speculation Some thoughts on how to add heuristics or optimizations. These are completely speculative and untested so should be thought of as rough notes or thoughts. Consider a tileset consisting of three groups, which will be called 'red', 'green' and 'blue'. Each group has the same type of tiles (endpoint, road, bend) but can only attach to other tiles of its same color. A constrained grid is created by putting two endpoints of each color on either side of the grid and removing all endpoints from the rest of the grid, forcing a condition where only a single road from one endpoint color to its corresponding endpoint colored tile is a feasible solution. In other words, try to find a solution of three self avoiding walks with giving endpoints. BP has no concept of \"self avoiding walk\" (SAW) and the fact that these are SAWs is a layer of interpretation we impose after the fact. The SAWs are embedded in the tiles present in the grid and the admissible tile pairs from the $f(\\cdot)$ function. For convenience, we might talk about the SAW as a convenience with the understanding that the SAW itself is not present as a concept in BP. One observation is that if the endpoints are too far apart, the SAW meanders from one of it's initial position. There looks to be a weak \"pull\" in the direction of the endpoint, which gets stronger as the grid gets filled out with choice, but the meandering can also lead to the SAW backing itself into a dead-end and failing to find a realization. In tests, setting the error to be extremely low (thus increasing the simulation steps) the realization is more consistently found (in the test case of 11x11x3 ). Were the grid larger, no doubt the meandering would not be as bad as, by chance, the SAWs might have a reasonable chance of connecting or, when they get close to each other, might be close enough to have BP kick in and nudge them towards each other. Though I lack the language or understanding to quantify what's going on well, it looks like there's a kind of \"mean field\" quality to BP, where it's good at understanding what the constraints should be in a kind of \"average\" sense (for some definition of \"average) with it potentially understanding and overcoming some local constraints, but it loosing its context on constraints or correlations that happen over larger distances. To overcome this effect, one can try and come up with heuristics that allow for those longer length constraints to be more apparent, especially when they're so strong as to lead to contradictions if they're not met. One idea is to chop up sections grid (in 3x3x3 sections, say) with outside boundary conditions on all but one edge, join them to another chopped section and see if a solution can be found (by running BP, sampling with MCMC or some other method). If a solution is always/almost always found and not found either when attached to other segments or in isolation, then use that information to try and correlate \"key\" tiles inside. This correlation can be used to modify the single tile probability function ( $g (\\cdot )$ ) do heuristically prefer these tiles when running BP proper. As an example, consider this ASCII map: , , , , , . , , , , , , S , , , . , , , E , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , [.-] , , , , , , , , , , . , , , , , Where . is the empty tile, , has choices of the empty, bend or road tiles, S , E are the start/end tiles (suitably rotated) and [.-] have only two choices, either the road or empty tile. So the only solution is a SAW from S to E through [.-] , with the choice of - needed to not end up in a contradiction. So as a guess, let's try using a 3x3 grid around the S tile and pairing it with some of the other chopped sections: Configuration SE : . . . . . . . . . , , , , , , . . , S , , E , . . , , , , , , . . . . . . . . . Configuration SB : . . . . . . . . . , , , , , , . . , S , , , , . . , , , , , , . . . . . . . . . Configuration SB+ : . . . . . . . . . , , , , , , , . , S , , , , , . , , , , , , , . . . . . . . . Configuration SM : . . . . . . . . . , , , , , , . . , S , , [.-] , . . , , , , , , . . . . . . . . . Configuration SM+ : . . . . . . . . . , , , , , , , . , S , , [.-] , , . , , , , , , , . . . . . . . . Configuration SW+ : . . . . . . . . . , , , , . , , . , S , , . , , . , , , , . , , . . . . . . . . So now we observe that SE has a valid realization whereas SB doesn't. SB+ has a valid realization that falls off the right end. SM fails whereas SM+ succeeds as in the SB+ case, with the path continuing over the right edge. SW+ also can't find a realization, even though the right hand side of the W tile is open. If we're able to do this 'blocking' and do some cataloging of which blocks can be joined together, with various boundary conditions set, we might be able to deduce some tiles are heavily dependent on others. There are many assumptions underneath this, some apparent and some potentially less so. One is that there's a kind symmetry, either translational or scale, that underlies some of these assumptions. Another is that some simple path heuristic will help, or that the 'meandering' nature of the search is something to mitigate against. One thought is to extend a minimum spanning tree from a particular point (say S ) through the whole grid, not allowing it to pass through cells that are fixed or have tiles that aren't from some set of admissible values (determined somehow by the correlation 'chopping' above?). One can then run BP on this implied tree directly and/or somehow use the MST to modulate the probabilities of the admissible set. Min-cut of the graph also gives some indication of the importance of certain cells. Some more off-the-cuff remarks. We might care about \"topological\" ordering of critical tiles or regions. We can potentially get away with looking at \"critical\" tiles or regions because of some underlying symmetry that allows us to compress the spatial region between these areas of interest. For example, in the above, we might care about an ordering that puts S - [.-] - E , potentially even S > [.-] > E or S < [.-] < E if we're clever. We might notice that the S , [.-] and E tiles/cells are the \"critical\" ones (that is, they're highly correlated and/or dependent on one another for a successful realization) and that the regions between the S , [.-] pair and the [.-] , E pair have a kind of translational symmetry in that all tiles between them are homogeneous and so can be represented by an exemplar tile/cell. With this reduction, we might be able to solve a smaller problem, something like S , [.-] , E , deducing the [.-] should be - and then backing out and filling in the larger grid. Some examples to consider: , , , , , . , , , , , , S0 , , , [.-_] , , , E1 , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , S1 , , , [.-_] , , , E0 , , , , , , . , , , , , Where - and _ are roads for 0 and 1 groups respectively. This may be too restrictive, even if still possible, so one could imagine a third dimension to allow the paths to \"jump\" over each other in the extra dimension. , , , . , . , , , , , , S , . , [.-_] , , , E , , , , . , . , , , , , , , , . , . , , , , , , , , . , . . . , , , , , , . , , , . , , , , , , . , , , . , , , , , , . . . , . , , , , , , . . . , . , , , , , , [.-] , , , . , , , , , , . , , , . , , , Where the path has to loop \"back\" before going through the other choke point. Assuming one can identify \"constrained\" tiles, then doing all pairs shortest path, by some energy metric to extend paths, is at least a proof of concept to allow a topological graph to be superimposed over the tiles in question. So, as a rough first pass at a heuristic extension: Chop up the grid into larger blocks, 3x3x3 say: shove blocks together with various boundary conditions and run a small BP algorithm on them to determine which blocks are admissible and get an indication of which tiles within the block are dependent/correlated with each other From the list of flagged tiles (tiles that are either constrained, flagged as important or whatever else), run an all pairs shortest path, choosing some \"background\" tile as the 0 energy cost path (for example, the , tile above) From the all pairs shortest path, construct an induced graph that gives a topological description of which tiles are neighbors From the topological induced graph, run BP, under some canonical cell construction or description, to determine, either directly or by weighting the individual tile probability, which tiles should be chosen from the constrained set. Appendix Mutual information $$ \\begin{align} I(X;Y) & = D_{KL}(p_{X,Y} || p_X \\cdot p_Y ) \\ & = \\sum_{x \\in X} \\sum_{y \\in Y} P_{X,Y}(x,y) \\ln( \\frac{P_{X,Y}(x,y)}{P_X(x) P_Y(y)} ) \\end{align} $$ For $p_X$ and $p_y$ independent, this reduces to: $$ \\sum_{x \\in X} \\sum_{y \\in Y} P_{X,Y}(x,y) \\ln( \\frac{P_X(x) P_Y(y)}{P_X(x) P_Y(y)} ) = 0 $$ For $X = Y$, this reduces to: $$ \\begin{align} & \\sum_{x \\in X} \\sum_{y \\in X} P_{X,X}(x,y) \\ln( \\frac{P_{X,X}(x,y) }{P_X(x) P_X(y)} ) \\ = & \\sum_{x \\in X} P_X(x) \\ln( \\frac{P_X(x) }{P_X(x) P_X(x)} ) \\ = & - \\sum_{x \\in X} P_X(x) \\ln( P_X(x) ) \\ \\end{align} $$ which is just the entropy of $X$. Free Energy Consider ... Belief Propagation on Trees Belief propagation is exact on trees. The algorithm is roughly as follows: Assign a privileged non-leaf node as the root Starting from the leaf nodes inwards to the root, calculate the marginal probability in a breadth first seach way Once the maximum a posteriori (MAP) is calculated for the root node, work outward from the root back to the leaves to calculate the true marginals The analogy to string matching is that to find a concrete alignment, one must first find the global score, then work backwards from the score to find a string alignment. In this case, BP on a tree must find the MAP and then work backwards, filling in the individual variable estimates once the MAP has been calculated. Call the join probability distribution function $f(\\cdot,\\cdot)$ with each node taking on one of $D$ possible values. For concreteness, the joint distribution is the same regardless of the node pairs in question (sort of a homogeniety condition), with each node drawn from the same domain of integers from $[0, D-1]$. Starting at the leaves and working inwards, we introduce temporary functions $u_i(\\cdot)$: $$ \\begin{align} u_0 ( v_6 ) & = \\sum _ { v_0 = 0 } ^ { D - 1 } f( v_0, v_6 ) \\ u_1 ( v_6 ) & = \\sum _ { v_1 = 0 } ^ { D - 1 } f( v_1, v_6 ) \\ u_2 ( v_5 ) & = \\sum _ { v_2 = 0 } ^ { D - 1 } f( v_2, v_5 ) \\ u_3 ( v_5 ) & = \\sum _ { v_3 = 0 } ^ { D - 1 } f( v_2, v_5 ) \\ u_4 ( v_5 ) & = \\sum _ { v_4 = 0 } ^ { D - 1 } f( v_4, v_5 ) \\ u_6 ( v_7 ) & = \\sum _ { v_6 = 0 } ^ { D - 1 } f( v_6, v_7 ) \\cdot u_0(v_6) \\cdot u_1(v_6) \\ u_5 ( v_7 ) & = \\sum _ { v_5 = 0 } ^ { D - 1 } f( v_5, v_7 ) \\cdot u_3(v_5) \\cdot u_4(v_5) \\ u_7 & = \\sum _ { v_7 = 0 } ^ { D - 1 } u_6(v_7) \\cdot u_5(v_7) \\end{align} $$ $u_7$ now holds the calculated MAP for the example graph. Once we have the MAP for $u_7$, and thus the whole graph, we can then use it to inform the rest of the nodes by References Island algorithm Tree decomposition / junction tree / clique tree / join tree Belief Propagation Splash Belief Propagation Relax, Compensate, Recover BP Misc Notes Arborescence Chu-Liu/Edmond's Algorithm 2022-08-16","title":"Belief Propagation"},{"location":"Belief-Propagation.html#thanks-belief-propagation","text":"","title":"Thanks. Belief Propagation"},{"location":"Belief-Propagation.html#introduction","text":"For a set of random variables, ${ \\bf x } = ( x_0, x_1, x_2, \\cdots, x_{ n-1 } )$, and a given probability distribution over those random variables, $p( { \\bf x } )$, we might like to know what the most likely configuration is. Another name for the most likely configuration is the maximum a posteriori (MAP) assignment of the variables over their domain. Sometimes there is additional structure on the random variables so that the probability function can be decomposed into a product of distributions over the subsets of variables. If the additional structure is encapsulated in a graphical model, connecting variables with an edge and corresponding distribution function, this this can be represented as a Markov Random Field (MRF), with a set of cliques, represented in a graph structure, whose probability distribution function can be written as: $$ \\begin{align} p( { \\bf x } ) & = \\frac{1}{Z} \\prod_{c \\in \\text{clique(} G \\text{)} } \\phi_c ( { \\bf x }_c ) \\end{align} $$ A marginal distribution is the probability of fixing a subset of the random variables over all possible values of the remaining variables. That is: $$ p( { \\bf x } _ s = { \\bf d } _ s ) = \\sum_{ { \\bf x } / x_s } \\text{ } \\prod_{ c \\in \\text{ clique( } G \\text{ ) } } \\phi_c ( { \\bf x } _ c ) $$ Where the term on the right fixes the values of ${ \\bf x }_s$ where appropriate. This problem is NP-Complete in general. If the form of the graphical model is a tree, then the marginals can be computed efficiently with a dynamic programming like algorithm. A dynamic programming like algorithm can be adapted to be used on non-tree like graphs and can be expected to work for graphs that have certain restrictions on their topology or structure. Since the graph now has loops, the independence property that was needed to make the algorithm efficient is violated. The dynamic programming like algorithm is called Belief Propagation (BP), sometimes called Loopy Belief Propagation (LBP) for non-tree like graphs. For non-tree graphs, the term \"probability\" is substituted with \"messages\", as calculations are no longer probability distributions any more, with the underlying algorithm for BP described as a \"message passing\" algorithm. The type of structure on the graphs that lead to proper marginal discovery is not well understood in general and certainly not understood by me. Some clue as to what the structure is comes from the degree of independence of the variables and a somewhat hand-waivy heuristic is that if graphs look locally \"tree-like\" then BP (or LBP) has some expectation of converging to a correct solution. There are three main graph models that belief propagation looks to be run on: Bayesian Network Factor Graph Markov Random Fields (MRF)","title":"Introduction"},{"location":"Belief-Propagation.html#markov-random-field","text":"Finite Markov Random Fields (MRF) will be considered. $$ G(V,E), \\ \\ \\ |V| = n, \\ \\ \\ x_i \\in D = { d_0, d_1, \\cdots, d_{m-1} } $$ $$ i \\in V \\to g_i(\\cdot) \\in \\mathbb{R} $$ $$ (i,j) \\in E \\to f_{i,j}( \\cdot, \\cdot ) \\in \\mathbb{R} $$ $x_i$ represents the value of vertex $i$. $g_i(x_i)$ is the mapping of vertex $i$ with value $x_i$ $f_{i,j}(x_i,x_j)$ is the mapping of the connected vertex $i$ and $j$ with values $x_i$ and $x_j$, respectively For example, $f_{i,j}(x_i,x_j)$ could be an indicator function that vertex $i$ and $j$ could have values $x_i$ and $x_j$.","title":"Markov Random Field"},{"location":"Belief-Propagation.html#belief-propagation-on-a-discrete-markov-random-field","text":"::mermaid graph LR u((u)) & v((v)) --> i((i)) -.-> j((j))) $$ \\mu_{i,j}(d) = \\sum_{a \\in D} f_{u,i}(a) \\cdot f_{v,i}(a) \\dots $$ Each vertex, $i$, can be associated with a random variable, $X_i$, taking on (discrete) values chosen from some domain $D = { d_0, d_1, \\cdots, d_{m-1} }$ with a probability distribution function $g_i(\\cdot)$. $$ \\mu_{i,j}^{t+1}(d) = \\sum_{a \\in D} f_{i,j}(a,d) \\cdot g_i(a) \\cdot \\prod_{k \\in N(i) \\text{ \\\\ } j} \\mu_{k,i}^{t}(a) $$ $$ P(X_i = a) \\approx b^t_i(a) \\propto g_i(a) \\cdot \\prod_{k \\in N(i)} \\mu^t_{k,i}(a) $$ $$ \\sum_{b \\in D} \\mu_{i,j}^{t}(d) = 1, \\ \\ \\ \\ \\sum_{a \\in D} b^t_i(a) = 1 $$ The product can be more compactly represented by a function $h^t_{i,j}(\\cdot)$: $$ h^t_{i,j}(a) = g_i(a) \\cdot \\prod_{k \\in N(i) \\text{\\\\} j } \\mu^t_{k,i}(a) $$ $$ \\mu_{i,j}^{t+1}(d) = \\sum_{a \\in D} f_{i,j}(a,d) \\cdot h^t_{i,j}(a) $$ One can recast this as a matrix multiplication: $$ \\begin{bmatrix} f_{i,j}(d_0,d_0) & f_{i,j}(d_1,d_0) & \\cdots & f_{i,j}(d_{m-1},d_0) \\\\ f_{i,j}(d_0,d_1) & f_{i,j}(d_1,d_1) & \\cdots & f_{i,j}(d_{m-1},d_1) \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\\\ f_{i,j}(d_0,d_{m-1}) & f_{i,j}(d_1,d_{m-1}) & \\cdots & f_{i,j}(d_{m-1},d_{m-1}) \\end{bmatrix} \\begin{bmatrix} h_{i,j}^{t}(d_0) \\\\ h_{i,j}^{t}(d_1) \\\\ \\vdots \\\\ h_{i,j}^{t}(d_{m-1}) \\end{bmatrix} = \\begin{bmatrix} \\mu_{i,j}^{t+1}(d_0) \\\\ \\mu_{i,j}^{t+1}(d_1) \\\\ \\vdots \\\\ \\mu_{i,j}^{t+1}(d_{m-1}) \\end{bmatrix} $$ $$ \\to F_{i,j} \\cdot \\vec{h}^t_{i,j} = \\vec{\\mu}^{t+1}_{i,j} $$ Since $h^t_{i,j}(\\cdot)$ has no dependence on $b$, this speeds up a naive calculation by re-using the product results instead of re-calculating them. If the $F_{i,j}$ matrix has low rank, $r < m$, it can be factored into a singular value decomposition (SVD) for performance: $$U \\cdot S \\cdot V = \\begin{bmatrix} \\vec u_0 & \\vec u_1 & \\cdots & \\vec u_{r-1} \\end{bmatrix} \\begin{bmatrix} s_0 & 0 & \\cdots & 0 \\\\ 0 & s_1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & s_{r-1} \\end{bmatrix} \\begin{bmatrix} \\vec{v}^\\dagger_0 \\\\ \\vec{v}^\\dagger_1 \\\\ \\vdots \\\\ \\vec{v}_{r-1}^\\dagger \\end{bmatrix} $$ Where $F_{i,j} = U \\cdot S \\cdot V$. The matrix multiplication that was $O(m^2)$ now becomes two matrix multiplications of order $O(r \\cdot m)$ for a potential speedup of $\\sim \\frac{m}{r}$. Note that if the $F$ matrix has any symmetry, there are potential efficiency gains by exploiting that symmetry. In the above, when $F$ is of low rank, the SVD is exploiting the \"symmetry' of having many zero eigenvalues. There is a well known optimization if the implied function has certain translational symmetry properties, allowing the multiplication to be done via a fast Fourier transform, producing an $O(m \\lg m)$ calculation over the $O(m^2)$.","title":"Belief Propagation on a (discrete) Markov Random Field"},{"location":"Belief-Propagation.html#bethe-free-energy","text":"(wip) Belief Propagation (BP) can be thought of as an algorithm that finds a fixed point of the Bethe Free Energy, $F_\\beta$: $$ \\begin{array}{ll} \\phi _ {i,j} (d _ 0, d _ 1 ) = & f ( d _ 0, d _ 1 ) g ( d _ 0 ) g ( d _ 1 ) \\ b _ {i,j} (d _ 0,d _ 1) = & \\alpha \\phi _ {i,j} ( d _ 0, d _ 1 ) \\prod _ { k \\in N(i) / j } \\mu _ {k,i} ( d_0) \\prod _ { l \\in N(j) / i } \\mu _ {l,j} (d _ 1) \\ F _ {\\beta} (f, g) = & \\sum _ {i,j} \\sum _ {d _ 0, d _ 1} b _ {i,j} ( d _ 0, d _ 1 ) [ \\ln(b _ {i,j} (d _ 0, d _ 1 )) - \\ln( \\phi _ {i,j} ( d _ 0 ) ) ] \\ & - \\sum _ {i} (q _ i - 1) \\sum _ d b _ i (d) [ \\ln ( b _ i ( d ) ) - \\ln( g ( d ) ) ] \\end{array} $$ Where $\\alpha$ is a normalization constant, $q_i$ is the degree of the MRF variable at site $i$ and $f$, $g$, $b$ etc. are as above (pair wise function, singleton function, belief, etc.). Imposing the constraints: $$ \\begin{array}{l} \\sum _ d b _ i (d) = 1 \\ \\sum _ { d _ 0 } b _ {i,j} ( d _ 0, d _ 1 ) = b _ j ( d _ 1 ) \\end{array} $$ Allows us to use Lagrange multipliers $\\lambda _ {i,j} ( d )$ for the $\\sum _ { d _ 0 } b _ {i,j} ( d _ 0, d _ 1 ) = b _ j ( d _ 1 )$ constraint, $\\gamma _ i$ for the $\\sum _ d b _ i (d) = 1$ constraint and $\\gamma _ {i,j}$ for the $\\sum _ {d} \\mu _ {i,j} (d) = 1$ constraint: $$ \\begin{array}{ll} L = & F _ {\\beta} \\ & - \\lambda _ {i,j} (d_1) \\left[ \\left( \\sum _ { d _ 0 } b _ {i,j} ( d _ 0, d _ 1 ) \\right) - b _ j ( d _ 1 ) \\right] \\ & - \\gamma _ i \\left[ \\sum _ d b _ i (d) - 1 \\right] \\ & - \\gamma _ {i,j} \\left[ \\sum _ {d} \\mu _ {i,j} (d) - 1 \\right] \\ \\end{array} $$ $$ \\begin{array}{rl} \\frac{ \\partial L }{ \\partial b _ {i,j} ( d _ 0, d _ 1 ) } = & 0 \\ \\to & \\ln b _ {i,j} (d _ 0, d _ 1) = \\ln( \\phi _ {i,j} (d _ 0, d _ 1) ) + \\lambda _ {i,j}(d _ 1) + \\lambda _ {j,i}(d _ 0) + \\gamma _ {i,j} - 1 \\ \\frac{ \\partial L }{ \\partial b _ i (d _ 0) } = & 0 \\ \\to & (q _ i - 1)( \\ln b _ i ( d _ 0 ) + 1) = \\ln g ( d _ 0) + \\sum _ {j \\in N(i) } \\lambda _ {j,i} (d _ 1) + \\gamma _ i \\ \\lambda _ {i,j} ( d _ 1 ) = & \\ln \\prod _ {k \\in N(j) / i } \\mu _ {k,j} ( d _ 1 ) \\ \\to & ??? \\end{array} $$","title":"Bethe Free Energy"},{"location":"Belief-Propagation.html#sum-product-belief-propagation","text":"$$ G(V,E) $$ $$ s, t \\in V, x_{*} \\in S = { s_0, s_1, \\cdots, s_{n-1} } $$ $$ \\mu_{s \\to t}(x_t) = \\sum_{x_s} [ \\phi_{s t}(x_s,x_t) \\prod_{u \\in N(s) / t} \\mu_{u \\to s} (x_s) ] $$ $$ b_t(x_t) \\propto \\prod_{s \\in N(t)} \\mu_{t \\to s}(x_s) $$ $s,t$ - verticies in the graph ($s,t \\in V$) $x_s, x_t \\in S$ - values at the vertices (one of a discrete set of values from $S = { s_0, s_1, \\cdots, s_{n-1} }$) $N(s)$ - neighbors of vertex $s$ $N(s) / t$ - neighbors of vertex $s$ excluding vertex $t$ $\\phi_{s t}(x_s,x_t)$ - probability/weight of finding value $x_s$ at position $s$ next to value $x_t$ at position $t$ $\\mu_{s \\to t}(x_t)$ - probability/message from position vertex $s$ that value $x_t$ at vertex $t$ is allowed $b_t(x_t)$ - \"belief\" of value $x_t$ at position $t$ (\"belief\" that vertex $t$ holds value $x_t$) A - B - C | | D - E","title":"Sum-Product Belief Propagation"},{"location":"Belief-Propagation.html#literature-notes","text":"This is a bit outside the scope of this document but I should take some notes on the various \"state of the art\" techniques from about a decade or more ago.","title":"Literature Notes"},{"location":"Belief-Propagation.html#fast-belief-propagation-for-early-vision","text":"paper talk The basic idea is that for problems with structure, many speedups can be had exploiting the symmetry or simplicity of the label-to-label cost function, locality of labels or other factors. The paper and talk focus on using loopy belief propagation (LBP) for stereo problems. Using linear or quadratic (truncated) label cost functions, you can go from $O(B^2)$ to $O(B)$ by various tricks Using a virtual hierarchy of nodes, you can compute initial messages and beliefs and then propagate those out to the nodes underneath (wholesale) to get faster convergence A point that doesn't really seem to be addressed is that this assumes the labels have locality or \"cohesion\" in that if you find one label somewhere, the chance of finding a similar label nearby is higher. This assumption is obvious if you're doing stereo matching or motion estimation but for general problems this is not the case and it's not clear that this method will do better (and might even perform worse?). Some other random notes: Can do 'checkerboard' updates to get half the memory and twice the speed with similar or the same convergence (guaranteed? empirical?) Other methods that try to make 'superblocks' change the graph and potentially reduce the node and edge count but at the cost of exploding the label state space, which becomes a Cartesian product. Huttenlocher talks about how the superblocks (generalized belief propagation (GBP)?) idea wasn't meant to make it more efficient but was done for other, theoretical, reasons","title":"Fast Belief Propagation for Early Vision"},{"location":"Belief-Propagation.html#scalable-detection-of-statistically-significant-communities-and-hierarchies-using-message-passing-for-modularity","text":"talk paper","title":"Scalable detection of statistically significant communities and hierarchies, using message passing for modularity"},{"location":"Belief-Propagation.html#focused-belief-propagation-for-query-specific-inference","text":"paper This builds on an idea of residual belief propagation (RBP) by Elidan et all that updates only one message per time step based on the difference of the messages, called the \"residual\". The idea is that one can weight message updates by a better heuristic than the residual, namely a \"path sensitivity\". The path sensitivity attempts to measure the effect of changing/updating one message on another and then picking a message to update that has maximal path sensitivity. Doing this wholesale and in general is as bad or worse then just running LBP but various heuristics can be used to estimate the path sensitivity which are more efficient than a wholesale recalculation. Other tricks need to be employed in order to make the algorithm \"anytime\", where an \"anytime\" algorithm can be stopped at anytime and still get a good estimate of the answer (maximum a posteriori (MAP) or distribution on end state).","title":"Focused Belief Propagation for Query-Specific Inference"},{"location":"Belief-Propagation.html#parallel-splash-belief-propagation","text":"paper talk The idea is to create a minimum spanning tree (MST) to schedule the BP messages.","title":"Parallel Splash Belief Propagation"},{"location":"Belief-Propagation.html#speculation","text":"Some thoughts on how to add heuristics or optimizations. These are completely speculative and untested so should be thought of as rough notes or thoughts. Consider a tileset consisting of three groups, which will be called 'red', 'green' and 'blue'. Each group has the same type of tiles (endpoint, road, bend) but can only attach to other tiles of its same color. A constrained grid is created by putting two endpoints of each color on either side of the grid and removing all endpoints from the rest of the grid, forcing a condition where only a single road from one endpoint color to its corresponding endpoint colored tile is a feasible solution. In other words, try to find a solution of three self avoiding walks with giving endpoints. BP has no concept of \"self avoiding walk\" (SAW) and the fact that these are SAWs is a layer of interpretation we impose after the fact. The SAWs are embedded in the tiles present in the grid and the admissible tile pairs from the $f(\\cdot)$ function. For convenience, we might talk about the SAW as a convenience with the understanding that the SAW itself is not present as a concept in BP. One observation is that if the endpoints are too far apart, the SAW meanders from one of it's initial position. There looks to be a weak \"pull\" in the direction of the endpoint, which gets stronger as the grid gets filled out with choice, but the meandering can also lead to the SAW backing itself into a dead-end and failing to find a realization. In tests, setting the error to be extremely low (thus increasing the simulation steps) the realization is more consistently found (in the test case of 11x11x3 ). Were the grid larger, no doubt the meandering would not be as bad as, by chance, the SAWs might have a reasonable chance of connecting or, when they get close to each other, might be close enough to have BP kick in and nudge them towards each other. Though I lack the language or understanding to quantify what's going on well, it looks like there's a kind of \"mean field\" quality to BP, where it's good at understanding what the constraints should be in a kind of \"average\" sense (for some definition of \"average) with it potentially understanding and overcoming some local constraints, but it loosing its context on constraints or correlations that happen over larger distances. To overcome this effect, one can try and come up with heuristics that allow for those longer length constraints to be more apparent, especially when they're so strong as to lead to contradictions if they're not met. One idea is to chop up sections grid (in 3x3x3 sections, say) with outside boundary conditions on all but one edge, join them to another chopped section and see if a solution can be found (by running BP, sampling with MCMC or some other method). If a solution is always/almost always found and not found either when attached to other segments or in isolation, then use that information to try and correlate \"key\" tiles inside. This correlation can be used to modify the single tile probability function ( $g (\\cdot )$ ) do heuristically prefer these tiles when running BP proper. As an example, consider this ASCII map: , , , , , . , , , , , , S , , , . , , , E , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , [.-] , , , , , , , , , , . , , , , , Where . is the empty tile, , has choices of the empty, bend or road tiles, S , E are the start/end tiles (suitably rotated) and [.-] have only two choices, either the road or empty tile. So the only solution is a SAW from S to E through [.-] , with the choice of - needed to not end up in a contradiction. So as a guess, let's try using a 3x3 grid around the S tile and pairing it with some of the other chopped sections: Configuration SE : . . . . . . . . . , , , , , , . . , S , , E , . . , , , , , , . . . . . . . . . Configuration SB : . . . . . . . . . , , , , , , . . , S , , , , . . , , , , , , . . . . . . . . . Configuration SB+ : . . . . . . . . . , , , , , , , . , S , , , , , . , , , , , , , . . . . . . . . Configuration SM : . . . . . . . . . , , , , , , . . , S , , [.-] , . . , , , , , , . . . . . . . . . Configuration SM+ : . . . . . . . . . , , , , , , , . , S , , [.-] , , . , , , , , , , . . . . . . . . Configuration SW+ : . . . . . . . . . , , , , . , , . , S , , . , , . , , , , . , , . . . . . . . . So now we observe that SE has a valid realization whereas SB doesn't. SB+ has a valid realization that falls off the right end. SM fails whereas SM+ succeeds as in the SB+ case, with the path continuing over the right edge. SW+ also can't find a realization, even though the right hand side of the W tile is open. If we're able to do this 'blocking' and do some cataloging of which blocks can be joined together, with various boundary conditions set, we might be able to deduce some tiles are heavily dependent on others. There are many assumptions underneath this, some apparent and some potentially less so. One is that there's a kind symmetry, either translational or scale, that underlies some of these assumptions. Another is that some simple path heuristic will help, or that the 'meandering' nature of the search is something to mitigate against. One thought is to extend a minimum spanning tree from a particular point (say S ) through the whole grid, not allowing it to pass through cells that are fixed or have tiles that aren't from some set of admissible values (determined somehow by the correlation 'chopping' above?). One can then run BP on this implied tree directly and/or somehow use the MST to modulate the probabilities of the admissible set. Min-cut of the graph also gives some indication of the importance of certain cells. Some more off-the-cuff remarks. We might care about \"topological\" ordering of critical tiles or regions. We can potentially get away with looking at \"critical\" tiles or regions because of some underlying symmetry that allows us to compress the spatial region between these areas of interest. For example, in the above, we might care about an ordering that puts S - [.-] - E , potentially even S > [.-] > E or S < [.-] < E if we're clever. We might notice that the S , [.-] and E tiles/cells are the \"critical\" ones (that is, they're highly correlated and/or dependent on one another for a successful realization) and that the regions between the S , [.-] pair and the [.-] , E pair have a kind of translational symmetry in that all tiles between them are homogeneous and so can be represented by an exemplar tile/cell. With this reduction, we might be able to solve a smaller problem, something like S , [.-] , E , deducing the [.-] should be - and then backing out and filling in the larger grid. Some examples to consider: , , , , , . , , , , , , S0 , , , [.-_] , , , E1 , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , , , , , . , , , , , , S1 , , , [.-_] , , , E0 , , , , , , . , , , , , Where - and _ are roads for 0 and 1 groups respectively. This may be too restrictive, even if still possible, so one could imagine a third dimension to allow the paths to \"jump\" over each other in the extra dimension. , , , . , . , , , , , , S , . , [.-_] , , , E , , , , . , . , , , , , , , , . , . , , , , , , , , . , . . . , , , , , , . , , , . , , , , , , . , , , . , , , , , , . . . , . , , , , , , . . . , . , , , , , , [.-] , , , . , , , , , , . , , , . , , , Where the path has to loop \"back\" before going through the other choke point. Assuming one can identify \"constrained\" tiles, then doing all pairs shortest path, by some energy metric to extend paths, is at least a proof of concept to allow a topological graph to be superimposed over the tiles in question. So, as a rough first pass at a heuristic extension: Chop up the grid into larger blocks, 3x3x3 say: shove blocks together with various boundary conditions and run a small BP algorithm on them to determine which blocks are admissible and get an indication of which tiles within the block are dependent/correlated with each other From the list of flagged tiles (tiles that are either constrained, flagged as important or whatever else), run an all pairs shortest path, choosing some \"background\" tile as the 0 energy cost path (for example, the , tile above) From the all pairs shortest path, construct an induced graph that gives a topological description of which tiles are neighbors From the topological induced graph, run BP, under some canonical cell construction or description, to determine, either directly or by weighting the individual tile probability, which tiles should be chosen from the constrained set.","title":"Speculation"},{"location":"Belief-Propagation.html#appendix","text":"","title":"Appendix"},{"location":"Belief-Propagation.html#mutual-information","text":"$$ \\begin{align} I(X;Y) & = D_{KL}(p_{X,Y} || p_X \\cdot p_Y ) \\ & = \\sum_{x \\in X} \\sum_{y \\in Y} P_{X,Y}(x,y) \\ln( \\frac{P_{X,Y}(x,y)}{P_X(x) P_Y(y)} ) \\end{align} $$ For $p_X$ and $p_y$ independent, this reduces to: $$ \\sum_{x \\in X} \\sum_{y \\in Y} P_{X,Y}(x,y) \\ln( \\frac{P_X(x) P_Y(y)}{P_X(x) P_Y(y)} ) = 0 $$ For $X = Y$, this reduces to: $$ \\begin{align} & \\sum_{x \\in X} \\sum_{y \\in X} P_{X,X}(x,y) \\ln( \\frac{P_{X,X}(x,y) }{P_X(x) P_X(y)} ) \\ = & \\sum_{x \\in X} P_X(x) \\ln( \\frac{P_X(x) }{P_X(x) P_X(x)} ) \\ = & - \\sum_{x \\in X} P_X(x) \\ln( P_X(x) ) \\ \\end{align} $$ which is just the entropy of $X$.","title":"Mutual information"},{"location":"Belief-Propagation.html#free-energy","text":"Consider ...","title":"Free Energy"},{"location":"Belief-Propagation.html#belief-propagation-on-trees","text":"Belief propagation is exact on trees. The algorithm is roughly as follows: Assign a privileged non-leaf node as the root Starting from the leaf nodes inwards to the root, calculate the marginal probability in a breadth first seach way Once the maximum a posteriori (MAP) is calculated for the root node, work outward from the root back to the leaves to calculate the true marginals The analogy to string matching is that to find a concrete alignment, one must first find the global score, then work backwards from the score to find a string alignment. In this case, BP on a tree must find the MAP and then work backwards, filling in the individual variable estimates once the MAP has been calculated. Call the join probability distribution function $f(\\cdot,\\cdot)$ with each node taking on one of $D$ possible values. For concreteness, the joint distribution is the same regardless of the node pairs in question (sort of a homogeniety condition), with each node drawn from the same domain of integers from $[0, D-1]$. Starting at the leaves and working inwards, we introduce temporary functions $u_i(\\cdot)$: $$ \\begin{align} u_0 ( v_6 ) & = \\sum _ { v_0 = 0 } ^ { D - 1 } f( v_0, v_6 ) \\ u_1 ( v_6 ) & = \\sum _ { v_1 = 0 } ^ { D - 1 } f( v_1, v_6 ) \\ u_2 ( v_5 ) & = \\sum _ { v_2 = 0 } ^ { D - 1 } f( v_2, v_5 ) \\ u_3 ( v_5 ) & = \\sum _ { v_3 = 0 } ^ { D - 1 } f( v_2, v_5 ) \\ u_4 ( v_5 ) & = \\sum _ { v_4 = 0 } ^ { D - 1 } f( v_4, v_5 ) \\ u_6 ( v_7 ) & = \\sum _ { v_6 = 0 } ^ { D - 1 } f( v_6, v_7 ) \\cdot u_0(v_6) \\cdot u_1(v_6) \\ u_5 ( v_7 ) & = \\sum _ { v_5 = 0 } ^ { D - 1 } f( v_5, v_7 ) \\cdot u_3(v_5) \\cdot u_4(v_5) \\ u_7 & = \\sum _ { v_7 = 0 } ^ { D - 1 } u_6(v_7) \\cdot u_5(v_7) \\end{align} $$ $u_7$ now holds the calculated MAP for the example graph. Once we have the MAP for $u_7$, and thus the whole graph, we can then use it to inform the rest of the nodes by","title":"Belief Propagation on Trees"},{"location":"Belief-Propagation.html#references","text":"Island algorithm Tree decomposition / junction tree / clique tree / join tree Belief Propagation Splash Belief Propagation Relax, Compensate, Recover BP","title":"References"},{"location":"Belief-Propagation.html#misc-notes","text":"Arborescence Chu-Liu/Edmond's Algorithm","title":"Misc Notes"},{"location":"Belief-Propagation.html#2022-08-16","text":"","title":"2022-08-16"},{"location":"Bicycle-Safety-Statistics.html","text":"Bicycle Safety Statistics The basic question to answer is how safe is riding a bike compared with driving a car. 37,461 motor deaths (2016) 1.15% to 2.6% fatality rate per 100 million vehicle miles travelled (VMT). Around 2.2M miles (2016) (light duty vehicle). As a check, $ \\frac{37,461 \\text{deaths}}{2.2 \\cdot 10^12 \\text{VMT}} \\approx 1.7% \\text{ per 100M VMT}$ which is within range of the reported statistic above. Pedalcyclist fatality count of 840 (2016). Pedestrian fatality count of 5987 (2016). If we take a rough estimate of the census 786k (pg2 census) bike commuters, assume 2 mile commute to work each way for a total of 4 miles and 5 work days, this leads to $ (786 \\cdot 10^3) \\cdot (\\frac{5}{7}) \\cdot (365) \\cdot (4) \\approx 8.2 \\cdot 10^8 \\text{ bike miles per year}$. This gives the (large) estimate of 8.2% fatality rate per 100M bike miles travelled. If we count by time instead, this is $ 800/(786000 * (46/60) * (5/7) * 365) \\approx 5.1 \\cdot 10^{-6} $ chance per hour of a fatality (bike). 85.4 percent of 150M workers drive to work ( src ). With a guess of a 20 minute commute time, gives 37461/(.854*(150*10^6)*(40/60)*(5/7)*365) or about $1.7 \\cdot 10^{-6}$ chance per hour of a fatality (car). Rough estimate Chicago: $ 2.46 \\cdot 10^6 $ daily minutes spent on bike. x Glossary Term Meaning VMT Vehicle Miles Traveled References US DoT: 2016 Fatal Motor Vehicle Crashes: Overview bts.gov: Vehicle Miles Chicago stats lnk bike census bike league ... lnk nhtsa pedbike crashstats","title":"Bicycle Safety Statistics"},{"location":"Bicycle-Safety-Statistics.html#bicycle-safety-statistics","text":"The basic question to answer is how safe is riding a bike compared with driving a car. 37,461 motor deaths (2016) 1.15% to 2.6% fatality rate per 100 million vehicle miles travelled (VMT). Around 2.2M miles (2016) (light duty vehicle). As a check, $ \\frac{37,461 \\text{deaths}}{2.2 \\cdot 10^12 \\text{VMT}} \\approx 1.7% \\text{ per 100M VMT}$ which is within range of the reported statistic above. Pedalcyclist fatality count of 840 (2016). Pedestrian fatality count of 5987 (2016). If we take a rough estimate of the census 786k (pg2 census) bike commuters, assume 2 mile commute to work each way for a total of 4 miles and 5 work days, this leads to $ (786 \\cdot 10^3) \\cdot (\\frac{5}{7}) \\cdot (365) \\cdot (4) \\approx 8.2 \\cdot 10^8 \\text{ bike miles per year}$. This gives the (large) estimate of 8.2% fatality rate per 100M bike miles travelled. If we count by time instead, this is $ 800/(786000 * (46/60) * (5/7) * 365) \\approx 5.1 \\cdot 10^{-6} $ chance per hour of a fatality (bike). 85.4 percent of 150M workers drive to work ( src ). With a guess of a 20 minute commute time, gives 37461/(.854*(150*10^6)*(40/60)*(5/7)*365) or about $1.7 \\cdot 10^{-6}$ chance per hour of a fatality (car).","title":"Bicycle Safety Statistics"},{"location":"Bicycle-Safety-Statistics.html#rough-estimate","text":"Chicago: $ 2.46 \\cdot 10^6 $ daily minutes spent on bike. x","title":"Rough estimate"},{"location":"Bicycle-Safety-Statistics.html#glossary","text":"Term Meaning VMT Vehicle Miles Traveled","title":"Glossary"},{"location":"Bicycle-Safety-Statistics.html#references","text":"US DoT: 2016 Fatal Motor Vehicle Crashes: Overview bts.gov: Vehicle Miles Chicago stats lnk bike census bike league ... lnk nhtsa pedbike crashstats","title":"References"},{"location":"Bitcoin-Moon-Math.html","text":"Bitcoin Moon Math Bitcoin Price Bounds ~$280T Global wealth 21M Total Bitcoins Assuming Bitcoin stores 10% of the total value of domestic companies, this gives (0.1) * ($280 * 10^12) / (21 * 10^6 BTC) = ~$1.3 * 10^6 / BTC or about $1.3M per Bitcoin. This gives a rough range of what the Bitcoin value should be. Increasing the stored wealth increases the number as would an increased global company market capitalization. The 10% was chosen as an arbitrary number. One can easily imagine 20%+ or more as also being reasonable. As a guide, if Bitcoin should ever significantly exceed the global wealth or still be volatile after being valued at a significant percentage of the global wealth, then this would indicate, to me, a bubble. Comments As has been said elsewhere, should Bitcoin store a significant amount of wealth to warrant a $1M+/BTC, this would probably come with some significant changes, such as: Major resellers accepting Bitcoin Banks going out of business or otherwise changing significantly Some countries or governments adopting Bitcoin as a major or primary currency The point being that Bitcoin adoption doesn't happen in a vacuum. If the price were to rise significantly, this would bring changes to our everyday lives including the way we do banking. References ~$64.4T Market Cap of domestic companies the World Bank (2016) ~$280T global wealth Global Wealth Report by Credit Suisse (2017) 2017-12-06","title":"Bitcoin Moon Math"},{"location":"Bitcoin-Moon-Math.html#bitcoin-moon-math","text":"","title":"Bitcoin Moon Math"},{"location":"Bitcoin-Moon-Math.html#bitcoin-price-bounds","text":"~$280T Global wealth 21M Total Bitcoins Assuming Bitcoin stores 10% of the total value of domestic companies, this gives (0.1) * ($280 * 10^12) / (21 * 10^6 BTC) = ~$1.3 * 10^6 / BTC or about $1.3M per Bitcoin. This gives a rough range of what the Bitcoin value should be. Increasing the stored wealth increases the number as would an increased global company market capitalization. The 10% was chosen as an arbitrary number. One can easily imagine 20%+ or more as also being reasonable. As a guide, if Bitcoin should ever significantly exceed the global wealth or still be volatile after being valued at a significant percentage of the global wealth, then this would indicate, to me, a bubble.","title":"Bitcoin Price Bounds"},{"location":"Bitcoin-Moon-Math.html#comments","text":"As has been said elsewhere, should Bitcoin store a significant amount of wealth to warrant a $1M+/BTC, this would probably come with some significant changes, such as: Major resellers accepting Bitcoin Banks going out of business or otherwise changing significantly Some countries or governments adopting Bitcoin as a major or primary currency The point being that Bitcoin adoption doesn't happen in a vacuum. If the price were to rise significantly, this would bring changes to our everyday lives including the way we do banking.","title":"Comments"},{"location":"Bitcoin-Moon-Math.html#references","text":"~$64.4T Market Cap of domestic companies the World Bank (2016) ~$280T global wealth Global Wealth Report by Credit Suisse (2017)","title":"References"},{"location":"Bitcoin-Moon-Math.html#2017-12-06","text":"","title":"2017-12-06"},{"location":"Business-Notes-to-Myself.html","text":"Business Notes to Myself I'm still very much a novice at the non-technical side of business. To help improve my views on business, I hope this list will help encapsulate some knowledge that's out there. There's a lot of folk lore wisdom as well as some research about strategies for success. Many of these are biased in one form or another (cultural, selective, etc.) but often there's a grain of truth or wisdom in them. These are a collection of thoughts on business from various sources around the internet in addition to some of my own interpretation and experience. The real teacher is by doing, so anything aside from trying with real stakes will be academic. The best these notes can be is a rough guide. Personal Productivity Use TODO lists for daily tasks Number of tasks should be 3-5 Also consider making a 'To Watch' and a 'For Later' list. The 'To Watch' list is for tasks that are not being blocked by you but are being blocked by some other external agent. The 'For Later' list are things you want to do but some other factor has made it difficult for you to do right now (time, money, other priorities, etc.). src Do Something you Love Or if not love, something you actively don't despise. Whatever you do, you'll be doing it a lot, especially if you're making a livelyhood out of it. If you actively hate the thing that you're spending most of your waking day on, especially if it's now tied to your existence, that's a recipe for misery. I don't think doing something that you're 'in love' with is necessary, I think there's a balance between doing something you can stand, that isn't painful or something that gives you a clear benefit for the sacrifice, but the sentiment stands. Don't do something you hate, if you can help it, because that will lead to bitterness and misery. src Beware the Stampeding Hordes Every area, even/especially technical ones, have fads. Mathematics, physics, computer science, engineering, programming, business, etc. all have methodologies that will look naive with the benefit of hindsight. Some current trends turn out to be fundamental and sometimes the kernel of quality in a fad gets widespread adoption. Knowing which is a short term fad or something more fundamental is hard without deep expertise in the subject. Always strive to understand what the value offering of a system is, on a deeper level. This requires more work, especially when it comes to sifting through the hype cycle that comes with every fad, but will give a clearer picture of which methodologies and markets are worth pursuing. This will also help in understanding which portions of a methodology or market are worth pursuing without chasing the other red herrings that come with the fad and it's cheerleaders. This has shades in many other 'rules of thumb', such as the Gartner hype cycle, Warren Buffet's strategy of only investing in areas he understands, the 'no greater fool' idea and even 'do what you love'. Focusing your attention where the stampeding hordes are going might lead to success but it will be a gamble that the masses have determined the value. If the masses are fickle and the focus shifts, often rapidly and drastically, you'll often be left with experience that has no value and, more likely than not, be of the mentality to chase the new fad that has appeared. src References Why I don\u2019t (usually) meet startups in person - Elizabeth Yin Picking winners is a myth, but the PowerLaw is not - Clint Korver 2020-06-05","title":"Business Notes to Myself"},{"location":"Business-Notes-to-Myself.html#business-notes-to-myself","text":"I'm still very much a novice at the non-technical side of business. To help improve my views on business, I hope this list will help encapsulate some knowledge that's out there. There's a lot of folk lore wisdom as well as some research about strategies for success. Many of these are biased in one form or another (cultural, selective, etc.) but often there's a grain of truth or wisdom in them. These are a collection of thoughts on business from various sources around the internet in addition to some of my own interpretation and experience. The real teacher is by doing, so anything aside from trying with real stakes will be academic. The best these notes can be is a rough guide.","title":"Business Notes to Myself"},{"location":"Business-Notes-to-Myself.html#personal-productivity","text":"Use TODO lists for daily tasks Number of tasks should be 3-5 Also consider making a 'To Watch' and a 'For Later' list. The 'To Watch' list is for tasks that are not being blocked by you but are being blocked by some other external agent. The 'For Later' list are things you want to do but some other factor has made it difficult for you to do right now (time, money, other priorities, etc.). src","title":"Personal Productivity"},{"location":"Business-Notes-to-Myself.html#do-something-you-love","text":"Or if not love, something you actively don't despise. Whatever you do, you'll be doing it a lot, especially if you're making a livelyhood out of it. If you actively hate the thing that you're spending most of your waking day on, especially if it's now tied to your existence, that's a recipe for misery. I don't think doing something that you're 'in love' with is necessary, I think there's a balance between doing something you can stand, that isn't painful or something that gives you a clear benefit for the sacrifice, but the sentiment stands. Don't do something you hate, if you can help it, because that will lead to bitterness and misery. src","title":"Do Something you Love"},{"location":"Business-Notes-to-Myself.html#beware-the-stampeding-hordes","text":"Every area, even/especially technical ones, have fads. Mathematics, physics, computer science, engineering, programming, business, etc. all have methodologies that will look naive with the benefit of hindsight. Some current trends turn out to be fundamental and sometimes the kernel of quality in a fad gets widespread adoption. Knowing which is a short term fad or something more fundamental is hard without deep expertise in the subject. Always strive to understand what the value offering of a system is, on a deeper level. This requires more work, especially when it comes to sifting through the hype cycle that comes with every fad, but will give a clearer picture of which methodologies and markets are worth pursuing. This will also help in understanding which portions of a methodology or market are worth pursuing without chasing the other red herrings that come with the fad and it's cheerleaders. This has shades in many other 'rules of thumb', such as the Gartner hype cycle, Warren Buffet's strategy of only investing in areas he understands, the 'no greater fool' idea and even 'do what you love'. Focusing your attention where the stampeding hordes are going might lead to success but it will be a gamble that the masses have determined the value. If the masses are fickle and the focus shifts, often rapidly and drastically, you'll often be left with experience that has no value and, more likely than not, be of the mentality to chase the new fad that has appeared. src","title":"Beware the Stampeding Hordes"},{"location":"Business-Notes-to-Myself.html#references","text":"Why I don\u2019t (usually) meet startups in person - Elizabeth Yin Picking winners is a myth, but the PowerLaw is not - Clint Korver","title":"References"},{"location":"Business-Notes-to-Myself.html#2020-06-05","text":"","title":"2020-06-05"},{"location":"C-Project-Template.html","text":"Example C Project Template This is a basic template for using automake tools for creating the configure and Makefile in a C/C++ program. Requirements automake - e.g. sudo apt-get install automake gcc main.c #include <stdio.h> #include <stdlib.h> int main(int argc, char **argv) { printf(\"hello, friend\\n\"); } configure.in AC_INIT([hellofriend], [0.1], [abetusk@mechaelephant.com]) AM_INIT_AUTOMAKE AC_PROG_CC AC_CONFIG_FILES([Makefile]) AC_OUTPUT Makefile.am AUTOMAKE_OPTIONS = foreign bin_PROGRAMS = hellofriend hellofriend_SOURCES = main.c Run automake aclocal autoconf automake --add-missing configure, make, make install ./configure make make install References The magic behind configure, make, make install 2017-08-05","title":"C Project Template"},{"location":"C-Project-Template.html#example-c-project-template","text":"This is a basic template for using automake tools for creating the configure and Makefile in a C/C++ program.","title":"Example C Project Template"},{"location":"C-Project-Template.html#requirements","text":"automake - e.g. sudo apt-get install automake gcc","title":"Requirements"},{"location":"C-Project-Template.html#mainc","text":"#include <stdio.h> #include <stdlib.h> int main(int argc, char **argv) { printf(\"hello, friend\\n\"); }","title":"main.c"},{"location":"C-Project-Template.html#configurein","text":"AC_INIT([hellofriend], [0.1], [abetusk@mechaelephant.com]) AM_INIT_AUTOMAKE AC_PROG_CC AC_CONFIG_FILES([Makefile]) AC_OUTPUT","title":"configure.in"},{"location":"C-Project-Template.html#makefileam","text":"AUTOMAKE_OPTIONS = foreign bin_PROGRAMS = hellofriend hellofriend_SOURCES = main.c","title":"Makefile.am"},{"location":"C-Project-Template.html#run-automake","text":"aclocal autoconf automake --add-missing","title":"Run automake"},{"location":"C-Project-Template.html#configure-make-make-install","text":"./configure make make install","title":"configure, make, make install"},{"location":"C-Project-Template.html#references","text":"The magic behind configure, make, make install","title":"References"},{"location":"C-Project-Template.html#2017-08-05","text":"","title":"2017-08-05"},{"location":"CLT.html","text":"Central Limit Theorem This is a quick proof of the central limit theorem. The basic idea is as follows: The moment generating function of a Normal distribution is again a Normal distribution Two distributions that have the same moment generating function are/converge to the same underlying distribution The sum of uniform random variables is a convolution The Fourier transform of the sum of uniform random variables turns the convolution into a simple product The simple product (of the Fourier transform of the convolution of uniform random variables) converges to (the Fourier transform/moment generating function of) a Normal distribution The proof is restricted to the simple case of identical uniform distributions centered at zero for simplicity. With any luck, we can then show an extension, where if we replace the product of transformed random variables with something else, we get Levy stable distributions or something that shows the motivation for it. I won't be proving here that if the moment generating functions of two distributions are equal then the underlying distributions are equal. $ M_{N(\\mu,\\sigma)}(t) = N( \\beta, \\gamma ) $ First let's quickly show that the moment generating function of a Normal is again Normal: $$ \\begin{array}{c} M_{X}(t) \\stackrel{def}{=} E[e^{t X}] = \\sum_{k=0}^{\\infty} t^k E[X^k] = \\int_{-\\infty}^{\\infty} e^{t x} f_X(x) dx \\ \\end{array} $$ $$ \\begin{array}{cl} M_{N(\\mu,\\sigma)}(t) & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int_{-\\infty}^{\\infty} e^{t x} e^{ -\\frac{(x - \\mu)^2}{2 \\sigma^2} } dx \\ & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int_{-\\infty}^{\\infty} \\sigma e^{t (\\sigma u + \\mu) } e^{-\\frac{u^2}{2}} du \\ & = \\frac{e^{t \\mu}}{\\sqrt{2 \\pi }} \\int_{-\\infty}^{\\infty} e^{ - \\frac{1}{2} ( u^2 + 2 t \\sigma + \\sigma^2 t^2) + \\frac{t^2 \\sigma}{2} } du \\ & = \\frac{e^{t \\mu} e^{\\frac{t^2\\sigma}{2} } }{\\sqrt{2 \\pi }} \\int_{-\\infty}^{\\infty} e^{ - \\frac{1}{2} ( u + \\sigma t)^2 } du \\ & = \\frac{e^{t \\mu} e^{\\frac{t^2\\sigma}{2} } }{\\sqrt{2 \\pi }} \\int_{-\\infty}^{\\infty} e^{ - \\frac{1}{2} v^2 } dv \\ & = \\frac{e^{t \\mu} e^{\\frac{t^2\\sigma}{2} } }{\\sqrt{2 \\pi }} \\sqrt{2 \\pi} \\ & = e^{t \\mu} e^{\\frac{t^2\\sigma}{2} } \\ \\end{array} $$ 2021-01-25","title":"CLT"},{"location":"CLT.html#central-limit-theorem","text":"This is a quick proof of the central limit theorem. The basic idea is as follows: The moment generating function of a Normal distribution is again a Normal distribution Two distributions that have the same moment generating function are/converge to the same underlying distribution The sum of uniform random variables is a convolution The Fourier transform of the sum of uniform random variables turns the convolution into a simple product The simple product (of the Fourier transform of the convolution of uniform random variables) converges to (the Fourier transform/moment generating function of) a Normal distribution The proof is restricted to the simple case of identical uniform distributions centered at zero for simplicity. With any luck, we can then show an extension, where if we replace the product of transformed random variables with something else, we get Levy stable distributions or something that shows the motivation for it. I won't be proving here that if the moment generating functions of two distributions are equal then the underlying distributions are equal.","title":"Central Limit Theorem"},{"location":"CLT.html#m_nmusigmat-n-beta-gamma","text":"First let's quickly show that the moment generating function of a Normal is again Normal: $$ \\begin{array}{c} M_{X}(t) \\stackrel{def}{=} E[e^{t X}] = \\sum_{k=0}^{\\infty} t^k E[X^k] = \\int_{-\\infty}^{\\infty} e^{t x} f_X(x) dx \\ \\end{array} $$ $$ \\begin{array}{cl} M_{N(\\mu,\\sigma)}(t) & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int_{-\\infty}^{\\infty} e^{t x} e^{ -\\frac{(x - \\mu)^2}{2 \\sigma^2} } dx \\ & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int_{-\\infty}^{\\infty} \\sigma e^{t (\\sigma u + \\mu) } e^{-\\frac{u^2}{2}} du \\ & = \\frac{e^{t \\mu}}{\\sqrt{2 \\pi }} \\int_{-\\infty}^{\\infty} e^{ - \\frac{1}{2} ( u^2 + 2 t \\sigma + \\sigma^2 t^2) + \\frac{t^2 \\sigma}{2} } du \\ & = \\frac{e^{t \\mu} e^{\\frac{t^2\\sigma}{2} } }{\\sqrt{2 \\pi }} \\int_{-\\infty}^{\\infty} e^{ - \\frac{1}{2} ( u + \\sigma t)^2 } du \\ & = \\frac{e^{t \\mu} e^{\\frac{t^2\\sigma}{2} } }{\\sqrt{2 \\pi }} \\int_{-\\infty}^{\\infty} e^{ - \\frac{1}{2} v^2 } dv \\ & = \\frac{e^{t \\mu} e^{\\frac{t^2\\sigma}{2} } }{\\sqrt{2 \\pi }} \\sqrt{2 \\pi} \\ & = e^{t \\mu} e^{\\frac{t^2\\sigma}{2} } \\ \\end{array} $$","title":"$ M_{N(\\mu,\\sigma)}(t) = N( \\beta, \\gamma ) $"},{"location":"CLT.html#2021-01-25","text":"","title":"2021-01-25"},{"location":"Calculus-Notes.html","text":"Calculus Notes Chain Rule $$ (f(g(x))' = g'(x)f'(g(x)) $$ Proof : $$ \\begin{align} f'(x) & = \\frac{f(x+h) - f(x)}{h} \\\\ (f(g(x))' & = \\frac{f(g(x+h)) - f(g(x))}{h} \\\\ & = \\frac{f(g(x) + h g'(x)) - f(g(x))}{h} \\\\ & = \\frac{f(g(x)) + h g'(x)f'(g(x)) - f(g(x))}{h} \\\\ & = g'(x) f'(g(x)) \\end{align} $$ Taylor Expansion $$ f(x) = \\sum_{k=0}^{\\infty} \\frac{ f^{(n)}(a)(x-a)^n }{n!} $$ Proof : w.l.o.g. $a=0$ $$ \\begin{align} f(x) & = \\int_{0}^{x} f'(y) dy + f(0) \\\\ \\to f(x) & = f(0) + \\int_{0}^{x} f'(y_0)dy_0 \\\\ & = f(0) + \\int_{0}^{x} \\left( \\left( \\int_{0}^{y0} f''(y_1)dy_1 dy_0 \\right) + f'(0)dy_0 \\right) \\\\ & = f(0) + \\int_{0}^{x}f'(0)dy_0 + \\int_{0}^{x} \\int_{0}^{y0} f''(y_1) dy_1 dy_0 \\\\ & = f(0) + x f'(0) + \\int \\left( \\int \\left( \\int f'''(y_2) dy_2 dy_1 dy_0 \\right) + f''(0) dy_1 dy_0 \\right) \\\\ & = f(0) + x f'(0) + \\int_0^x \\int_0^{y_0} f''(0) dy_1 dy_0 + \\int \\int \\int f'''(y_2) dy_2 dy_1 dy_0 \\\\ & = f(0) + x f'(0) + \\frac{1}{2} x^2 f''(0) + \\cdots \\end{align} $$ $$ \\int_0^{x} \\int_0^{y_0} \\int_0^{y_1} \\dots \\int_0^{y_{n-1}} f^{(n+1)}(0) dy_n dy_{n-1} \\dots dy_0 = \\frac{ f^{(n+1)}(0) x^{n+1} }{(n+1)!} $$ Use a instead of 0. $$ | e^{i \\theta} - 1| \\le | \\theta | , \\ \\ \\theta \\in \\mathbb{R} $$ Proof : $$ \\begin{align} i \\int_0^{\\theta} e^{i t} dt & = i [ \\frac{1}{i} e^{i t} |_0^{\\theta} \\\\ & = e^{ i \\theta } - 1 \\end{align} $$ $$ \\begin{align} | i \\int_0^{\\theta} e^{i t} | & = | \\int_0^{\\theta} e^{i t} dt | \\\\ & \\le \\int_0^{\\theta} |e^{i t} dt \\\\ & = 0 \\end{align} $$ $$ \\to | e^{i \\theta} - 1 | \\le | \\theta | $$ $$ | e^{i \\theta} - 1 - i \\theta | \\le 2 | \\theta | $$ Proof : $$ | e^{i \\theta} - 1 - i \\theta | \\lte | e^{i \\theta} - 1 | + | \\theta| \\le 2 \\theta $$ $$ \\to | e^i \\theta} - 1 - i \\theta| \\le \\frac{\\theta^2}{2} $$ $$ | e^{i \\theta} - 1 - i \\theta| \\le \\frac{\\theta^2}{2} $$ Proof : $$ \\begin{align} i \\int_0^{\\theta} ( e^{i t} - 1 ) dt & = i [ \\frac{1}{i} e^{i t} - \\frac{1}{i} - t |_0^{\\theta} \\\\ & = e^{i \\theta} - 1 - i \\theta \\end{align} $$ $$ \\begin{align} | i \\int_0^{\\theta} ( e^{i t} - 1) dt| & = | \\int_0^{\\theta} ( e^{i t} -1) dt | \\\\ \\le \\int_0^{\\theta} | e^{i t} - 1)dt & \\le \\int_0^{\\theta} |t| dt \\\\ & = \\frac{\\theta^2}{2} \\end{align} $$","title":"Calculus Notes"},{"location":"Calculus-Notes.html#calculus-notes","text":"","title":"Calculus Notes"},{"location":"Calculus-Notes.html#chain-rule","text":"$$ (f(g(x))' = g'(x)f'(g(x)) $$ Proof : $$ \\begin{align} f'(x) & = \\frac{f(x+h) - f(x)}{h} \\\\ (f(g(x))' & = \\frac{f(g(x+h)) - f(g(x))}{h} \\\\ & = \\frac{f(g(x) + h g'(x)) - f(g(x))}{h} \\\\ & = \\frac{f(g(x)) + h g'(x)f'(g(x)) - f(g(x))}{h} \\\\ & = g'(x) f'(g(x)) \\end{align} $$","title":"Chain Rule"},{"location":"Calculus-Notes.html#taylor-expansion","text":"$$ f(x) = \\sum_{k=0}^{\\infty} \\frac{ f^{(n)}(a)(x-a)^n }{n!} $$ Proof : w.l.o.g. $a=0$ $$ \\begin{align} f(x) & = \\int_{0}^{x} f'(y) dy + f(0) \\\\ \\to f(x) & = f(0) + \\int_{0}^{x} f'(y_0)dy_0 \\\\ & = f(0) + \\int_{0}^{x} \\left( \\left( \\int_{0}^{y0} f''(y_1)dy_1 dy_0 \\right) + f'(0)dy_0 \\right) \\\\ & = f(0) + \\int_{0}^{x}f'(0)dy_0 + \\int_{0}^{x} \\int_{0}^{y0} f''(y_1) dy_1 dy_0 \\\\ & = f(0) + x f'(0) + \\int \\left( \\int \\left( \\int f'''(y_2) dy_2 dy_1 dy_0 \\right) + f''(0) dy_1 dy_0 \\right) \\\\ & = f(0) + x f'(0) + \\int_0^x \\int_0^{y_0} f''(0) dy_1 dy_0 + \\int \\int \\int f'''(y_2) dy_2 dy_1 dy_0 \\\\ & = f(0) + x f'(0) + \\frac{1}{2} x^2 f''(0) + \\cdots \\end{align} $$ $$ \\int_0^{x} \\int_0^{y_0} \\int_0^{y_1} \\dots \\int_0^{y_{n-1}} f^{(n+1)}(0) dy_n dy_{n-1} \\dots dy_0 = \\frac{ f^{(n+1)}(0) x^{n+1} }{(n+1)!} $$ Use a instead of 0. $$ | e^{i \\theta} - 1| \\le | \\theta | , \\ \\ \\theta \\in \\mathbb{R} $$ Proof : $$ \\begin{align} i \\int_0^{\\theta} e^{i t} dt & = i [ \\frac{1}{i} e^{i t} |_0^{\\theta} \\\\ & = e^{ i \\theta } - 1 \\end{align} $$ $$ \\begin{align} | i \\int_0^{\\theta} e^{i t} | & = | \\int_0^{\\theta} e^{i t} dt | \\\\ & \\le \\int_0^{\\theta} |e^{i t} dt \\\\ & = 0 \\end{align} $$ $$ \\to | e^{i \\theta} - 1 | \\le | \\theta | $$ $$ | e^{i \\theta} - 1 - i \\theta | \\le 2 | \\theta | $$ Proof : $$ | e^{i \\theta} - 1 - i \\theta | \\lte | e^{i \\theta} - 1 | + | \\theta| \\le 2 \\theta $$ $$ \\to | e^i \\theta} - 1 - i \\theta| \\le \\frac{\\theta^2}{2} $$ $$ | e^{i \\theta} - 1 - i \\theta| \\le \\frac{\\theta^2}{2} $$ Proof : $$ \\begin{align} i \\int_0^{\\theta} ( e^{i t} - 1 ) dt & = i [ \\frac{1}{i} e^{i t} - \\frac{1}{i} - t |_0^{\\theta} \\\\ & = e^{i \\theta} - 1 - i \\theta \\end{align} $$ $$ \\begin{align} | i \\int_0^{\\theta} ( e^{i t} - 1) dt| & = | \\int_0^{\\theta} ( e^{i t} -1) dt | \\\\ \\le \\int_0^{\\theta} | e^{i t} - 1)dt & \\le \\int_0^{\\theta} |t| dt \\\\ & = \\frac{\\theta^2}{2} \\end{align} $$","title":"Taylor Expansion"},{"location":"Coding-Style.html","text":"Elements of Coding Style This is meant as a light style guide for coding. Rules here are guides and not meant to be used when common sense dictates otherwise. The following rules are mostly semantic and are done with a 'polyglot' point of view even though JavaScript is mostly used for coding examples. These are coding format rules that I mostly gravitate towards and is my attempt at formalizing them in some way. Prefer two space indentation to tab or four spaces Two space indentation should be preferred to four spaces or tabs for indentation. Four spaces causes code to gravitate too far to the right. Tabs are inconsistently rendered, annoying to manipulate and camouflage themselves as spaces. function f() { console.log(\"hello\"); if (true) { console.log(\"hello, friend\"); } } Prefer start block token on the same line as control statements and function headings For languages that have a start block token, a { say, prefer to put them on the same line as the function heading. Putting start blocks only increases vertical length with no real benefit. If breaks in text are needed, add returns as needed. function f() { for (var i=0; i<1; i++) { console.log(\"hello\"); } } Encase conditional blocks within block tokens even if the language allows otherwise Even if the language allows you to forgo block tokens to encase the control block of a conditional, encase them in the block tokens anyway. If statements are extended, it's an easy mistake to think the added code falls within the same conditional if not encased in the block tokens. if (true) { console.log(\"hello\"); } Place small conditional statements on a single line, formatting as necessary If the conditional expression and statement body is small enough, place it all on the same line. If the line is too long, this should be avoided but for small statements, terseness wins over beauty. else if and else statements following the initial if can also be placed inline if the test expression and statement block are small enough. To help with readability, aligning test expression and statement blocks should be preferred. if (true) { console.log(\"hello\"); } if (ok) { console.log(\"ok\"; } else if (ko) { console.log(\"ko\"); } else { console.log(\"..\"); } Prefer alternate else conditionals on their own line for longer statement blocks For ease of reading, put else and else if conditionals on their own line if the statement blocks are too large to warrant putting them all on their own line. Each alternate conditional is easier to read, align and comment if necessary if they're as independent as possible from the surrounding sibling conditionals. if (ok) { console.log(\"The hand that mocked them\"); console.log(\"and the heart that fed\"); } else if (ko) { console.log(\"Of that colossal wreck,\"); console.log(\"boundless and bare\"); } else { console.log(\"A shattered visage lies,\"); console.log(\"whose frown\"); } Prefer ternary conditional assignment over if/else assignment If the language allows for ternary expressions, use them for alternate assignments. Terseness wins out over beauty. var v = ( vv ? true : false ); Prefer putting comments at the top of statement blocks and put a blank comment line after the comment text Comments should appear on their own line, indented appropriately, above the logic it's commenting on, where appropriate. In the cases that comments need to go into a statement block instead of above it, they should appear at the top of the statement block. Inline comments should be avoided. Comments should describe what the high level concept of a piece of code is. The least it should do is to remind the author of the intent of the code were it to be revisited after a time. Comment blocks should have a trailing 'blank' comment line to ease of reading. // If everything is 'ok', // provide a friendly greeting. // if (ok) { console.log(\"hello\"); } End of function returns should have no blank line between the ending block token Trailing newlines after the last return should be culled for conciseness. function f() { console.log(\"hello\"); return true; } No trailing whitespace at end of lines Trailing whitespace should be culled at the end of the line. Use whitespace highlighting in your editor to notice it. When doing many variable assignments, align them for readability When assigning variables values, on initialization say, prefer to format them so they can easily be read by aligning the equals sign and putting multiple variables on a single line if need be. Small differences are more noticeable when aligned in this way. Structure and intent of why the variables are being assigned is sometimes more apparent when aligned properly. Add space between the end of the variable and the = if need be. Group in blocks to ease readability. var x00 = 0, x01 = 1, x10 = 1, x11 = 0; var mammal = 'cat', reptile = 'chameleon', fish = 'catfish', insect = 'ladybug', dinosaur = 'brontosaurus', extinct_elephant = 'woolly mammoth'; Place a single space between parent expression and nested sub expressions, aligning inner expressions with proper indentation. Place Boolean operations at end of line, where appropriate. For a parent control statement expression, put a space between the outer parenthesis and the inner expression. For the expressions within the control statement, align them for readability with proper indentation if the control is nested. Boolean tokens should be placed at the end of the line so the first relevant piece of information on the line is relevant logic. Nested multi-line conditionals are hard enough to read, try to ease the cognitive load on the reader as much as possible. function f(v) { var k=0; while ( (v>10) && (k<v) && ( ((k+v) > 100) || ((k-v) < 50) ) ) { console.log(\"hello\"); } } Use parenthesis for explicit order of operations Use parenthesis to explicitly show what the order of operations should be for an expression or statement. It puts a lot of cognitive load on the reader to remember which order of operations takes precedence in the language that the code is written in. Sometimes order of operations aren't the same between languages. Sometimes a token is overloaded and mean different things in different contexts, often within the same line. Ease the readers burden by explicitly spelling it out for them. while ( ((*p) != 0) && ((5*(*p)) > 100) ) { p++; } Contents of commenting Comments should be a high level description of what the code does. The code describes what the code does so a comments job is to enlighten what the code is doing and give motivation for why it's doing it, not repeat the logic verbatim. A good rule of thumb is to write a comment that you will be able to understand in 6 months time (or whatever unit of time causes forgetfulness) should you look at the code again. 2018-02-17","title":"Coding Style"},{"location":"Coding-Style.html#elements-of-coding-style","text":"This is meant as a light style guide for coding. Rules here are guides and not meant to be used when common sense dictates otherwise. The following rules are mostly semantic and are done with a 'polyglot' point of view even though JavaScript is mostly used for coding examples. These are coding format rules that I mostly gravitate towards and is my attempt at formalizing them in some way.","title":"Elements of Coding Style"},{"location":"Coding-Style.html#prefer-two-space-indentation-to-tab-or-four-spaces","text":"Two space indentation should be preferred to four spaces or tabs for indentation. Four spaces causes code to gravitate too far to the right. Tabs are inconsistently rendered, annoying to manipulate and camouflage themselves as spaces. function f() { console.log(\"hello\"); if (true) { console.log(\"hello, friend\"); } }","title":"Prefer two space indentation to tab or four spaces"},{"location":"Coding-Style.html#prefer-start-block-token-on-the-same-line-as-control-statements-and-function-headings","text":"For languages that have a start block token, a { say, prefer to put them on the same line as the function heading. Putting start blocks only increases vertical length with no real benefit. If breaks in text are needed, add returns as needed. function f() { for (var i=0; i<1; i++) { console.log(\"hello\"); } }","title":"Prefer start block token on the same line as control statements and function headings"},{"location":"Coding-Style.html#encase-conditional-blocks-within-block-tokens-even-if-the-language-allows-otherwise","text":"Even if the language allows you to forgo block tokens to encase the control block of a conditional, encase them in the block tokens anyway. If statements are extended, it's an easy mistake to think the added code falls within the same conditional if not encased in the block tokens. if (true) { console.log(\"hello\"); }","title":"Encase conditional blocks within block tokens even if the language allows otherwise"},{"location":"Coding-Style.html#place-small-conditional-statements-on-a-single-line-formatting-as-necessary","text":"If the conditional expression and statement body is small enough, place it all on the same line. If the line is too long, this should be avoided but for small statements, terseness wins over beauty. else if and else statements following the initial if can also be placed inline if the test expression and statement block are small enough. To help with readability, aligning test expression and statement blocks should be preferred. if (true) { console.log(\"hello\"); } if (ok) { console.log(\"ok\"; } else if (ko) { console.log(\"ko\"); } else { console.log(\"..\"); }","title":"Place small conditional statements on a single line, formatting as necessary"},{"location":"Coding-Style.html#prefer-alternate-else-conditionals-on-their-own-line-for-longer-statement-blocks","text":"For ease of reading, put else and else if conditionals on their own line if the statement blocks are too large to warrant putting them all on their own line. Each alternate conditional is easier to read, align and comment if necessary if they're as independent as possible from the surrounding sibling conditionals. if (ok) { console.log(\"The hand that mocked them\"); console.log(\"and the heart that fed\"); } else if (ko) { console.log(\"Of that colossal wreck,\"); console.log(\"boundless and bare\"); } else { console.log(\"A shattered visage lies,\"); console.log(\"whose frown\"); }","title":"Prefer alternate else conditionals on their own line for longer statement blocks"},{"location":"Coding-Style.html#prefer-ternary-conditional-assignment-over-ifelse-assignment","text":"If the language allows for ternary expressions, use them for alternate assignments. Terseness wins out over beauty. var v = ( vv ? true : false );","title":"Prefer ternary conditional assignment over if/else assignment"},{"location":"Coding-Style.html#prefer-putting-comments-at-the-top-of-statement-blocks-and-put-a-blank-comment-line-after-the-comment-text","text":"Comments should appear on their own line, indented appropriately, above the logic it's commenting on, where appropriate. In the cases that comments need to go into a statement block instead of above it, they should appear at the top of the statement block. Inline comments should be avoided. Comments should describe what the high level concept of a piece of code is. The least it should do is to remind the author of the intent of the code were it to be revisited after a time. Comment blocks should have a trailing 'blank' comment line to ease of reading. // If everything is 'ok', // provide a friendly greeting. // if (ok) { console.log(\"hello\"); }","title":"Prefer putting comments at the top of statement blocks and put a blank comment line after the comment text"},{"location":"Coding-Style.html#end-of-function-returns-should-have-no-blank-line-between-the-ending-block-token","text":"Trailing newlines after the last return should be culled for conciseness. function f() { console.log(\"hello\"); return true; }","title":"End of function returns should have no blank line between the ending block token"},{"location":"Coding-Style.html#no-trailing-whitespace-at-end-of-lines","text":"Trailing whitespace should be culled at the end of the line. Use whitespace highlighting in your editor to notice it.","title":"No trailing whitespace at end of lines"},{"location":"Coding-Style.html#when-doing-many-variable-assignments-align-them-for-readability","text":"When assigning variables values, on initialization say, prefer to format them so they can easily be read by aligning the equals sign and putting multiple variables on a single line if need be. Small differences are more noticeable when aligned in this way. Structure and intent of why the variables are being assigned is sometimes more apparent when aligned properly. Add space between the end of the variable and the = if need be. Group in blocks to ease readability. var x00 = 0, x01 = 1, x10 = 1, x11 = 0; var mammal = 'cat', reptile = 'chameleon', fish = 'catfish', insect = 'ladybug', dinosaur = 'brontosaurus', extinct_elephant = 'woolly mammoth';","title":"When doing many variable assignments, align them for readability"},{"location":"Coding-Style.html#place-a-single-space-between-parent-expression-and-nested-sub-expressions-aligning-inner-expressions-with-proper-indentation-place-boolean-operations-at-end-of-line-where-appropriate","text":"For a parent control statement expression, put a space between the outer parenthesis and the inner expression. For the expressions within the control statement, align them for readability with proper indentation if the control is nested. Boolean tokens should be placed at the end of the line so the first relevant piece of information on the line is relevant logic. Nested multi-line conditionals are hard enough to read, try to ease the cognitive load on the reader as much as possible. function f(v) { var k=0; while ( (v>10) && (k<v) && ( ((k+v) > 100) || ((k-v) < 50) ) ) { console.log(\"hello\"); } }","title":"Place a single space between parent expression and nested sub expressions, aligning inner expressions with proper indentation.  Place Boolean operations at end of line, where appropriate."},{"location":"Coding-Style.html#use-parenthesis-for-explicit-order-of-operations","text":"Use parenthesis to explicitly show what the order of operations should be for an expression or statement. It puts a lot of cognitive load on the reader to remember which order of operations takes precedence in the language that the code is written in. Sometimes order of operations aren't the same between languages. Sometimes a token is overloaded and mean different things in different contexts, often within the same line. Ease the readers burden by explicitly spelling it out for them. while ( ((*p) != 0) && ((5*(*p)) > 100) ) { p++; }","title":"Use parenthesis for explicit order of operations"},{"location":"Coding-Style.html#contents-of-commenting","text":"Comments should be a high level description of what the code does. The code describes what the code does so a comments job is to enlighten what the code is doing and give motivation for why it's doing it, not repeat the logic verbatim. A good rule of thumb is to write a comment that you will be able to understand in 6 months time (or whatever unit of time causes forgetfulness) should you look at the code again.","title":"Contents of commenting"},{"location":"Coding-Style.html#2018-02-17","text":"","title":"2018-02-17"},{"location":"Command-Line-Option-Loose-Standard.html","text":"Loose Standards for Command Line Options option description -a all -b buffer / block size -c comand -d debug or delete -D define -e execute -f file (input) -h help -i interactive / initialize -I include -k keep / kill -l list / load / login / length / lock -m message / mode -n number -o output -p port / protocol -q quiet -r / -R recurse -s silent / subject -t tag -u user -v verbose / version -V version -w width / warning -x debug / extract -y yes -z compress From an answer on SO , a good convention seems to be: When no options are given, show help and print to stderr with an error code When the -h or --help option are given, provide help on stdout and give no error References taoup ch. 10 SO: Should the command line \u201cusage\u201d be printed on stdout or stderr? 2017-12-25","title":"Command Line Option Loose Standard"},{"location":"Command-Line-Option-Loose-Standard.html#loose-standards-for-command-line-options","text":"option description -a all -b buffer / block size -c comand -d debug or delete -D define -e execute -f file (input) -h help -i interactive / initialize -I include -k keep / kill -l list / load / login / length / lock -m message / mode -n number -o output -p port / protocol -q quiet -r / -R recurse -s silent / subject -t tag -u user -v verbose / version -V version -w width / warning -x debug / extract -y yes -z compress From an answer on SO , a good convention seems to be: When no options are given, show help and print to stderr with an error code When the -h or --help option are given, provide help on stdout and give no error","title":"Loose Standards for Command Line Options"},{"location":"Command-Line-Option-Loose-Standard.html#references","text":"taoup ch. 10 SO: Should the command line \u201cusage\u201d be printed on stdout or stderr?","title":"References"},{"location":"Command-Line-Option-Loose-Standard.html#2017-12-25","text":"","title":"2017-12-25"},{"location":"Complexity-Criticality.html","text":"Complexity and Criticality Complexity and criticality, in the physics sense of the word, refer to complex behavior from the repeated application of simple rules and the study of systems at or near phase transitions. These notes follow Christensen and Moloney's book \"Complexity and Criticality\". The three major points will be: The Bethe lattice as a basis for talking about complexity and criticality The Ising model The rice pile model Bethe Lattice The Bethe lattice is an infinite rooted tree of degree $z$. Since the Bethe lattice is a tree, each leaf of the root node is independent, often times allowing for simple analysis. Any given node can be thought of as the root of the tree, giving the tree a symmetry that also helps in analysis. Consider coloring each node of the tree black or white with probability $p$. The following concepts are then considered: $p_c$ - critical probability when an infinite cluster appears $chi(p)$ - average cluster size $n(s,p)$ - density of cluster size $s$ $s_{\\eps}$ - characteristic cluster size (explained below) $P_{\\inf}(p)$ - probability root node is part of infinite cluster","title":"Complexity Criticality"},{"location":"Complexity-Criticality.html#complexity-and-criticality","text":"Complexity and criticality, in the physics sense of the word, refer to complex behavior from the repeated application of simple rules and the study of systems at or near phase transitions. These notes follow Christensen and Moloney's book \"Complexity and Criticality\". The three major points will be: The Bethe lattice as a basis for talking about complexity and criticality The Ising model The rice pile model","title":"Complexity and Criticality"},{"location":"Complexity-Criticality.html#bethe-lattice","text":"The Bethe lattice is an infinite rooted tree of degree $z$. Since the Bethe lattice is a tree, each leaf of the root node is independent, often times allowing for simple analysis. Any given node can be thought of as the root of the tree, giving the tree a symmetry that also helps in analysis. Consider coloring each node of the tree black or white with probability $p$. The following concepts are then considered: $p_c$ - critical probability when an infinite cluster appears $chi(p)$ - average cluster size $n(s,p)$ - density of cluster size $s$ $s_{\\eps}$ - characteristic cluster size (explained below) $P_{\\inf}(p)$ - probability root node is part of infinite cluster","title":"Bethe Lattice"},{"location":"Currency-Editorial.html","text":"Currency Editorial These are some opinions I have about currencies, especially as they relate to cryptocurrency. Overview Currency is, in some very high level aspect, a representation of energy. The value of currency is a socially contract that everyone who participates in its use agrees on. We use currency as a representation of energy to trade for goods or services which are another form of energy in a different, often more usable, form. By abstracting the notion of currency, a representational form of energy, we can exchange money we own for goods or services to a party without the need to understand what the receiving party will use it for. While we exchange the representation of energy, the currency, for actual energy, the representation is not itself energy and is only valid so long as everyone agrees on its credibility. As a side note, this \"fungible\" feature of currency is often used in a narrative about graduating from a barter system to one that uses currency but this should viewed with some skepticism. There is evidence that the barter economy never really existed and that people more likely used a 'gift economy'. A gift has difficulty scaling past a certain population size and so a currency is desirable to facilitate trade. Wikipedia has a list of accepted properties of money: Fungibility - the ability to substitute one unit of currency for another Durability - can be used repeatedly Divisibility - can be divided into smaller parts Portability - can be easily transported or carried Legibility - can be easily identified Scarcity - supply is limited and is not easily counterfeited All of these aspects allow for money to be used easily and without worry that the system will collapse from accidental or malicious use. Interest Rates and Lending Once currency is understood to be a representation of energy, we can come up with a reason why interest rates grow exponentially. I don't have a good understanding of inflation to work out why inflation should occur. One view of the technological pace of our civilization is that it grows exponentially. Technology, in a vague definition, is the conversion of unusable energy to a usable form. In this sense, our civilization's technological progress can be seen as the conversion of the energy that would otherwise not be usable by us, mostly coming from the sun, to a form that we can use. The form can be calories from food, creation of needed proteins or nutrients or directly as electricity. This pace of energy conversion has been growing exponentially for most of recorded history. This means that any investment in technology today, in whatever form, yields, on average, exponential returns in the future. If an investment into a particular technology will yield exponential returns for the recipient of the loan, the lender can then ask for exponential returns, in the form of some interest rate, so long as the interest rate works out to be less than the profit that the lendee realizes. The term 'technology' is used but this could be a business endeavor, a personal loan for education, farm cultivation, computer manufacture, health, communication or any host of other engineering, business or scientific pursuits. I don't have a good understanding of inflation and the mechanisms behind it but here is my best guess as to what's happening: Inflation is an engineered situation. For the US, the target is 2%. This means, extra money is pumped into the economy so as to devalue the dollar by 2% annually. This inflation is supposed to drive investment as holding onto money without investing will decrease its value. As an argument for why 2% is the specific number chosen for inflation, I would point out that global energy usage has been growing at a rate of 2%-2.5% per annum since 1980. The US had a steady energy consumption rate of 2%-2.5% since the 1900s and only started faltering from that annual growth rate in the late 1970s, presumably because of 1979 oil crisis or some other oil price shock, from which the US never recovered fully. So maybe what's going on is that the US government is acting as a lender to the population, be it directly or through banks, and effectively lending money at a rate of 2% annually. The idea is that, at least globally, in some gross sense, investments should yield somewhere in the range of 2%-2.5% annual returns, so lending at 2%, somewhere a bit below the global annual returns as represented by the energy usage, provides money \"lending liquidity\" while still giving adequate return on the investment. A Note on Gold Backed Currency A quick word about currency that's backed by some other asset, such as gold or some other precious metal or material. I will focus on gold but the same argument could just as easily apply to most other materials. While gold has some particular properties and has some scarcity in the universe because of it's atomic composition, in some sense the value we imbue gold with is arbitrary. Gold is a metal that has a particular prevalence on earth. We could just have easily lived on a planet where gold is as common as water. Gold's scarcity and difficulty in production allows it to be used as a currency but insisting that gold be used as an asset backing currency is just one more layer of indirection trying to obfuscate that any currency's value is a social contract that is, in any other respect, arbitrary. Locally, We Do Not Exist in an Energy Equilibrium One point that comes up is the idea that we might be violating some energy conversation laws by our energy creation or that exponential growth cannot last unfettered. In response to a violation of the conservation of energy, the answer is that we do not live in an energy equilibrium. The sun dumps energy onto the earth in a form that is not easily used by us and is mostly lost by us. The conversion of this unusable energy to a useable form is what is increasing at an exponential pace and does not violate any conservation of energy laws. Our technological pace has been growing exponentially and is most likely going to continue to grow exponentially for some time. Unfettered, indefinite, exponential growth cannot continue, if energy is finite, so this exponential growth will need to halt at some point. Only a sliver of the sun's energy is deposited on earth, with most being dispersed in all other directions. Our sun is one many stars and is one of many celestial bodies that we can extract energy from. Even at a large 2.5% annual increase in energy usage, it would take over 250 years to reach the limits of the sun's energy being deposited on earth. Beyond our earth, the stores of energy that are available are galactic. Cryptocurrencies If people can agree to a crypto-currency's value then it can be used as a currency. All money's value is fundamentally arbitrary. We give a currency it's value by agreeing on how of it to give for a good or service. If instead of using currency that is arbitrary given value by a government, or a currency that is arbitrarily given it's value from a physical material, like gold or silver, then we can use a currency that's given it's value arbitrarily by an agreed on protocol as is present with cryptocurrencies. I'm skeptical that anything other than proof-of-work can be made to be a viable decentralized cryptocurrency, so I'll focus on proof-of-work (PoW) that's used by Bitcoin. The major elements of a currency are satisfied by a cryptocurrency: Fungibility - A piece of cryptocurrency is just as valid as any other Durability - A piece of cryptocurrency can be traded and re-used ad infinitum Divisibility - A piece of cryptocurrency can be sub-divided as need be Portability - Cryptocurrency is inherently information and so is easily communicated and thus easily transportable Legibility - Cryptocurrency can be easily valued and compared as it's just a number Scarcity - is achieved either by limiting the number of cryptocurrency to ever be produced or by inflating the proof of work to match current computational resources A note on divisibility: Bitcoin has a smallest unit of a 'satoshi', just as a United States Dollar has a smallest unit of a cent (0.01 dollar). This divisibility seems like a kind of artifact of an older currency discussion and so I'm not sure how beneficial it is to dive into it. I will say that cryptocurrencies, specifically Bitcoin, are as divisible as all fiat currencies and, I suspect, have the possibility of a software upgrade to be able to further divide the 'satoshi' if need be. Scarcity also warrants a bit more discussion. Bitcoin's proof of work mechanism allows for an ever intensifying work mechanism to make sure that the mining of new Bitcoin and, eventually, steady-state economic transfer and transaction of Bitcoin, will dynamically scale to the available resources. The incentive structure ensures that if there is cheaper compute to gain an advantage, people will use it and there will be a race for who can use the cheapest compute to gain the reward in the cryptocurrency. So Bitcoin uses two scarcity mechanism, one that limits the amount of actual Bitcoin produced (around 21M BTC) and the other as a scaled proof of work mechanism. I'm unclear as to the advantages and disadvantages of a finite number of Bitcoin. One advantage of the finite number of Bitcoin is that it's easy to reason about, since the number itself is capped. As I see it, possible disadvantages include: Note dealing with inflation well, as a satoshi might need to be further subdivided to account for large scale inflation Uncapping the amount of cryptocurrency might lead to a less volatile currency as fluctuations can be absorbed by creating more cryptocurrency that wouldn't be possible with a fixed number of potential coins One could imagine a cryptocurrency that yields mined coins on a decreasing power law scale, potentially making the number of theoretical coins infinite while still giving fewer coins as the cryptocurrency matures. Anyway, I have to profess ignorance. Addendum: A Note on the Wealthy One idea is that wealthy individuals are wardens of money rather than owners. The social contract is that people give an individual their money as payment for goods and services but also with the hope that they will use that money for other, potentially societally beneficial, endeavours. If an individual that is a warden of wealth does not use their surplus of money better than how the public trust can, then the wealth should be transferred from the individual to the public trust. There will always be a tension between what is good for society and what is good for the individual. Wealth is a form of power and disproportionate power can warp policy to create more difficulty in separating the wealth from the wealthy individual. There will always be a battle between individuals who want to accumulate wealth that is better served under the direction of the public trust and the public. Disproportionate wealth is not inherently bad just as system of completely flat, equal wealth is not inherently good. The question is what distribution of wealth should we see in a healthy economy or system of government. My opinion is that some wealth aggregation is functionally better for society than insisting a completely flat distribution. We see this with elected officials who wield a disproportionate amount of power. There are limits to terms of officials and limits to their power, but the reason they are allowed an aggregation of power is to enact the public will they've been elected for. So too can this be with wealth. References 0 ( a ) 1 2021-03-16","title":"Currency Editorial"},{"location":"Currency-Editorial.html#currency-editorial","text":"These are some opinions I have about currencies, especially as they relate to cryptocurrency.","title":"Currency Editorial"},{"location":"Currency-Editorial.html#overview","text":"Currency is, in some very high level aspect, a representation of energy. The value of currency is a socially contract that everyone who participates in its use agrees on. We use currency as a representation of energy to trade for goods or services which are another form of energy in a different, often more usable, form. By abstracting the notion of currency, a representational form of energy, we can exchange money we own for goods or services to a party without the need to understand what the receiving party will use it for. While we exchange the representation of energy, the currency, for actual energy, the representation is not itself energy and is only valid so long as everyone agrees on its credibility. As a side note, this \"fungible\" feature of currency is often used in a narrative about graduating from a barter system to one that uses currency but this should viewed with some skepticism. There is evidence that the barter economy never really existed and that people more likely used a 'gift economy'. A gift has difficulty scaling past a certain population size and so a currency is desirable to facilitate trade. Wikipedia has a list of accepted properties of money: Fungibility - the ability to substitute one unit of currency for another Durability - can be used repeatedly Divisibility - can be divided into smaller parts Portability - can be easily transported or carried Legibility - can be easily identified Scarcity - supply is limited and is not easily counterfeited All of these aspects allow for money to be used easily and without worry that the system will collapse from accidental or malicious use.","title":"Overview"},{"location":"Currency-Editorial.html#interest-rates-and-lending","text":"Once currency is understood to be a representation of energy, we can come up with a reason why interest rates grow exponentially. I don't have a good understanding of inflation to work out why inflation should occur. One view of the technological pace of our civilization is that it grows exponentially. Technology, in a vague definition, is the conversion of unusable energy to a usable form. In this sense, our civilization's technological progress can be seen as the conversion of the energy that would otherwise not be usable by us, mostly coming from the sun, to a form that we can use. The form can be calories from food, creation of needed proteins or nutrients or directly as electricity. This pace of energy conversion has been growing exponentially for most of recorded history. This means that any investment in technology today, in whatever form, yields, on average, exponential returns in the future. If an investment into a particular technology will yield exponential returns for the recipient of the loan, the lender can then ask for exponential returns, in the form of some interest rate, so long as the interest rate works out to be less than the profit that the lendee realizes. The term 'technology' is used but this could be a business endeavor, a personal loan for education, farm cultivation, computer manufacture, health, communication or any host of other engineering, business or scientific pursuits. I don't have a good understanding of inflation and the mechanisms behind it but here is my best guess as to what's happening: Inflation is an engineered situation. For the US, the target is 2%. This means, extra money is pumped into the economy so as to devalue the dollar by 2% annually. This inflation is supposed to drive investment as holding onto money without investing will decrease its value. As an argument for why 2% is the specific number chosen for inflation, I would point out that global energy usage has been growing at a rate of 2%-2.5% per annum since 1980. The US had a steady energy consumption rate of 2%-2.5% since the 1900s and only started faltering from that annual growth rate in the late 1970s, presumably because of 1979 oil crisis or some other oil price shock, from which the US never recovered fully. So maybe what's going on is that the US government is acting as a lender to the population, be it directly or through banks, and effectively lending money at a rate of 2% annually. The idea is that, at least globally, in some gross sense, investments should yield somewhere in the range of 2%-2.5% annual returns, so lending at 2%, somewhere a bit below the global annual returns as represented by the energy usage, provides money \"lending liquidity\" while still giving adequate return on the investment.","title":"Interest Rates and Lending"},{"location":"Currency-Editorial.html#a-note-on-gold-backed-currency","text":"A quick word about currency that's backed by some other asset, such as gold or some other precious metal or material. I will focus on gold but the same argument could just as easily apply to most other materials. While gold has some particular properties and has some scarcity in the universe because of it's atomic composition, in some sense the value we imbue gold with is arbitrary. Gold is a metal that has a particular prevalence on earth. We could just have easily lived on a planet where gold is as common as water. Gold's scarcity and difficulty in production allows it to be used as a currency but insisting that gold be used as an asset backing currency is just one more layer of indirection trying to obfuscate that any currency's value is a social contract that is, in any other respect, arbitrary.","title":"A Note on Gold Backed Currency"},{"location":"Currency-Editorial.html#locally-we-do-not-exist-in-an-energy-equilibrium","text":"One point that comes up is the idea that we might be violating some energy conversation laws by our energy creation or that exponential growth cannot last unfettered. In response to a violation of the conservation of energy, the answer is that we do not live in an energy equilibrium. The sun dumps energy onto the earth in a form that is not easily used by us and is mostly lost by us. The conversion of this unusable energy to a useable form is what is increasing at an exponential pace and does not violate any conservation of energy laws. Our technological pace has been growing exponentially and is most likely going to continue to grow exponentially for some time. Unfettered, indefinite, exponential growth cannot continue, if energy is finite, so this exponential growth will need to halt at some point. Only a sliver of the sun's energy is deposited on earth, with most being dispersed in all other directions. Our sun is one many stars and is one of many celestial bodies that we can extract energy from. Even at a large 2.5% annual increase in energy usage, it would take over 250 years to reach the limits of the sun's energy being deposited on earth. Beyond our earth, the stores of energy that are available are galactic.","title":"Locally, We Do Not Exist in an Energy Equilibrium"},{"location":"Currency-Editorial.html#cryptocurrencies","text":"If people can agree to a crypto-currency's value then it can be used as a currency. All money's value is fundamentally arbitrary. We give a currency it's value by agreeing on how of it to give for a good or service. If instead of using currency that is arbitrary given value by a government, or a currency that is arbitrarily given it's value from a physical material, like gold or silver, then we can use a currency that's given it's value arbitrarily by an agreed on protocol as is present with cryptocurrencies. I'm skeptical that anything other than proof-of-work can be made to be a viable decentralized cryptocurrency, so I'll focus on proof-of-work (PoW) that's used by Bitcoin. The major elements of a currency are satisfied by a cryptocurrency: Fungibility - A piece of cryptocurrency is just as valid as any other Durability - A piece of cryptocurrency can be traded and re-used ad infinitum Divisibility - A piece of cryptocurrency can be sub-divided as need be Portability - Cryptocurrency is inherently information and so is easily communicated and thus easily transportable Legibility - Cryptocurrency can be easily valued and compared as it's just a number Scarcity - is achieved either by limiting the number of cryptocurrency to ever be produced or by inflating the proof of work to match current computational resources A note on divisibility: Bitcoin has a smallest unit of a 'satoshi', just as a United States Dollar has a smallest unit of a cent (0.01 dollar). This divisibility seems like a kind of artifact of an older currency discussion and so I'm not sure how beneficial it is to dive into it. I will say that cryptocurrencies, specifically Bitcoin, are as divisible as all fiat currencies and, I suspect, have the possibility of a software upgrade to be able to further divide the 'satoshi' if need be. Scarcity also warrants a bit more discussion. Bitcoin's proof of work mechanism allows for an ever intensifying work mechanism to make sure that the mining of new Bitcoin and, eventually, steady-state economic transfer and transaction of Bitcoin, will dynamically scale to the available resources. The incentive structure ensures that if there is cheaper compute to gain an advantage, people will use it and there will be a race for who can use the cheapest compute to gain the reward in the cryptocurrency. So Bitcoin uses two scarcity mechanism, one that limits the amount of actual Bitcoin produced (around 21M BTC) and the other as a scaled proof of work mechanism. I'm unclear as to the advantages and disadvantages of a finite number of Bitcoin. One advantage of the finite number of Bitcoin is that it's easy to reason about, since the number itself is capped. As I see it, possible disadvantages include: Note dealing with inflation well, as a satoshi might need to be further subdivided to account for large scale inflation Uncapping the amount of cryptocurrency might lead to a less volatile currency as fluctuations can be absorbed by creating more cryptocurrency that wouldn't be possible with a fixed number of potential coins One could imagine a cryptocurrency that yields mined coins on a decreasing power law scale, potentially making the number of theoretical coins infinite while still giving fewer coins as the cryptocurrency matures. Anyway, I have to profess ignorance.","title":"Cryptocurrencies"},{"location":"Currency-Editorial.html#addendum-a-note-on-the-wealthy","text":"One idea is that wealthy individuals are wardens of money rather than owners. The social contract is that people give an individual their money as payment for goods and services but also with the hope that they will use that money for other, potentially societally beneficial, endeavours. If an individual that is a warden of wealth does not use their surplus of money better than how the public trust can, then the wealth should be transferred from the individual to the public trust. There will always be a tension between what is good for society and what is good for the individual. Wealth is a form of power and disproportionate power can warp policy to create more difficulty in separating the wealth from the wealthy individual. There will always be a battle between individuals who want to accumulate wealth that is better served under the direction of the public trust and the public. Disproportionate wealth is not inherently bad just as system of completely flat, equal wealth is not inherently good. The question is what distribution of wealth should we see in a healthy economy or system of government. My opinion is that some wealth aggregation is functionally better for society than insisting a completely flat distribution. We see this with elected officials who wield a disproportionate amount of power. There are limits to terms of officials and limits to their power, but the reason they are allowed an aggregation of power is to enact the public will they've been elected for. So too can this be with wealth.","title":"Addendum: A Note on the Wealthy"},{"location":"Currency-Editorial.html#references","text":"0 ( a ) 1","title":"References"},{"location":"Currency-Editorial.html#2021-03-16","text":"","title":"2021-03-16"},{"location":"Diophantine-Approximation.html","text":"Diophantine Approximation Dirichlet's Approximation Theorem $$ \\begin{array}{c} \\forall \\alpha \\in \\mathbb{R}, \\forall n \\in \\mathbb{Z}_{+} \\\\ \\exists p, q \\in \\mathbb{Z}_{+}, \\ \\ 1 \\le q \\le n \\\\ \\to 0 < | q \\alpha - p | < \\frac{1}{n} \\\\ \\end{array} $$ Consider $$ \\begin{array}{c} r_q = q \\alpha - \\lfloor q \\alpha \\rfloor \\\\ r_q \\in [0,1) \\\\ \\end{array} $$ Allow for $r_0 = 0$ and there are $(n+1)$ points for $(0 \\le q \\le n)$, $r_q$, in the unit interval. By pigeonhole, there must be two that fall in some interval $[\\frac{s}{n}, \\frac{s+1}{n})$ for $0 \\le s < n$. Call the two points $r_m$ and $r_l$, $\\ m > l$. $$ \\begin{array}{cl} & |r_m - r_l| < \\frac{1}{n} \\\\ \\to & | \\{ m \\alpha \\} - \\{ l \\alpha \\} | < \\frac{1}{n} \\\\ \\to & | (m \\alpha - \\lfloor m \\alpha \\rfloor) - (l \\alpha - \\lfloor l \\alpha \\rfloor) | < \\frac{1}{n} \\\\ \\to & | (m - l) \\alpha - (\\lfloor m \\alpha \\rfloor - \\lfloor l \\alpha \\rfloor ) | < \\frac{1}{n} \\\\ \\end{array} $$ $$ \\begin{array}{cl} & q' = m - l \\in \\mathbb{Z} \\\\ & p' = (\\lfloor m \\alpha \\rfloor - \\lfloor l \\alpha \\rfloor ) \\in \\mathbb{Z} \\\\ \\to & | q' \\alpha - p' | < \\frac{1}{n} \\end{array} $$ So long as $\\alpha$ is irrational: $$ \\begin{array}{cl} & | \\alpha - \\frac{p}{q} | < \\frac{1}{q n} < \\frac{1}{n} \\\\ \\to & | \\alpha - \\frac{p}{q} | > \\frac{1}{C} \\end{array} $$ For some constant, $C$. We can now pick an $n'$ s.t.: $$ \\begin{array}{cl} & \\frac{1}{n'} < \\frac{1}{C} \\\\ \\to & | \\alpha - \\frac{p'}{q'} | < \\frac{1}{C} \\to & q' \\ne q \\end{array} $$ So there are infinitely many pairs, ${p,q}$ s.t. $ | q \\alpha - p | < \\frac{1}{n} $. Rearranging, $$ \\begin{array}{cl} & | \\alpha - \\frac{p}{q} | < \\frac{1}{q n} < \\frac{1}{q^2} \\\\ \\to & | \\alpha - \\frac{p}{q} | < \\frac{1}{q^2} \\end{array} $$ Note that this only says \"there exists\" a $q$ and is not a relation for all $q$. $$ \\begin{array}{cl} & \\forall \\alpha \\in \\mathbb{R}, \\forall n_0 \\in \\mathbb{Z}_{+} \\\\ & \\exists p,q \\in \\mathbb{Z}_{+}, \\ \\ q > n_0 \\\\ \\to & | \\alpha - \\frac{p}{q} | < \\frac{1}{q^2} \\end{array} $$ Here is an example for $2^{\\frac{1}{4}}$: #!/usr/bin/python3 import gmpy2 with gmpy2.local_context(gmpy2.coneext(), precision=1000) as ctx: x = gmpy2.sqrt(gmpy2.sqrt(2)) N = 1000000 for q in range(1,N): pf = gmpy2.floor(q*x) pc = gmpy2.ceil(q*x) r = q*x - pf rp = pc - q*x if (r<rp): y = x - (pf/gmpy2.mpfr(q)) print(q,y) else: y = (pc/gmpy2.mpfr(q)) - x print(q,y) Liouville Bounds For $\\alpha$ the root of a polynomial of degree $n$: $$ \\begin{array}{cc} \\forall p(x) = \\sum_{k=0}^{n} c_k x^k, & c_k \\in \\mathbb{Z}, \\ \\ p(\\alpha) = 0 \\\\ \\exists A > 0, & \\forall p,q \\in \\mathbb{Z}_{+} \\\\ | \\alpha - \\frac{p}{a}| > \\frac{A}{q^n} \\end{array} $$ $\\alpha_k$ the roots of $p(x)$ above, with $\\alpha$ distinct from the rest, and $M$ is the maximum value of $|p'(x)|$ on the interval $[\\alpha-1,\\alpha+1]$: $$ A < \\text{min} \\left( 1, \\frac{1}{M}, | \\alpha - \\alpha_1|, | \\alpha - \\alpha_2|, \\cdots, |\\alpha - \\alpha_{n-1}| \\right) $$ Assume, for contradiction, some $p,q$, with $p(\\frac{p}{q}) \\ne 0 $ s.t.: $$ | \\alpha - \\frac{p}{a} | \\le \\frac{A}{q^n} \\le A < \\text{min} \\left( 1, \\frac{1}{M}, | \\alpha - \\alpha_1|, | \\alpha - \\alpha_2|, \\cdots, |\\alpha - \\alpha_{n-1}| \\right) $$ My the mean value theorem: $$ \\begin{array}{cc} & p(\\alpha) - p(\\frac{p}{q}) = (\\alpha - \\frac{p}{q}) \\cdot p'(x_0) \\\\ \\to & | \\alpha - \\frac{p}{q}| = \\frac{ |p(\\alpha) - p(\\frac{p}{q}) }{ | p'(x_0) | } \\\\ \\to & | \\alpha - \\frac{p}{q}| = | \\frac{p(\\frac{p}{q})}{ p'(x_0) } | \\\\ \\to & | \\alpha - \\frac{p}{q}| = | \\frac{ \\sum_{k=0}^{n} c_k p^k q^{-k} }{ p'(x_0) } | \\\\ \\to & | \\alpha - \\frac{p}{q}| = \\frac{1}{q^n} | \\frac{ \\sum_{k=0}^{n} c_k p^k q^{n-k} }{ p'(x_0) } | \\ge \\frac{ \\frac{1}{q^n} }{ p'(x_0) } \\\\ \\to & | \\alpha - \\frac{p}{q}| \\ge \\frac{ \\frac{1}{q^n} }{ p'(x_0) } \\ge \\frac{ \\frac{1}{q^n} }{ M } > \\frac{A}{q^n} \\ge | \\alpha - \\frac{p}{q} | \\\\ \\to & | \\alpha - \\frac{p}{q}| > | \\alpha - \\frac{p}{q} | \\\\ \\end{array} $$ A contradiction, so the inequality holds. 2020-05-22","title":"Diophantine Approximation"},{"location":"Diophantine-Approximation.html#diophantine-approximation","text":"","title":"Diophantine Approximation"},{"location":"Diophantine-Approximation.html#dirichlets-approximation-theorem","text":"$$ \\begin{array}{c} \\forall \\alpha \\in \\mathbb{R}, \\forall n \\in \\mathbb{Z}_{+} \\\\ \\exists p, q \\in \\mathbb{Z}_{+}, \\ \\ 1 \\le q \\le n \\\\ \\to 0 < | q \\alpha - p | < \\frac{1}{n} \\\\ \\end{array} $$ Consider $$ \\begin{array}{c} r_q = q \\alpha - \\lfloor q \\alpha \\rfloor \\\\ r_q \\in [0,1) \\\\ \\end{array} $$ Allow for $r_0 = 0$ and there are $(n+1)$ points for $(0 \\le q \\le n)$, $r_q$, in the unit interval. By pigeonhole, there must be two that fall in some interval $[\\frac{s}{n}, \\frac{s+1}{n})$ for $0 \\le s < n$. Call the two points $r_m$ and $r_l$, $\\ m > l$. $$ \\begin{array}{cl} & |r_m - r_l| < \\frac{1}{n} \\\\ \\to & | \\{ m \\alpha \\} - \\{ l \\alpha \\} | < \\frac{1}{n} \\\\ \\to & | (m \\alpha - \\lfloor m \\alpha \\rfloor) - (l \\alpha - \\lfloor l \\alpha \\rfloor) | < \\frac{1}{n} \\\\ \\to & | (m - l) \\alpha - (\\lfloor m \\alpha \\rfloor - \\lfloor l \\alpha \\rfloor ) | < \\frac{1}{n} \\\\ \\end{array} $$ $$ \\begin{array}{cl} & q' = m - l \\in \\mathbb{Z} \\\\ & p' = (\\lfloor m \\alpha \\rfloor - \\lfloor l \\alpha \\rfloor ) \\in \\mathbb{Z} \\\\ \\to & | q' \\alpha - p' | < \\frac{1}{n} \\end{array} $$ So long as $\\alpha$ is irrational: $$ \\begin{array}{cl} & | \\alpha - \\frac{p}{q} | < \\frac{1}{q n} < \\frac{1}{n} \\\\ \\to & | \\alpha - \\frac{p}{q} | > \\frac{1}{C} \\end{array} $$ For some constant, $C$. We can now pick an $n'$ s.t.: $$ \\begin{array}{cl} & \\frac{1}{n'} < \\frac{1}{C} \\\\ \\to & | \\alpha - \\frac{p'}{q'} | < \\frac{1}{C} \\to & q' \\ne q \\end{array} $$ So there are infinitely many pairs, ${p,q}$ s.t. $ | q \\alpha - p | < \\frac{1}{n} $. Rearranging, $$ \\begin{array}{cl} & | \\alpha - \\frac{p}{q} | < \\frac{1}{q n} < \\frac{1}{q^2} \\\\ \\to & | \\alpha - \\frac{p}{q} | < \\frac{1}{q^2} \\end{array} $$ Note that this only says \"there exists\" a $q$ and is not a relation for all $q$. $$ \\begin{array}{cl} & \\forall \\alpha \\in \\mathbb{R}, \\forall n_0 \\in \\mathbb{Z}_{+} \\\\ & \\exists p,q \\in \\mathbb{Z}_{+}, \\ \\ q > n_0 \\\\ \\to & | \\alpha - \\frac{p}{q} | < \\frac{1}{q^2} \\end{array} $$ Here is an example for $2^{\\frac{1}{4}}$: #!/usr/bin/python3 import gmpy2 with gmpy2.local_context(gmpy2.coneext(), precision=1000) as ctx: x = gmpy2.sqrt(gmpy2.sqrt(2)) N = 1000000 for q in range(1,N): pf = gmpy2.floor(q*x) pc = gmpy2.ceil(q*x) r = q*x - pf rp = pc - q*x if (r<rp): y = x - (pf/gmpy2.mpfr(q)) print(q,y) else: y = (pc/gmpy2.mpfr(q)) - x print(q,y)","title":"Dirichlet's Approximation Theorem"},{"location":"Diophantine-Approximation.html#liouville-bounds","text":"For $\\alpha$ the root of a polynomial of degree $n$: $$ \\begin{array}{cc} \\forall p(x) = \\sum_{k=0}^{n} c_k x^k, & c_k \\in \\mathbb{Z}, \\ \\ p(\\alpha) = 0 \\\\ \\exists A > 0, & \\forall p,q \\in \\mathbb{Z}_{+} \\\\ | \\alpha - \\frac{p}{a}| > \\frac{A}{q^n} \\end{array} $$ $\\alpha_k$ the roots of $p(x)$ above, with $\\alpha$ distinct from the rest, and $M$ is the maximum value of $|p'(x)|$ on the interval $[\\alpha-1,\\alpha+1]$: $$ A < \\text{min} \\left( 1, \\frac{1}{M}, | \\alpha - \\alpha_1|, | \\alpha - \\alpha_2|, \\cdots, |\\alpha - \\alpha_{n-1}| \\right) $$ Assume, for contradiction, some $p,q$, with $p(\\frac{p}{q}) \\ne 0 $ s.t.: $$ | \\alpha - \\frac{p}{a} | \\le \\frac{A}{q^n} \\le A < \\text{min} \\left( 1, \\frac{1}{M}, | \\alpha - \\alpha_1|, | \\alpha - \\alpha_2|, \\cdots, |\\alpha - \\alpha_{n-1}| \\right) $$ My the mean value theorem: $$ \\begin{array}{cc} & p(\\alpha) - p(\\frac{p}{q}) = (\\alpha - \\frac{p}{q}) \\cdot p'(x_0) \\\\ \\to & | \\alpha - \\frac{p}{q}| = \\frac{ |p(\\alpha) - p(\\frac{p}{q}) }{ | p'(x_0) | } \\\\ \\to & | \\alpha - \\frac{p}{q}| = | \\frac{p(\\frac{p}{q})}{ p'(x_0) } | \\\\ \\to & | \\alpha - \\frac{p}{q}| = | \\frac{ \\sum_{k=0}^{n} c_k p^k q^{-k} }{ p'(x_0) } | \\\\ \\to & | \\alpha - \\frac{p}{q}| = \\frac{1}{q^n} | \\frac{ \\sum_{k=0}^{n} c_k p^k q^{n-k} }{ p'(x_0) } | \\ge \\frac{ \\frac{1}{q^n} }{ p'(x_0) } \\\\ \\to & | \\alpha - \\frac{p}{q}| \\ge \\frac{ \\frac{1}{q^n} }{ p'(x_0) } \\ge \\frac{ \\frac{1}{q^n} }{ M } > \\frac{A}{q^n} \\ge | \\alpha - \\frac{p}{q} | \\\\ \\to & | \\alpha - \\frac{p}{q}| > | \\alpha - \\frac{p}{q} | \\\\ \\end{array} $$ A contradiction, so the inequality holds.","title":"Liouville Bounds"},{"location":"Diophantine-Approximation.html#2020-05-22","text":"","title":"2020-05-22"},{"location":"Distance-Metrics.html","text":"Distance Metrics $$ \\begin{array}{ll} X = (x _ 0, x _ 1, \\cdots, x _ {n-1} ) & \\in \\mathbb{R}^n \\ Y = (y _ 0, y _ 1, \\cdots, y _ {n-1} ) & \\in \\mathbb{R}^n \\ \\end{array} $$ Chebyshev $$ \\begin{array}{ll} || X - Y || _ {\\infty} & = \\max _ i | x _ i - y _ i | \\end{array} $$ wiki Cosine $$ \\begin{array}{ll} ||X|| \\cdot ||Y|| \\cos(\\theta) & = \\frac{\\sum _ {i=0}^{n-1} x _ i \\cdot y _ i}{ \\left( \\sum _ {i=0}^{n-1} x _ i ^ 2 \\right)^{\\frac{1}{2}} \\cdot \\left( \\sum _ {i=0}^{n-1} y _ i ^ 2 \\right)^{\\frac{1}{2}} } \\end{array} $$ wiki Euclidean $$ \\begin{array}{ll} || X - Y || _ {2} & = \\left( \\sum _ {i=0} ^ {n-1} | x _ i - y _ i |^2 \\right)^{\\frac{1}{2}} \\end{array} $$ Manhattan $$ \\begin{array}{ll} || X - Y || _ {1} & = \\sum _ {i=0} ^ {n-1} | x _ i - y _ i | \\end{array} $$ Minkowski $$ \\begin{array}{ll} || X - Y || _ {q} & = \\left( \\sum _ {i=0} ^ {n-1} | x _ i - y _ i |^q \\right)^{\\frac{1}{q}} \\end{array} $$ wiki $|| X - Y || _ 1$ : Manhattan distance ( wiki ) $|| X - Y || _ 2$ : Euclidean distance ( wiki ) $|| X - Y || _ {\\infty}$ : Chebyshev distance ( wiki ) 2024-01-08","title":"Distance Metrics"},{"location":"Distance-Metrics.html#distance-metrics","text":"$$ \\begin{array}{ll} X = (x _ 0, x _ 1, \\cdots, x _ {n-1} ) & \\in \\mathbb{R}^n \\ Y = (y _ 0, y _ 1, \\cdots, y _ {n-1} ) & \\in \\mathbb{R}^n \\ \\end{array} $$","title":"Distance Metrics"},{"location":"Distance-Metrics.html#chebyshev","text":"$$ \\begin{array}{ll} || X - Y || _ {\\infty} & = \\max _ i | x _ i - y _ i | \\end{array} $$ wiki","title":"Chebyshev"},{"location":"Distance-Metrics.html#cosine","text":"$$ \\begin{array}{ll} ||X|| \\cdot ||Y|| \\cos(\\theta) & = \\frac{\\sum _ {i=0}^{n-1} x _ i \\cdot y _ i}{ \\left( \\sum _ {i=0}^{n-1} x _ i ^ 2 \\right)^{\\frac{1}{2}} \\cdot \\left( \\sum _ {i=0}^{n-1} y _ i ^ 2 \\right)^{\\frac{1}{2}} } \\end{array} $$ wiki","title":"Cosine"},{"location":"Distance-Metrics.html#euclidean","text":"$$ \\begin{array}{ll} || X - Y || _ {2} & = \\left( \\sum _ {i=0} ^ {n-1} | x _ i - y _ i |^2 \\right)^{\\frac{1}{2}} \\end{array} $$","title":"Euclidean"},{"location":"Distance-Metrics.html#manhattan","text":"$$ \\begin{array}{ll} || X - Y || _ {1} & = \\sum _ {i=0} ^ {n-1} | x _ i - y _ i | \\end{array} $$","title":"Manhattan"},{"location":"Distance-Metrics.html#minkowski","text":"$$ \\begin{array}{ll} || X - Y || _ {q} & = \\left( \\sum _ {i=0} ^ {n-1} | x _ i - y _ i |^q \\right)^{\\frac{1}{q}} \\end{array} $$ wiki $|| X - Y || _ 1$ : Manhattan distance ( wiki ) $|| X - Y || _ 2$ : Euclidean distance ( wiki ) $|| X - Y || _ {\\infty}$ : Chebyshev distance ( wiki )","title":"Minkowski"},{"location":"Distance-Metrics.html#2024-01-08","text":"","title":"2024-01-08"},{"location":"Empirical-Data-and-Prediction-Estimates.html","text":"Empirical Data and Prediction Estimates World Economy Market capitalization is the outstanding shares times the share price. As of 2014, the estimated total market capitalization of the world's domestic companies is in the $64 trillion range. (64 * 10^12). From the same world bank chart as above, the USA is at around $30T with China at second with $6.3T, Japan at third with $5.3T. This means the USA is 3x more than the next highest and around 8x the third highest. Country Market Cap. (2017) USA $30T China $6.3T Japan $5.3T Bitcoin Estimates As of Dec. 3, 2017 16,718,425 BTC * $11,404.50 = 190665277912.50 or around $190B. For comparison, Amazon has a market capitalization now of $560B. If we assume Bitcoin grows to 10% of the total global market capitalization, that would put it at about $5T. Assuming all 21M bitcoins get mined, that puts the total worth of each Bitcoin at $5T / 21M or approximately $238k/BTC . Population Country Population (2019) World 7.63B China 1.43B India 1.35B USA 327M Wikipedia 2019-10-14","title":"Empirical Data and Prediction Estimates"},{"location":"Empirical-Data-and-Prediction-Estimates.html#empirical-data-and-prediction-estimates","text":"","title":"Empirical Data and Prediction Estimates"},{"location":"Empirical-Data-and-Prediction-Estimates.html#world-economy","text":"Market capitalization is the outstanding shares times the share price. As of 2014, the estimated total market capitalization of the world's domestic companies is in the $64 trillion range. (64 * 10^12). From the same world bank chart as above, the USA is at around $30T with China at second with $6.3T, Japan at third with $5.3T. This means the USA is 3x more than the next highest and around 8x the third highest. Country Market Cap. (2017) USA $30T China $6.3T Japan $5.3T","title":"World Economy"},{"location":"Empirical-Data-and-Prediction-Estimates.html#bitcoin-estimates","text":"As of Dec. 3, 2017 16,718,425 BTC * $11,404.50 = 190665277912.50 or around $190B. For comparison, Amazon has a market capitalization now of $560B. If we assume Bitcoin grows to 10% of the total global market capitalization, that would put it at about $5T. Assuming all 21M bitcoins get mined, that puts the total worth of each Bitcoin at $5T / 21M or approximately $238k/BTC .","title":"Bitcoin Estimates"},{"location":"Empirical-Data-and-Prediction-Estimates.html#population","text":"Country Population (2019) World 7.63B China 1.43B India 1.35B USA 327M Wikipedia","title":"Population"},{"location":"Empirical-Data-and-Prediction-Estimates.html#2019-10-14","text":"","title":"2019-10-14"},{"location":"Empirical-Laws.html","text":"Empirical Laws Conway's Law Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure. A not so clear implication is that dividing work into subgroups allows for specialization and parallelism at the cost of restricting the possibility space by imposing a communication interface between different modules of the organization chart. Future organization charts inherit past organization chart structures and the resulting code ends up being a merge of the two. Wikipedia Gall's Law A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system. Wikipedia Hanlon's Razor Never attribute to malice that which is adequately explained by stupidity. Wikipedia Gervais Principle Sociopaths, in their own best interests, knowingly promote over-performing losers into middle-management, groom under-performing losers into sociopaths, and leave the average bare-minimum-effort losers to fend for themselves. Note: \"The Losers are not social losers (as in the opposite of \u201ccool\u201d), but people who have struck bad bargains economically \u2013 giving up capitalist striving for steady paychecks.\" ( source , archive ) Gilder's Law Bandwidth grows at least three times faster than computer power ( source , archive ) Koomey's Law The number of computations per joule of energy dissipated doubled about every 1.57 years. Note: The scaling looks to have slowed to about 2.6 years as of 2000. Wikipedia Metcalfe's Law The effect of a telecommunications network is proportional to the square of the number of connected users of the system. Wikipedia Moore's Law The number of transistors in a dense integrated circuit doubles about every two years. Wikipedia Norton's Law All data, over time, approaches deleted, or public. ( tweet , archive ) Occam's Razor Simpler solutions are more likely to be correct than complex ones. Wikipedia Pareto's Principle For many events, roughly 80% of the effects come from 20% of the causes. Wikipedia Peter Principle People in a hierarchy tend to rise to their \"level of incompetence\". Wikipedia Planck's Principle Scientific change does not occur because individual scientists change their mind, but rather that successive generations of scientists have different views. Wikipedia Proebsting's Law Compiler Advances Double Computing Power Every 18 Years ( source , archive ) Reed's Law The utility of a group forming network of $n$ members scales as $O(2^n)$. There are $2^n$ subgroups of a set of $n$ members. Whereas Sarnoff's law is one to many, thus linear. Metcalf's law is many to many, but only on an individual connection basis, like a telephone network. Reed's law considers subgroup formation as an added factor to the utility and points out that Metcalf's law might undervalue network effects significantly. Wikipedia Sinclair's Law It is difficult to get a person to understand something, when their salary depends on not understanding it. Wikipedia Sarnoff's Law The value of a broadcast network is proportional to the number of viewers. That is, the value of a network, in this above case a broadcast network, scales as $O(n)$. Wikipedia Spolsky's Observation Smart companies try to commoditize their products' complements. Note: In this context, there are 'substitutes' and 'compliments', where a substitute is a product you buy that replaces the core product and a compliment is a product you buy with the core product. ( source , archive ) Ucaetano's Addendum to Spolsky's Observation Create a desert of profitability around you. ( source , archive ) Swanson's Law The price of solar photovoltaic modules tends to drop 20 percent for every doubling of cumulative shipped volume. Note: at present, costs halve at every 10 years. Wikipedia Wagner's Law The advent of modern industrial society will result in increasing political pressure for social progress and increased allowance for social consideration by industry. Wikipedia Wright's Law Production doubling leads to a constant percent decrease in cost. aka Experience curve effects. The above is paraphrased. Wright noticed that every time total aircraft production doubled, the required labor cost fell 20%. For unit number $n$, cost $P_n$ is approximately: $$ P_n = P_1 n^{ \\lg(\\beta) } = P_1 n^{-\\alpha} $$ Where $(1-\\beta)$ is the reduction proportion and $\\alpha$ is the \"elasticity of cost\" ($\\alpha = -\\lg(\\beta)$). Note that $ \\frac{ P_{2n} }{P_n} = 2^{-\\alpha} $ It appears that $(1-\\beta)$ estimates can typically range from $0.1$ to $0.25$. Also note that economies of scale might be intertwined with with the experience curve effects and that it might be hard to separate the two. Wikipedia Wu's Razor For sufficiently powerful actors, Hanlon's Razor is invoked to give malice plausible deniability. The above is paraphrased from the original: I've come to the conclusion Hanlon's Razor isn't particularly useful. It's catchy, popular- but as a cognitive tool it's almost never applicable and it's usually invoked not to aid the determination of truth, but as rhetorical squid ink to give malice plausible deniability. ( tweet , archive ) 2019-03-18","title":"Empirical Laws"},{"location":"Empirical-Laws.html#empirical-laws","text":"","title":"Empirical Laws"},{"location":"Empirical-Laws.html#conways-law","text":"Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure. A not so clear implication is that dividing work into subgroups allows for specialization and parallelism at the cost of restricting the possibility space by imposing a communication interface between different modules of the organization chart. Future organization charts inherit past organization chart structures and the resulting code ends up being a merge of the two. Wikipedia","title":"Conway's Law"},{"location":"Empirical-Laws.html#galls-law","text":"A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system. Wikipedia","title":"Gall's Law"},{"location":"Empirical-Laws.html#hanlons-razor","text":"Never attribute to malice that which is adequately explained by stupidity. Wikipedia","title":"Hanlon's Razor"},{"location":"Empirical-Laws.html#gervais-principle","text":"Sociopaths, in their own best interests, knowingly promote over-performing losers into middle-management, groom under-performing losers into sociopaths, and leave the average bare-minimum-effort losers to fend for themselves. Note: \"The Losers are not social losers (as in the opposite of \u201ccool\u201d), but people who have struck bad bargains economically \u2013 giving up capitalist striving for steady paychecks.\" ( source , archive )","title":"Gervais Principle"},{"location":"Empirical-Laws.html#gilders-law","text":"Bandwidth grows at least three times faster than computer power ( source , archive )","title":"Gilder's Law"},{"location":"Empirical-Laws.html#koomeys-law","text":"The number of computations per joule of energy dissipated doubled about every 1.57 years. Note: The scaling looks to have slowed to about 2.6 years as of 2000. Wikipedia","title":"Koomey's Law"},{"location":"Empirical-Laws.html#metcalfes-law","text":"The effect of a telecommunications network is proportional to the square of the number of connected users of the system. Wikipedia","title":"Metcalfe's Law"},{"location":"Empirical-Laws.html#moores-law","text":"The number of transistors in a dense integrated circuit doubles about every two years. Wikipedia","title":"Moore's Law"},{"location":"Empirical-Laws.html#nortons-law","text":"All data, over time, approaches deleted, or public. ( tweet , archive )","title":"Norton's Law"},{"location":"Empirical-Laws.html#occams-razor","text":"Simpler solutions are more likely to be correct than complex ones. Wikipedia","title":"Occam's Razor"},{"location":"Empirical-Laws.html#paretos-principle","text":"For many events, roughly 80% of the effects come from 20% of the causes. Wikipedia","title":"Pareto's Principle"},{"location":"Empirical-Laws.html#peter-principle","text":"People in a hierarchy tend to rise to their \"level of incompetence\". Wikipedia","title":"Peter Principle"},{"location":"Empirical-Laws.html#plancks-principle","text":"Scientific change does not occur because individual scientists change their mind, but rather that successive generations of scientists have different views. Wikipedia","title":"Planck's Principle"},{"location":"Empirical-Laws.html#proebstings-law","text":"Compiler Advances Double Computing Power Every 18 Years ( source , archive )","title":"Proebsting's Law"},{"location":"Empirical-Laws.html#reeds-law","text":"The utility of a group forming network of $n$ members scales as $O(2^n)$. There are $2^n$ subgroups of a set of $n$ members. Whereas Sarnoff's law is one to many, thus linear. Metcalf's law is many to many, but only on an individual connection basis, like a telephone network. Reed's law considers subgroup formation as an added factor to the utility and points out that Metcalf's law might undervalue network effects significantly. Wikipedia","title":"Reed's Law"},{"location":"Empirical-Laws.html#sinclairs-law","text":"It is difficult to get a person to understand something, when their salary depends on not understanding it. Wikipedia","title":"Sinclair's Law"},{"location":"Empirical-Laws.html#sarnoffs-law","text":"The value of a broadcast network is proportional to the number of viewers. That is, the value of a network, in this above case a broadcast network, scales as $O(n)$. Wikipedia","title":"Sarnoff's Law"},{"location":"Empirical-Laws.html#spolskys-observation","text":"Smart companies try to commoditize their products' complements. Note: In this context, there are 'substitutes' and 'compliments', where a substitute is a product you buy that replaces the core product and a compliment is a product you buy with the core product. ( source , archive )","title":"Spolsky's Observation"},{"location":"Empirical-Laws.html#ucaetanos-addendum-to-spolskys-observation","text":"Create a desert of profitability around you. ( source , archive )","title":"Ucaetano's Addendum to Spolsky's Observation"},{"location":"Empirical-Laws.html#swansons-law","text":"The price of solar photovoltaic modules tends to drop 20 percent for every doubling of cumulative shipped volume. Note: at present, costs halve at every 10 years. Wikipedia","title":"Swanson's Law"},{"location":"Empirical-Laws.html#wagners-law","text":"The advent of modern industrial society will result in increasing political pressure for social progress and increased allowance for social consideration by industry. Wikipedia","title":"Wagner's Law"},{"location":"Empirical-Laws.html#wrights-law","text":"Production doubling leads to a constant percent decrease in cost. aka Experience curve effects. The above is paraphrased. Wright noticed that every time total aircraft production doubled, the required labor cost fell 20%. For unit number $n$, cost $P_n$ is approximately: $$ P_n = P_1 n^{ \\lg(\\beta) } = P_1 n^{-\\alpha} $$ Where $(1-\\beta)$ is the reduction proportion and $\\alpha$ is the \"elasticity of cost\" ($\\alpha = -\\lg(\\beta)$). Note that $ \\frac{ P_{2n} }{P_n} = 2^{-\\alpha} $ It appears that $(1-\\beta)$ estimates can typically range from $0.1$ to $0.25$. Also note that economies of scale might be intertwined with with the experience curve effects and that it might be hard to separate the two. Wikipedia","title":"Wright's Law"},{"location":"Empirical-Laws.html#wus-razor","text":"For sufficiently powerful actors, Hanlon's Razor is invoked to give malice plausible deniability. The above is paraphrased from the original: I've come to the conclusion Hanlon's Razor isn't particularly useful. It's catchy, popular- but as a cognitive tool it's almost never applicable and it's usually invoked not to aid the determination of truth, but as rhetorical squid ink to give malice plausible deniability. ( tweet , archive )","title":"Wu's Razor"},{"location":"Empirical-Laws.html#2019-03-18","text":"","title":"2019-03-18"},{"location":"Enabling-Server-HTTPS.html","text":"Enabling HTTPS Using Let's Encrypt Let's Encrypt is a certificate authority that gives out digital certificates to the community free of charge. Using thir \"shell access\" method, one can install a program called certbot that gives quick start instructions for installing and running certbot to issue a certificate for your host and system. Detailed below is what I did for some of my servers (HAProxy on Ubuntu and Apache on Ubuntu). HAProxy on Ubuntu sudo apt-get install software-properties-common sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install certbot certbot certonly --webroot -w /var/www -d meowcad.com -d www.meowcad.com I had to create a meow.pem for my version of HAProxy to work: cd /etc/letsencrypt/live/meowcad.com cat private.pem fullchain.pem > meow.pem Apache on Ubuntu sudo apt-get install software-properties-common sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install python-certbot-apache certbot certonly -n --agree-tos --email $EMAIL --domain mechaelephant.com --domain www.mechaelephant.com --webroot --webroot-path /var/www/ --expand Notes Make sure to add the extra domain so that both www.domain.com and domain.com work. If you've already issued a certificate and want to add a domain like I needed to for www.mechalephant.com , adding the domain mechaelephant.com because I forgot the non www prefixed web page, then I think the --expand flag will work to add an additional domain. I'm still not sure how to renew or automatically renew the certs... 2017-06-12","title":"Enabling Server HTTPS"},{"location":"Enabling-Server-HTTPS.html#enabling-https-using-lets-encrypt","text":"Let's Encrypt is a certificate authority that gives out digital certificates to the community free of charge. Using thir \"shell access\" method, one can install a program called certbot that gives quick start instructions for installing and running certbot to issue a certificate for your host and system. Detailed below is what I did for some of my servers (HAProxy on Ubuntu and Apache on Ubuntu).","title":"Enabling HTTPS Using Let's Encrypt"},{"location":"Enabling-Server-HTTPS.html#haproxy-on-ubuntu","text":"sudo apt-get install software-properties-common sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install certbot certbot certonly --webroot -w /var/www -d meowcad.com -d www.meowcad.com I had to create a meow.pem for my version of HAProxy to work: cd /etc/letsencrypt/live/meowcad.com cat private.pem fullchain.pem > meow.pem","title":"HAProxy on Ubuntu"},{"location":"Enabling-Server-HTTPS.html#apache-on-ubuntu","text":"sudo apt-get install software-properties-common sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install python-certbot-apache certbot certonly -n --agree-tos --email $EMAIL --domain mechaelephant.com --domain www.mechaelephant.com --webroot --webroot-path /var/www/ --expand","title":"Apache on Ubuntu"},{"location":"Enabling-Server-HTTPS.html#notes","text":"Make sure to add the extra domain so that both www.domain.com and domain.com work. If you've already issued a certificate and want to add a domain like I needed to for www.mechalephant.com , adding the domain mechaelephant.com because I forgot the non www prefixed web page, then I think the --expand flag will work to add an additional domain. I'm still not sure how to renew or automatically renew the certs...","title":"Notes"},{"location":"Enabling-Server-HTTPS.html#2017-06-12","text":"","title":"2017-06-12"},{"location":"Energy-Consumption-Stats.html","text":"Energy Consumption Stats From the U.S. Energy Information Administration : Average yearly houshold usage of 10.812 MWh (Megawatthours) ~30 kWh daily household usage Highs of 15.435 MWh (~42.28 kWh/day) to lows of 6.166 MWh (~17 kWh/day). As of now, for a consumer solar it's about $ 1 per Watt of solar. Assuming a 12 hour window of sun at full power, and 36 kWh usage, thats 3 kW or about $ 3k for the solar panels (at about .6x1x.035 m^3). As of now, for consumer deep cycle lead acid batteries, it's about $ .15 / (Watt hour). At 36 kWh, thats about $ 5400. A LiFePO4 32650 5.5 Ah 3.2 V rechargeable battery goes for ~ $ 120.00 / 80 units. At 36 kWh, that works out to about (120 / (80 * 5.5 * 3.2)) * 36000 = $ 3070 This does not include all the control hardware and electronics that are needed for proper usage. As a rough estimate, New York average price for $ .1854 / kWh at 601 kWh / month for about $ 111.32 / month or $ 1335.84 / year. Future Galactic Scale Energy Annual Energy Review Interestingly, I think the blog post was trying to point out the absurdity of continue economic growth but I take it as a rough time frame for our progression on moving up the Kardashev scale. There's some talk about wealth and CO2 emissions going hand in hand, with some suggesting a total civilization collapse (from \"Decline of the Empire\" blog ). One post claims solar produces 20-30x less CO2 than coal , so I'm not sure if apocalyptic forecasts are to be believed. 2017-09-21","title":"Energy Consumption Stats"},{"location":"Energy-Consumption-Stats.html#energy-consumption-stats","text":"From the U.S. Energy Information Administration : Average yearly houshold usage of 10.812 MWh (Megawatthours) ~30 kWh daily household usage Highs of 15.435 MWh (~42.28 kWh/day) to lows of 6.166 MWh (~17 kWh/day). As of now, for a consumer solar it's about $ 1 per Watt of solar. Assuming a 12 hour window of sun at full power, and 36 kWh usage, thats 3 kW or about $ 3k for the solar panels (at about .6x1x.035 m^3). As of now, for consumer deep cycle lead acid batteries, it's about $ .15 / (Watt hour). At 36 kWh, thats about $ 5400. A LiFePO4 32650 5.5 Ah 3.2 V rechargeable battery goes for ~ $ 120.00 / 80 units. At 36 kWh, that works out to about (120 / (80 * 5.5 * 3.2)) * 36000 = $ 3070 This does not include all the control hardware and electronics that are needed for proper usage. As a rough estimate, New York average price for $ .1854 / kWh at 601 kWh / month for about $ 111.32 / month or $ 1335.84 / year.","title":"Energy Consumption Stats"},{"location":"Energy-Consumption-Stats.html#future","text":"Galactic Scale Energy Annual Energy Review Interestingly, I think the blog post was trying to point out the absurdity of continue economic growth but I take it as a rough time frame for our progression on moving up the Kardashev scale. There's some talk about wealth and CO2 emissions going hand in hand, with some suggesting a total civilization collapse (from \"Decline of the Empire\" blog ). One post claims solar produces 20-30x less CO2 than coal , so I'm not sure if apocalyptic forecasts are to be believed.","title":"Future"},{"location":"Energy-Consumption-Stats.html#2017-09-21","text":"","title":"2017-09-21"},{"location":"Energy-Discussion.html","text":"Solar Energy Discussion Energy Limits of Earth All estimates are going to be conservative. $$ \\begin{array}{ll} \\textbf{Earth Radius} & \\approx 6.371 \\cdot 10^6 (m) \\\\ \\textbf{Solar Energy} & \\approx 240 \\frac{W}{m^2} \\\\ \\textbf{Earth Population} & \\approx 7.7 \\cdot 10^9 \\\\ \\textbf{Average Energy Per Person} & \\approx 30 kWh \\text{ daily usage} \\end{array} $$ Approximate the solar energy available on land by underestimating the area by taking the disk as described by the radius of the Earth: $$ \\begin{array}{l} \\pi \\cdot (6.371 \\cdot 10^6)^2 (m^2) \\cdot (240 \\frac{W}{m^2}) \\cdot (24 \\frac{h}{day}) \\\\ \\to ~ 700 \\cdot 10^{15} \\frac{W h}{day} \\end{array} $$ Estimating global population usage: $$ \\begin{array}{l} (7.7 \\cdot 10^9) \\cdot (30 \\cdot 10^3 \\frac{W h}{day}) \\\\ \\to ~ 0.231 \\cdot 10^{15} \\frac{W h}{day} \\end{array} $$ So, as a rough upper bound, the energy requirement of humans that could be \"theoretically\" satisfied by the sun's energy is about an increase of $ \\frac{700}{.231} (\\approx 3030) $ more people, or about a population of 23 trillion people. World Adoption of Solar At a current growth rate of $29\\%$ with $2.1\\%$ of the population being supplied by solar, an estimate for the full adoption would be: $$ \\begin{array}{l} 2.1 \\cdot (1.29)^y = 100 \\\\ \\to y = \\frac{ \\ln(\\frac{100}{2.1}) }{ \\ln(1.29) } \\\\ \\to y \\approx 15.17 \\end{array} $$ Or about 15 years till we see a large scale rollout and adoption. Some caveats are that this assumes exponential growth for the entirety of the adoption where the more likely event is some \"s-curve\" or other logistic curve as it nears full adoption. It further assumes constant $29\\%$ growth. Also note that the vast majority of this growth is from China production. World Energy Consumption One estimate for the energy consumption used by the world is about a $3\\%$ annual increase, globally. Here a plot of energy usage globally, taken from the EIA . Here is the energy usage for the USA: Some caveats: The units are different in the two different graphs because of differing sources for the data China makes up for most of the energy usage, as do other developing nations, whereas the USA and Europe have slowed to less than $2\\%$ energy usage The USA had some sort of change in the late 1970s and went from $3\\%+$ energy usage to around $1\\%$ energy usage If a conservative $3\\%$ annual increase is taken, the numbers above for the suns available energy on the earth and the current energy consumption of the worlds population, we can get an estimate for when the energy consumption will exceed the earths available energy from the sun: $$ \\begin{array}{l} \\frac{700 \\cdot 10^15 \\cdot \\frac{W h}{day} }{ .231 \\cdot 10^{15} \\frac{W h}{day} } \\\\ \\approx 3030 \\end{array} $$ $$ \\begin{array}{l} \\exp( y \\cdot \\ln(1.03) ) = 3030 \\\\ \\to y = \\frac{ \\ln(3030) }{ \\ln(1.03) } \\\\ \\to y \\approx 271 \\text{ years } \\end{array} $$ Which gives us a rough estimate of 270 years for us to reach a \"Type I\" civilization on the Kardashev scale. References Earth Radius Solar Energy Potential Global Population Growth of Photovoltaics ( source ) World Energy Usage Country Energy Usage Kardashev scale 2021-04-24","title":"Energy Discussion"},{"location":"Energy-Discussion.html#solar-energy-discussion","text":"","title":"Solar Energy Discussion"},{"location":"Energy-Discussion.html#energy-limits-of-earth","text":"All estimates are going to be conservative. $$ \\begin{array}{ll} \\textbf{Earth Radius} & \\approx 6.371 \\cdot 10^6 (m) \\\\ \\textbf{Solar Energy} & \\approx 240 \\frac{W}{m^2} \\\\ \\textbf{Earth Population} & \\approx 7.7 \\cdot 10^9 \\\\ \\textbf{Average Energy Per Person} & \\approx 30 kWh \\text{ daily usage} \\end{array} $$ Approximate the solar energy available on land by underestimating the area by taking the disk as described by the radius of the Earth: $$ \\begin{array}{l} \\pi \\cdot (6.371 \\cdot 10^6)^2 (m^2) \\cdot (240 \\frac{W}{m^2}) \\cdot (24 \\frac{h}{day}) \\\\ \\to ~ 700 \\cdot 10^{15} \\frac{W h}{day} \\end{array} $$ Estimating global population usage: $$ \\begin{array}{l} (7.7 \\cdot 10^9) \\cdot (30 \\cdot 10^3 \\frac{W h}{day}) \\\\ \\to ~ 0.231 \\cdot 10^{15} \\frac{W h}{day} \\end{array} $$ So, as a rough upper bound, the energy requirement of humans that could be \"theoretically\" satisfied by the sun's energy is about an increase of $ \\frac{700}{.231} (\\approx 3030) $ more people, or about a population of 23 trillion people.","title":"Energy Limits of Earth"},{"location":"Energy-Discussion.html#world-adoption-of-solar","text":"At a current growth rate of $29\\%$ with $2.1\\%$ of the population being supplied by solar, an estimate for the full adoption would be: $$ \\begin{array}{l} 2.1 \\cdot (1.29)^y = 100 \\\\ \\to y = \\frac{ \\ln(\\frac{100}{2.1}) }{ \\ln(1.29) } \\\\ \\to y \\approx 15.17 \\end{array} $$ Or about 15 years till we see a large scale rollout and adoption. Some caveats are that this assumes exponential growth for the entirety of the adoption where the more likely event is some \"s-curve\" or other logistic curve as it nears full adoption. It further assumes constant $29\\%$ growth. Also note that the vast majority of this growth is from China production.","title":"World Adoption of Solar"},{"location":"Energy-Discussion.html#world-energy-consumption","text":"One estimate for the energy consumption used by the world is about a $3\\%$ annual increase, globally. Here a plot of energy usage globally, taken from the EIA . Here is the energy usage for the USA: Some caveats: The units are different in the two different graphs because of differing sources for the data China makes up for most of the energy usage, as do other developing nations, whereas the USA and Europe have slowed to less than $2\\%$ energy usage The USA had some sort of change in the late 1970s and went from $3\\%+$ energy usage to around $1\\%$ energy usage If a conservative $3\\%$ annual increase is taken, the numbers above for the suns available energy on the earth and the current energy consumption of the worlds population, we can get an estimate for when the energy consumption will exceed the earths available energy from the sun: $$ \\begin{array}{l} \\frac{700 \\cdot 10^15 \\cdot \\frac{W h}{day} }{ .231 \\cdot 10^{15} \\frac{W h}{day} } \\\\ \\approx 3030 \\end{array} $$ $$ \\begin{array}{l} \\exp( y \\cdot \\ln(1.03) ) = 3030 \\\\ \\to y = \\frac{ \\ln(3030) }{ \\ln(1.03) } \\\\ \\to y \\approx 271 \\text{ years } \\end{array} $$ Which gives us a rough estimate of 270 years for us to reach a \"Type I\" civilization on the Kardashev scale.","title":"World Energy Consumption"},{"location":"Energy-Discussion.html#references","text":"Earth Radius Solar Energy Potential Global Population Growth of Photovoltaics ( source ) World Energy Usage Country Energy Usage Kardashev scale","title":"References"},{"location":"Energy-Discussion.html#2021-04-24","text":"","title":"2021-04-24"},{"location":"File-Naming-Conventions.html","text":"File Naming Conventions Three principles: Machine readable Human Readable Plays well with default ordering Conventions - (dash) separate words in concept _ (underscore) separate units of meta-data pad numbers to allow for default file sort use YYYY-MM-DD or YYYYMMDD format for dates ( ISO 8601 ) put dates first to help with default ordering (where appropriate) prefer lower case to mixed case prefer upper case to mixed case prefer consistent case Examples Good 2017-08-18_article01.md 2017-08-18_article02.md 2017-08-18_article10.md 2016-01-08_roy_wg.vcf.gz 2016-02-14_priss_wg.vcf.gz 2016-06-12_zora_wg.vcf.gz 2017-04-10_leon_wg.vcf.gz Bad acticle1.md article2.md article10.md priss2142015.vcf.gz leon17410.vcf.gz zora16612.vcf.gz roy1618.vcf.gz priss16214.vcf.gz References naming things by Jenny Bryan 2017-08-18","title":"File Naming Conventions"},{"location":"File-Naming-Conventions.html#file-naming-conventions","text":"Three principles: Machine readable Human Readable Plays well with default ordering","title":"File Naming Conventions"},{"location":"File-Naming-Conventions.html#conventions","text":"- (dash) separate words in concept _ (underscore) separate units of meta-data pad numbers to allow for default file sort use YYYY-MM-DD or YYYYMMDD format for dates ( ISO 8601 ) put dates first to help with default ordering (where appropriate) prefer lower case to mixed case prefer upper case to mixed case prefer consistent case","title":"Conventions"},{"location":"File-Naming-Conventions.html#examples","text":"","title":"Examples"},{"location":"File-Naming-Conventions.html#good","text":"2017-08-18_article01.md 2017-08-18_article02.md 2017-08-18_article10.md 2016-01-08_roy_wg.vcf.gz 2016-02-14_priss_wg.vcf.gz 2016-06-12_zora_wg.vcf.gz 2017-04-10_leon_wg.vcf.gz","title":"Good"},{"location":"File-Naming-Conventions.html#bad","text":"acticle1.md article2.md article10.md priss2142015.vcf.gz leon17410.vcf.gz zora16612.vcf.gz roy1618.vcf.gz priss16214.vcf.gz","title":"Bad"},{"location":"File-Naming-Conventions.html#references","text":"naming things by Jenny Bryan","title":"References"},{"location":"File-Naming-Conventions.html#2017-08-18","text":"","title":"2017-08-18"},{"location":"Fisher-Yates-Shuffle.html","text":"Fisher-Yates Shuffle The Fisher-Yates shuffle algorithm is used to create a random permutation. The derivation is relatively straight forward: function fisher_yates_shuffle(a) { var t, n = a.length; for (var i=0; i<(n-1); i++) { var idx = i + Math.floor(Math.random()*(n-i)); t = a[i]; a[i] = a[idx]; a[idx] = t; } } We choose the first element at random, then proceed to choose subsequent entries from the remaining elements. As a spot check, we can confirm that there are $n!$ configurations yielding approximately $ n (lg(n) - 1) $ bits of entropy. Each poll of the random number generator is for $ lg(n-i) $ bits over $n-1$ entries: $$ lg(2) + lg(3) + \\cdots + lg(n) = \\sum_{k=1}^{n} lg(k) = lg(n!) $$ The Wrong Way One can consider the following incorrect way to do the shuffle: function nofish_shuffle(a) { var t, n = a.length; for (var i=0; i<n; i++) { var idx = Math.floor(Math.random()*n); t = a[i]; a[i] = a[idx]; a[idx] = t; } } a slight variant: function noyaks_shuffle(a) { var t, n = a.length; for (var i=0; i<n; i++) { var idx = Math.floor(Math.random()*(n-1)); if (idx==i) { idx = n-1; } t = a[i]; a[i] = a[idx]; a[idx] = t; } } and another: function nomaar_shuffle(a) { var t, n = a.length; for (var i=0; i<n; i++) { var idx0 = Math.floor(Math.random()*n); var idx1 = Math.floor(Math.random()*n); t = a[idx0]; a[idx0] = a[idx1]; a[idx1] = t; } } Where the difference in nofish_shuffle and noyaks_shuffle is to skip the current index when considering which element to permute. nomaar_shuffle is yet another variant where each two elements are chosen at random and swapped $n$ times. A friend of mine suggested an nice proof to show the above two shuffle algorithms provide incorrect results. As above, there are $n!$ possible shuffles we want to choose from, with equal probability. Since nofish_shuffle is choosing each element to permute from the whole array, there are $n^n$ possible choices for the permutation, where some permutations might be represented more than once. Producing multiple configurations is permissible so long as nofish_shuffle would produce an equal distribution for each of the $n!$ configurations. Since $ n! \\nmid n^n $ for $n>2$, there must be some configurations that appear more often by the pigeonhole principle. noyaks_shuffle doesn't fare much better since there are $n^{n-1}$ possible choices of permutation schedules and $n! \\nmid n^{n-1}$ for $n>2$. The same type of analysis works for the nomaar_shuffle by noticing that the number of permutation schedules is $n^{2 n}$ and that still $n! \\nmid n^{2 n}$. Though hidden in such a large configuration space, nofish_shuffle , noyaks_shuffle and nomaar_shuffle produce configurations that are not uniformly distributed. Addendum Sattalo's algorithm creates a random single cycle permutation. The algorithm is similar to Fisher-Yates but does not allow the choice of the current index element when swapping: function sattalo_shuffle(a) { var t, n = a.length; for (var i=0; i<(n-1); i++) { var idx = i + 1 + Math.floor(Math.random()*(n-i-1)); t = a[i]; a[i] = a[idx]; a[idx] = t; } } There are $(n-1)!$ configurations, so we know the above algorithm subsamples from the space of all permutation possibilities. To see that it produces a single cycle, note that swapping elements has two possibilities: If both elements are in the same cycle, swapping elements creates two disjoint cycles If both elements are in different cycles, swapping elements creates a single cycle The swap step in Sattalo's algorithm can be thought of as swapping a cycle of length one, the current index position, with another cycle pointed to by the chosen random index. Since these are two distinct cycles, they join to create a single cycle. This is done $(n-1)$ times forcing a single large cycle. To see that this draws uniformly from single cycle permutations, proceed inductively by noticing that if a single cycle of length $(n-1)$ is produced uniformly at random, then extending it to a single cycle of length $n$ by the above method will favor each of the $(n-1)$ possible extensions equally. 2018-06-13","title":"Fisher Yates Shuffle"},{"location":"Fisher-Yates-Shuffle.html#fisher-yates-shuffle","text":"The Fisher-Yates shuffle algorithm is used to create a random permutation. The derivation is relatively straight forward: function fisher_yates_shuffle(a) { var t, n = a.length; for (var i=0; i<(n-1); i++) { var idx = i + Math.floor(Math.random()*(n-i)); t = a[i]; a[i] = a[idx]; a[idx] = t; } } We choose the first element at random, then proceed to choose subsequent entries from the remaining elements. As a spot check, we can confirm that there are $n!$ configurations yielding approximately $ n (lg(n) - 1) $ bits of entropy. Each poll of the random number generator is for $ lg(n-i) $ bits over $n-1$ entries: $$ lg(2) + lg(3) + \\cdots + lg(n) = \\sum_{k=1}^{n} lg(k) = lg(n!) $$","title":"Fisher-Yates Shuffle"},{"location":"Fisher-Yates-Shuffle.html#the-wrong-way","text":"One can consider the following incorrect way to do the shuffle: function nofish_shuffle(a) { var t, n = a.length; for (var i=0; i<n; i++) { var idx = Math.floor(Math.random()*n); t = a[i]; a[i] = a[idx]; a[idx] = t; } } a slight variant: function noyaks_shuffle(a) { var t, n = a.length; for (var i=0; i<n; i++) { var idx = Math.floor(Math.random()*(n-1)); if (idx==i) { idx = n-1; } t = a[i]; a[i] = a[idx]; a[idx] = t; } } and another: function nomaar_shuffle(a) { var t, n = a.length; for (var i=0; i<n; i++) { var idx0 = Math.floor(Math.random()*n); var idx1 = Math.floor(Math.random()*n); t = a[idx0]; a[idx0] = a[idx1]; a[idx1] = t; } } Where the difference in nofish_shuffle and noyaks_shuffle is to skip the current index when considering which element to permute. nomaar_shuffle is yet another variant where each two elements are chosen at random and swapped $n$ times. A friend of mine suggested an nice proof to show the above two shuffle algorithms provide incorrect results. As above, there are $n!$ possible shuffles we want to choose from, with equal probability. Since nofish_shuffle is choosing each element to permute from the whole array, there are $n^n$ possible choices for the permutation, where some permutations might be represented more than once. Producing multiple configurations is permissible so long as nofish_shuffle would produce an equal distribution for each of the $n!$ configurations. Since $ n! \\nmid n^n $ for $n>2$, there must be some configurations that appear more often by the pigeonhole principle. noyaks_shuffle doesn't fare much better since there are $n^{n-1}$ possible choices of permutation schedules and $n! \\nmid n^{n-1}$ for $n>2$. The same type of analysis works for the nomaar_shuffle by noticing that the number of permutation schedules is $n^{2 n}$ and that still $n! \\nmid n^{2 n}$. Though hidden in such a large configuration space, nofish_shuffle , noyaks_shuffle and nomaar_shuffle produce configurations that are not uniformly distributed.","title":"The Wrong Way"},{"location":"Fisher-Yates-Shuffle.html#addendum","text":"Sattalo's algorithm creates a random single cycle permutation. The algorithm is similar to Fisher-Yates but does not allow the choice of the current index element when swapping: function sattalo_shuffle(a) { var t, n = a.length; for (var i=0; i<(n-1); i++) { var idx = i + 1 + Math.floor(Math.random()*(n-i-1)); t = a[i]; a[i] = a[idx]; a[idx] = t; } } There are $(n-1)!$ configurations, so we know the above algorithm subsamples from the space of all permutation possibilities. To see that it produces a single cycle, note that swapping elements has two possibilities: If both elements are in the same cycle, swapping elements creates two disjoint cycles If both elements are in different cycles, swapping elements creates a single cycle The swap step in Sattalo's algorithm can be thought of as swapping a cycle of length one, the current index position, with another cycle pointed to by the chosen random index. Since these are two distinct cycles, they join to create a single cycle. This is done $(n-1)$ times forcing a single large cycle. To see that this draws uniformly from single cycle permutations, proceed inductively by noticing that if a single cycle of length $(n-1)$ is produced uniformly at random, then extending it to a single cycle of length $n$ by the above method will favor each of the $(n-1)$ possible extensions equally.","title":"Addendum"},{"location":"Fisher-Yates-Shuffle.html#2018-06-13","text":"","title":"2018-06-13"},{"location":"Food-CO2-Water.html","text":"CO2 and Water Usage of Food Consumed Diet Per Capita Yearly CO2 Emission $(\\frac{kg}{(cap)(yr)})$ Per Capita Yearly Water Consumption $(\\frac{kg}{(cap)(yr)})$ American Omnivore 1422.96 756.97 American Vegetarian 515.53 275.21 American Vegan (2 cup almond milk replacement) 449.97 491.80 American Vegan (2% almond milk replacement) 449.97 231.60 American Omnivore - Meat and milk consumption taken from the National Chicken Council and the USDA American Vegetarian - Meat, pork and poultry replaced with soybean (by weight, assumed 158.49kg of soybean) for CO2 emission. Meat, pork and poultry replaced with rice (by weight, assumed 158.49kg of rice) for water consumption. Cow milk still used (67.59kg) American Vegan (2 cup almond milk replacement0) - Meat, pork and poultry replaced with soybean (by weight, assumed 158.49kg of soybean) for CO2 emission. Meat, pork and poultry replaced with rice (by weight, assumed 158.49kg of rice) for water consumption. Almond milk used with 225g of almonds per litre of almond milk (assuming 67.59 L of almond milk consumed per capita per year) American Vegan (2% almond milk replacement) - Meat, pork and poultry replaced with soybean (by weight, assumed 158.49kg of soybean) for CO2 emission. Meat, pork and poultry replaced with rice (by weight, assumed 158.49kg of rice) for water consumption. Almond milk used with 20g of almonds per litre of almond milk (assuming 67.59 L of almond milk consumed per capita per year) Worked calculations Type Per Capita Yearly Consumption $(kg)$ Per Capita Yearly CO2 Emission $(\\frac{kg}{(cap)(yr)})$ Per Capita Yearly Water Consumption $(\\frac{kg}{(cap)(yr)})$ Meat (beef) 25.85 742.67 324.68 Meat (pork) 23.04 134.78 102.76 Meat (poultry) 109.6 451.55 261.94 Milk (cow) 67.59 93.95 67.59 w = 25.85 + 23.04 + 109.6 = 158.49 milk = 67.59 omni_co2 = (25.85*28.73) + (23.04*5.85) + (109.6*4.12) + (milk*1.39) omni_h2o = (25.85*12.56) + (23.04*4.46) + (109.6*2.39) + (milk*1) veg_co2 = w*2.66 + milk*1.39 veg_h2o = w*1.31 + milk*1 vegan_co2_0 = w*2.66 + milk*0.42 vegan_h2o_0 = w*1.31 + milk*(17.74*.237) vegan_co2_1 = w*2.66 + milk*0.42 vegan_h2o_1 = w*1.31 + milk*(17.74*.02) The following additional notes should be considered: I've found conflicting reports for the number of almonds in almond milk. If it's assumed that there are 225g of almonds per gallon of almond milk, this is estimate 0 above (.237kg almonds per litre). If it's assumed the almond milk is 2% (by weight), then this is estimate 1 above (.02kg almonds per litre). The American Omnivore diet estimates does not include rice, wheat, maize, vegetables or other non-animal products so can be considered a 'lower bound' on per capita CO2 and water consumption. The non American Omnivore entries use soybeans as a proxy for the meat (by weight) for CO2 emission and rice as a proxy for meat (by weight) for water consumption to give a conservative estimate of the CO2 emissions and water usage. References \"Per Capita Consumption of Poultry and Livestock, 1965 to Estimated 2019, in Pounds\" Year Beef (lbs) Pork (lbs) Total Poultry (lbs) 208 57.0 50.8 109.6 This implies 25.85kg of beef, 23.04kg of pork and 49.71kg of poultry for the average American. \"Dairy Data\" The per capita consumption of dairy for Americans : Year Milk Consumption (lbs) 2017 149 149 pounds per person per year is about 67.59kg per person per year. \"Food consumption patterns and their effect on water requirement in China\" by J. Liu and H. H. G. Savenije Food Water footprint $(\\frac{m^3}{kg})$ Per capita annual food consumption (2003) $(\\frac{kg}{(cap)(yr)})$ Rice 1.31 79 Wheat 0.98 61 Maize 0.84 15 Soybeans 3.20 7 Vegetables 0.19 270 Fruits 0.50 50 Beef 12.56 5 Pork 4.46 35 Poultry 2.39 11 Milk 1.00 17 \"Systematic review of greenhouse gas emissions for different fresh food categories\" by Stephen Clune, Enda Crossin, Karli Verghese Food Mean $\\frac{(kg) (CO2\\ eq)}{kg}$ (2015?) Rice 2.66 Wheat 0.51 Maize 0.63 Soybeans 0.58 Vegetables 0.47 Fruits 0.50 Beef 28.73 Pork 5.85 Chicken 4.12 Milk 1.39 Almond/coconut milk 0.42 \"The green, blue and grey water footprint of crops and derived crop products\" by M. M. Mekonnen and A. Y. Hoekstra Food Global average water footprint $(\\frac{m^3}{ton})$ (1996-2005) Global average water footprint ($\\frac{m^3}{kg}$ (1996-2005)) Almonds 16095 17.74 How many ounces of almonds go into making each half-gallon of almond milk (example: Silk or Almond Breeze brand)? As a conservative estimate of the almonds used in almond milk: Around 2 cups of almonds for a half gallon. Around 225 g per cup of ground almonds . 2 cups almonds = 450 g almonds. 1 gallon = 3.79 kg (water). 1/2 gallon = 1.90 kg 450 g almonds per 1.90 kg of almond milk .237 kg of almonds per 1 kg of almond milk As a potentially more realistic estimate of the number of almonds used in almond milk: If almond milk has 2% of almonds, by weight, then for every litre of almond milk, there are approximately 20g of almonds .02 kg of almonds per 1kg of almond milk Greenhouse Gas Emissions from a Typical Passenger Vehicle $4600 \\frac{kg}{yr}$ of CO2 emissions for the typical US passenger vehicle. \"Almond milk: quite good for you \u2013 very bad for the planet\" by Emine Saner \"Your Almond Habit Is Sucking California Dry\" by Tom Philpott \"Lay Off the Almond Milk, You Ignorant Hipsters\" by Tom Philpott \"Almond Milk is Taking a Toll on the Environment\" 2019-04-02","title":"Food CO2 Water"},{"location":"Food-CO2-Water.html#co2-and-water-usage-of-food-consumed","text":"Diet Per Capita Yearly CO2 Emission $(\\frac{kg}{(cap)(yr)})$ Per Capita Yearly Water Consumption $(\\frac{kg}{(cap)(yr)})$ American Omnivore 1422.96 756.97 American Vegetarian 515.53 275.21 American Vegan (2 cup almond milk replacement) 449.97 491.80 American Vegan (2% almond milk replacement) 449.97 231.60 American Omnivore - Meat and milk consumption taken from the National Chicken Council and the USDA American Vegetarian - Meat, pork and poultry replaced with soybean (by weight, assumed 158.49kg of soybean) for CO2 emission. Meat, pork and poultry replaced with rice (by weight, assumed 158.49kg of rice) for water consumption. Cow milk still used (67.59kg) American Vegan (2 cup almond milk replacement0) - Meat, pork and poultry replaced with soybean (by weight, assumed 158.49kg of soybean) for CO2 emission. Meat, pork and poultry replaced with rice (by weight, assumed 158.49kg of rice) for water consumption. Almond milk used with 225g of almonds per litre of almond milk (assuming 67.59 L of almond milk consumed per capita per year) American Vegan (2% almond milk replacement) - Meat, pork and poultry replaced with soybean (by weight, assumed 158.49kg of soybean) for CO2 emission. Meat, pork and poultry replaced with rice (by weight, assumed 158.49kg of rice) for water consumption. Almond milk used with 20g of almonds per litre of almond milk (assuming 67.59 L of almond milk consumed per capita per year)","title":"CO2 and Water Usage of Food Consumed"},{"location":"Food-CO2-Water.html#worked-calculations","text":"Type Per Capita Yearly Consumption $(kg)$ Per Capita Yearly CO2 Emission $(\\frac{kg}{(cap)(yr)})$ Per Capita Yearly Water Consumption $(\\frac{kg}{(cap)(yr)})$ Meat (beef) 25.85 742.67 324.68 Meat (pork) 23.04 134.78 102.76 Meat (poultry) 109.6 451.55 261.94 Milk (cow) 67.59 93.95 67.59 w = 25.85 + 23.04 + 109.6 = 158.49 milk = 67.59 omni_co2 = (25.85*28.73) + (23.04*5.85) + (109.6*4.12) + (milk*1.39) omni_h2o = (25.85*12.56) + (23.04*4.46) + (109.6*2.39) + (milk*1) veg_co2 = w*2.66 + milk*1.39 veg_h2o = w*1.31 + milk*1 vegan_co2_0 = w*2.66 + milk*0.42 vegan_h2o_0 = w*1.31 + milk*(17.74*.237) vegan_co2_1 = w*2.66 + milk*0.42 vegan_h2o_1 = w*1.31 + milk*(17.74*.02) The following additional notes should be considered: I've found conflicting reports for the number of almonds in almond milk. If it's assumed that there are 225g of almonds per gallon of almond milk, this is estimate 0 above (.237kg almonds per litre). If it's assumed the almond milk is 2% (by weight), then this is estimate 1 above (.02kg almonds per litre). The American Omnivore diet estimates does not include rice, wheat, maize, vegetables or other non-animal products so can be considered a 'lower bound' on per capita CO2 and water consumption. The non American Omnivore entries use soybeans as a proxy for the meat (by weight) for CO2 emission and rice as a proxy for meat (by weight) for water consumption to give a conservative estimate of the CO2 emissions and water usage.","title":"Worked calculations"},{"location":"Food-CO2-Water.html#references","text":"","title":"References"},{"location":"Food-CO2-Water.html#per-capita-consumption-of-poultry-and-livestock-1965-to-estimated-2019-in-pounds","text":"Year Beef (lbs) Pork (lbs) Total Poultry (lbs) 208 57.0 50.8 109.6 This implies 25.85kg of beef, 23.04kg of pork and 49.71kg of poultry for the average American.","title":"\"Per Capita Consumption of Poultry and Livestock, 1965 to Estimated 2019, in Pounds\""},{"location":"Food-CO2-Water.html#dairy-data","text":"The per capita consumption of dairy for Americans : Year Milk Consumption (lbs) 2017 149 149 pounds per person per year is about 67.59kg per person per year.","title":"\"Dairy Data\""},{"location":"Food-CO2-Water.html#food-consumption-patterns-and-their-effect-on-water-requirement-in-china-by-j-liu-and-h-h-g-savenije","text":"Food Water footprint $(\\frac{m^3}{kg})$ Per capita annual food consumption (2003) $(\\frac{kg}{(cap)(yr)})$ Rice 1.31 79 Wheat 0.98 61 Maize 0.84 15 Soybeans 3.20 7 Vegetables 0.19 270 Fruits 0.50 50 Beef 12.56 5 Pork 4.46 35 Poultry 2.39 11 Milk 1.00 17","title":"\"Food consumption patterns and their effect on water requirement in China\" by  J. Liu and H. H. G. Savenije"},{"location":"Food-CO2-Water.html#systematic-review-of-greenhouse-gas-emissions-for-different-fresh-food-categories-by-stephen-clune-enda-crossin-karli-verghese","text":"Food Mean $\\frac{(kg) (CO2\\ eq)}{kg}$ (2015?) Rice 2.66 Wheat 0.51 Maize 0.63 Soybeans 0.58 Vegetables 0.47 Fruits 0.50 Beef 28.73 Pork 5.85 Chicken 4.12 Milk 1.39 Almond/coconut milk 0.42","title":"\"Systematic review of greenhouse gas emissions for different fresh food categories\" by Stephen Clune, Enda Crossin, Karli Verghese"},{"location":"Food-CO2-Water.html#the-green-blue-and-grey-water-footprint-of-crops-and-derived-crop-products-by-m-m-mekonnen-and-a-y-hoekstra","text":"Food Global average water footprint $(\\frac{m^3}{ton})$ (1996-2005) Global average water footprint ($\\frac{m^3}{kg}$ (1996-2005)) Almonds 16095 17.74","title":"\"The green, blue and grey water footprint of crops and derived crop products\" by M. M. Mekonnen and A. Y. Hoekstra"},{"location":"Food-CO2-Water.html#how-many-ounces-of-almonds-go-into-making-each-half-gallon-of-almond-milk-example-silk-or-almond-breeze-brand","text":"As a conservative estimate of the almonds used in almond milk: Around 2 cups of almonds for a half gallon. Around 225 g per cup of ground almonds . 2 cups almonds = 450 g almonds. 1 gallon = 3.79 kg (water). 1/2 gallon = 1.90 kg 450 g almonds per 1.90 kg of almond milk .237 kg of almonds per 1 kg of almond milk As a potentially more realistic estimate of the number of almonds used in almond milk: If almond milk has 2% of almonds, by weight, then for every litre of almond milk, there are approximately 20g of almonds .02 kg of almonds per 1kg of almond milk","title":"How many ounces of almonds go into making each half-gallon of almond milk (example: Silk or Almond Breeze brand)?"},{"location":"Food-CO2-Water.html#greenhouse-gas-emissions-from-a-typical-passenger-vehicle","text":"$4600 \\frac{kg}{yr}$ of CO2 emissions for the typical US passenger vehicle.","title":"Greenhouse Gas Emissions from a Typical Passenger Vehicle"},{"location":"Food-CO2-Water.html#almond-milk-quite-good-for-you-very-bad-for-the-planet-by-emine-saner","text":"","title":"\"Almond milk: quite good for you \u2013 very bad for the planet\" by Emine Saner"},{"location":"Food-CO2-Water.html#your-almond-habit-is-sucking-california-dry-by-tom-philpott","text":"","title":"\"Your Almond Habit Is Sucking California Dry\" by Tom Philpott"},{"location":"Food-CO2-Water.html#lay-off-the-almond-milk-you-ignorant-hipsters-by-tom-philpott","text":"","title":"\"Lay Off the Almond Milk, You Ignorant Hipsters\" by Tom Philpott"},{"location":"Food-CO2-Water.html#almond-milk-is-taking-a-toll-on-the-environment","text":"","title":"\"Almond Milk is Taking a Toll on the Environment\""},{"location":"Food-CO2-Water.html#2019-04-02","text":"","title":"2019-04-02"},{"location":"Future-Predictions.html","text":"Future Predictions These are a set of predictions about what's going to happen in the future. Each prediction will have conditions that need to be met in order for them to be considered true as well as conditions that indicate if the prediction was wrong. Each prediction is meant to be falsifiable to avoid making broad claims that could be considered having come to pass because of ambiguous assertions. Artificial Intelligence Human level artificial intelligence will be a reality by the year 2030 Conditions for acceptance Text conversation that is indistinguishable from human text conversation (passing the Turing test) Conditions for failure Only independent aspects that have been solved without putting them together (image recognition, natural language process, etc. without a complete system that integrates all of them together) Easily fooled chatbots The fundamental assumptions for this prediction are: Human cognition is the result of simple algorithms working in parallel on massive data sets The human brain capacity is roughly 2.3 Pb of information Hard disk space is a good proxy for the feasibility of producing human level intelligence AI will be achieved when commodity hardware is available, in the form of roughly a $1000 option, that has the compute and storage of the human brain (3.2Pb) Using jcmit.net/diskprice as a source, depending on how you count, hard drive prices fall at a rate of 1/2 every 2-3 years. As of this writing, the prices is about $\\$$150/8Tb (.00000000001705302565 $\\$$US/byte). $$ \\begin{equation} \\begin{split} \\frac{\\$ 1000}{2.3 \\text{Pb} \\cdot 2^{10} } & = 2^{-y \\alpha}\\frac{\\$150}{8 \\text{Tb}} \\\\ \\to y &= \\frac{ - \\lg{\\left( \\frac{8000}{2.3 \\cdot 150 \\cdot 2^{10} }\\right)} }{\\alpha} \\\\ \\end{split} \\end{equation} $$ $y$ is the number of years and $\\alpha$ is the doubling factor. If $\\alpha \\in (\\frac{1}{2}, \\frac{1}{3})$, this gives a timeline of roughly 10 to 16 years before we'll see a hard drive that has the storage capacity of the human brain for about $\\$$1000 USD. The 2030 prediction is a bit aggressive and considering Moore's law might be slowing down, the timeline might be closer to 2040. Regardless, I am staying with my 2030 prediction. Bet: Strong AI by 2027 (1-2 year leeway) Condition: strong AI - machines capable of carrying on normal human conversation on any topic -- not so much about physical robotics. It should be able to convince you its' intelligent. It's not meant to 'trick' you. Amount: $100 Crypto Currency A crypto currency, most likely Bitcoin, will be a major global currency, storing at least %20 of the global wealth by 2030 Conditions for acceptance Used in day-to-day life for purchases and money transfers By some reasonable estimate, 20% of the worlds wealth is stored in a crypto currency Conditions for failure Not used for day-to-day purchases Only a select few have crypto currency wallets Crypto currency is treated a stock or bond rather than a currency I believe Bitcoin to be the most likely leader as there's so much work and buy in currently, but this could change. A good rule of thumb of crypto currency not taking over is that if the USD price per coin is not high or the price per coin is very high but is not in widespread usage, signaling that it's being used by the wealthy as a kind of modern stock market. The prediction of 2030 is done by using historic USD-BTC price ( src ) and roughly fitting an exponential curve (by \"eye\"). Bitcoin is still in it's infancy so it's highly volatile, so this prediction might be significantly off. Part of this prediction is the fact that crypto currency solves a lot of problems and opens up new avenues for global trade: Online purchases have the potential to be much easier Money transfers have the potential to be much more seamless and cheap Currency exchanges (for purchases, etc.) are normalized to a de-factor standard (e.g. Bitcoin) Non provincial ownership allowing for de-centralized operation and direction As a side note, total global wealth as of 2019 is estimated to be $\\$$360T ($ \\$360 \\cdot 10^{12} $) ( src ). If Bitcoin is capped at 21M coins and establishes itself by storing %20 of the $\\$$360T, this gives $ \\frac{ \\$360T \\cdot 0.2 }{ 21M BTC } \\approx \\$3.4M/BTC $. UPDATE : If we estimate Bitcoin's growth at about 27% per year, this gives us about a 5-7 year time frame for when 1BTC will reach $1M. As of this writing, this gives us an estimate that BTC will start to become ubiquitous around 2026 to 2028. Bet: 1 BTC >= $1,000,000 USD by 2026 Condition: One (1) Bitcoin will be worth $1M USD by the year 2026 Amount: .002 BTC Energy Production Solar, wind and battery technology will account for the majority of the worlds energy production by 2040 Conditions for acceptance Over 50% of the world's energy usage will come from solar, wind and stored energy in the form of battery technology Conditions for failure Fossil fuels (coal), nuclear or other energy sources still being the primary energy provider Swanson's law ( w ) has photo voltaic cell prices falling at about 10% per year. This is roughly a price halving every 6-7 years. Current prices are about $\\$$0.75 / W for low volume consumer panels and $\\$$0.20 / W for higher volumes. I guess there's debate on whether batteries follow a Moore's law like price curve ( src src ) but prices are falling at about %20 per year ( src ) perhaps due to Wright's law ( w ). Regardless, as of this writing, lead acid batteries are at $\\$$0.15/Wh and lithium batteries look to be somewhere in the range of $\\$$0.15/Wh to $\\$$0.05/Wh. The average US household consumes 30 kWh per day. Assuming a 12 hour sunlight window, that's $30 kWh / 12h = 2500 W$ needed. The average American pays $\\$$0.1854/kWh leading to about $\\$$1400/year in electricity costs ( src ). If we assume we want at least a 2.5KW solar panel array and storage for 30kWh, that's $\\$$2000 to $\\$$6500, which is 2-5 years before the installation pays for itself. In five years time that changes to about $\\$$800 to $\\$$2500, which is about 0.5 to 2 years before the installation pays for itself. Solar and battery technology might have a lot of hidden costs and other factors that might prohibit them from falling in price so drastically over the coming years but when an installation gets within the cost of a single months electricity payment, it's hard to see how this couldn't become widely adopted. 2040 is a conservative estimate and is made in consideration that I'm no doubt missing a large number of other factors that come into play from creating, deploying and installing energy stations, on the consumer or business side. UPDATE : There is a current growth rate of 29% capacity, mostly supplied by China now ( w , iea ). There is %2.1 energy usage being satisfied by solar currently, leading to about a 15 year adoption schedule. Assuming the growth rate holds relatively steady, this amounts to adoption by 2036 to 2040. Bet: Most (50%+) of the worlds energy production will be solar by 2036 Condition: More than half of the worlds energy production will come from solar (only) by the year 2036 Amount: $100 2021-04-27","title":"Future Predictions"},{"location":"Future-Predictions.html#future-predictions","text":"These are a set of predictions about what's going to happen in the future. Each prediction will have conditions that need to be met in order for them to be considered true as well as conditions that indicate if the prediction was wrong. Each prediction is meant to be falsifiable to avoid making broad claims that could be considered having come to pass because of ambiguous assertions.","title":"Future Predictions"},{"location":"Future-Predictions.html#artificial-intelligence","text":"Human level artificial intelligence will be a reality by the year 2030 Conditions for acceptance Text conversation that is indistinguishable from human text conversation (passing the Turing test) Conditions for failure Only independent aspects that have been solved without putting them together (image recognition, natural language process, etc. without a complete system that integrates all of them together) Easily fooled chatbots The fundamental assumptions for this prediction are: Human cognition is the result of simple algorithms working in parallel on massive data sets The human brain capacity is roughly 2.3 Pb of information Hard disk space is a good proxy for the feasibility of producing human level intelligence AI will be achieved when commodity hardware is available, in the form of roughly a $1000 option, that has the compute and storage of the human brain (3.2Pb) Using jcmit.net/diskprice as a source, depending on how you count, hard drive prices fall at a rate of 1/2 every 2-3 years. As of this writing, the prices is about $\\$$150/8Tb (.00000000001705302565 $\\$$US/byte). $$ \\begin{equation} \\begin{split} \\frac{\\$ 1000}{2.3 \\text{Pb} \\cdot 2^{10} } & = 2^{-y \\alpha}\\frac{\\$150}{8 \\text{Tb}} \\\\ \\to y &= \\frac{ - \\lg{\\left( \\frac{8000}{2.3 \\cdot 150 \\cdot 2^{10} }\\right)} }{\\alpha} \\\\ \\end{split} \\end{equation} $$ $y$ is the number of years and $\\alpha$ is the doubling factor. If $\\alpha \\in (\\frac{1}{2}, \\frac{1}{3})$, this gives a timeline of roughly 10 to 16 years before we'll see a hard drive that has the storage capacity of the human brain for about $\\$$1000 USD. The 2030 prediction is a bit aggressive and considering Moore's law might be slowing down, the timeline might be closer to 2040. Regardless, I am staying with my 2030 prediction. Bet: Strong AI by 2027 (1-2 year leeway) Condition: strong AI - machines capable of carrying on normal human conversation on any topic -- not so much about physical robotics. It should be able to convince you its' intelligent. It's not meant to 'trick' you. Amount: $100","title":"Artificial Intelligence"},{"location":"Future-Predictions.html#crypto-currency","text":"A crypto currency, most likely Bitcoin, will be a major global currency, storing at least %20 of the global wealth by 2030 Conditions for acceptance Used in day-to-day life for purchases and money transfers By some reasonable estimate, 20% of the worlds wealth is stored in a crypto currency Conditions for failure Not used for day-to-day purchases Only a select few have crypto currency wallets Crypto currency is treated a stock or bond rather than a currency I believe Bitcoin to be the most likely leader as there's so much work and buy in currently, but this could change. A good rule of thumb of crypto currency not taking over is that if the USD price per coin is not high or the price per coin is very high but is not in widespread usage, signaling that it's being used by the wealthy as a kind of modern stock market. The prediction of 2030 is done by using historic USD-BTC price ( src ) and roughly fitting an exponential curve (by \"eye\"). Bitcoin is still in it's infancy so it's highly volatile, so this prediction might be significantly off. Part of this prediction is the fact that crypto currency solves a lot of problems and opens up new avenues for global trade: Online purchases have the potential to be much easier Money transfers have the potential to be much more seamless and cheap Currency exchanges (for purchases, etc.) are normalized to a de-factor standard (e.g. Bitcoin) Non provincial ownership allowing for de-centralized operation and direction As a side note, total global wealth as of 2019 is estimated to be $\\$$360T ($ \\$360 \\cdot 10^{12} $) ( src ). If Bitcoin is capped at 21M coins and establishes itself by storing %20 of the $\\$$360T, this gives $ \\frac{ \\$360T \\cdot 0.2 }{ 21M BTC } \\approx \\$3.4M/BTC $. UPDATE : If we estimate Bitcoin's growth at about 27% per year, this gives us about a 5-7 year time frame for when 1BTC will reach $1M. As of this writing, this gives us an estimate that BTC will start to become ubiquitous around 2026 to 2028. Bet: 1 BTC >= $1,000,000 USD by 2026 Condition: One (1) Bitcoin will be worth $1M USD by the year 2026 Amount: .002 BTC","title":"Crypto Currency"},{"location":"Future-Predictions.html#energy-production","text":"Solar, wind and battery technology will account for the majority of the worlds energy production by 2040 Conditions for acceptance Over 50% of the world's energy usage will come from solar, wind and stored energy in the form of battery technology Conditions for failure Fossil fuels (coal), nuclear or other energy sources still being the primary energy provider Swanson's law ( w ) has photo voltaic cell prices falling at about 10% per year. This is roughly a price halving every 6-7 years. Current prices are about $\\$$0.75 / W for low volume consumer panels and $\\$$0.20 / W for higher volumes. I guess there's debate on whether batteries follow a Moore's law like price curve ( src src ) but prices are falling at about %20 per year ( src ) perhaps due to Wright's law ( w ). Regardless, as of this writing, lead acid batteries are at $\\$$0.15/Wh and lithium batteries look to be somewhere in the range of $\\$$0.15/Wh to $\\$$0.05/Wh. The average US household consumes 30 kWh per day. Assuming a 12 hour sunlight window, that's $30 kWh / 12h = 2500 W$ needed. The average American pays $\\$$0.1854/kWh leading to about $\\$$1400/year in electricity costs ( src ). If we assume we want at least a 2.5KW solar panel array and storage for 30kWh, that's $\\$$2000 to $\\$$6500, which is 2-5 years before the installation pays for itself. In five years time that changes to about $\\$$800 to $\\$$2500, which is about 0.5 to 2 years before the installation pays for itself. Solar and battery technology might have a lot of hidden costs and other factors that might prohibit them from falling in price so drastically over the coming years but when an installation gets within the cost of a single months electricity payment, it's hard to see how this couldn't become widely adopted. 2040 is a conservative estimate and is made in consideration that I'm no doubt missing a large number of other factors that come into play from creating, deploying and installing energy stations, on the consumer or business side. UPDATE : There is a current growth rate of 29% capacity, mostly supplied by China now ( w , iea ). There is %2.1 energy usage being satisfied by solar currently, leading to about a 15 year adoption schedule. Assuming the growth rate holds relatively steady, this amounts to adoption by 2036 to 2040. Bet: Most (50%+) of the worlds energy production will be solar by 2036 Condition: More than half of the worlds energy production will come from solar (only) by the year 2036 Amount: $100","title":"Energy Production"},{"location":"Future-Predictions.html#2021-04-27","text":"","title":"2021-04-27"},{"location":"GCode-Common.html","text":"Common GCode Commands GCode Description G0 Rapid motion G1 Slow motion G20 Units in inches G21 Units in mm G38.2 Probe towards workspace (-), stop on contact, alarm if probe not tripped G38.3 Probe towards workspace (-), stop on contact G38.4 Probe away from workspace (+), stop on loss of contact, alarm if probe not tripped G38.5 Probe away from workspace (+), stop on loss of contact M3[S<S>] Start spindle M5 Stop spindle 2020-09-09","title":"GCode Common"},{"location":"GCode-Common.html#common-gcode-commands","text":"GCode Description G0 Rapid motion G1 Slow motion G20 Units in inches G21 Units in mm G38.2 Probe towards workspace (-), stop on contact, alarm if probe not tripped G38.3 Probe towards workspace (-), stop on contact G38.4 Probe away from workspace (+), stop on loss of contact, alarm if probe not tripped G38.5 Probe away from workspace (+), stop on loss of contact M3[S<S>] Start spindle M5 Stop spindle","title":"Common GCode Commands"},{"location":"GCode-Common.html#2020-09-09","text":"","title":"2020-09-09"},{"location":"GCode-Conversion.html","text":"GCode Conversion Tools At one point, I had good success with an Inkscape plugin called Gcodetools but it seems to have succumbed to bit-rot and doesn't work on my current Ubuntu installation (16.04). I'm settling on a rough toolchain that takes some base format (PostScript/PDF/SVG/etc.), converts to \"GNUPlot format\" then converts to GCocde. Basic Workflow Orig -> SVG Create object in whatever tool and export to SVG SVG -> PS Use rsvg-convert to convert from SVG to PostScript PS -> GP Use pstoedit to convert to \"gnuplot\" polygon format GP -> GCode Order the polygons properly, removing duplicate boundaries and convert to GCode using clipcli , convert from GNUPlot format to GCode using gp2ngc and then rescale using other cli GCode tools. Installation Some tools of relevance are: rsvg-convert pstoedit clipcli abes_cnc_utilities grecode Under Ubuntu, some of the tools can be installed via: sudo apt-get install pstoedit librsvg2-bin Conversion Though this is pretty hodge-podge, there are a few things to consider: pstoedit loses units when converting to RS274 GCode. I believe this only considers PostScript with \"pixel\" units, regardless of original units, then converts a pixel to 1/72 inches. A post scale has to be done if using pstoedit to rescale to the appropriate units Even if pstoedit is used, this creates a problem when trying to cut out shapes in the correct order. clipcli has an option to print out polygons in 'tree' order which should print the inner polygons first. I'll be using some of the tools that I've created below to rescale/etc. but in theory anything could be used, including (maybe the more standard and robust?) grecode as linked above. The following is an example script to convert an input SVG file into GCode: inpsvg=\"$1\" sf=`echo '72/25.4' | bc -l` premul=`echo 1000000 | bc -l` invmul=`echo \"1/$premul\" | bc -l` frapid=\"\" fslow=\"F800\" S=\"1.0\" if [[ \"$inpsvg\" == \"\" ]] ; then echo \"provide input svg\" exit 1 fi rawtype=`file $inpsvg` checktype=`file -b $inpsvg | cut -f1 -d' '` if [[ \"$checktype\" != \"SVG\" ]] ; then echo -e \"file $inpsvg is of type:\\n\\n$rawtype\\n\\nNnot an SVG file? Exiting.\\n\" exit 1 fi bn=`basename $inpsvg .svg` # causes duplicate paths otherwise # sed -i 's/fill=\"[^\"]*\"/fill=\"none\"/g' $inpsvg echo \"creating $bn.ps\" rsvg-convert -f ps -o $bn.ps $inpsvg pstoedit -f gnuplot $bn.ps $bn.gp clipcli -s $bn.gp -F -x $premul -T > ${bn}-ord.gp sfx_slow=\"$frapid S$S\" sfx_rapid=\"$fslow S0\" echo gp2ngc -i ${bn}-ord.gp -s \"$invmul\" --sfx-rapid \"$sfx_rapid\" --sfx-slow \"$sfx_slow\" -o ${bn}.ngc gp2ngc -i ${bn}-ord.gp --sfx-rapid \"$sfx_rapid\" --sfx-slow \"$sfx_slow\" | ngc_scale -s \"$invmul\" > ${bn}.ngc Misc. In theory, pstoedit can be used to create GCode but pstoedit converts to the RS274 standard. Among other things, the RS274 includes variables so a substitution step needs to be involved in order to \"normalize\" to something that other GCode interpreters can understand (for example, the smoothieboard or grbl). There's still the problem of polygon ordering but assuming that's not an issue, the following is a \"hacky\" script does the substitution (no nested expressions, no non-trivial functions, run at your own risk): #!/usr/bin/python # # regexp substitution of variables. # Uses Python's \"eval\" to evaluate interior # after variable substitution. # # AGPLv3 license # import sys import re var_map = {} # variable decleration # var_decl_pat = re.compile( r'\\s*#(\\d+)\\s*=\\s*([^\\s]+)\\s*(\\([^\\)]*\\))?\\s*$' ) # not [], [], not [] # expr_pat = re.compile( r'([^\\[]*)\\[([^\\]]*)\\]([^\\[]*)' ) # not #*, #\\d+, not #* # var_sub_pat = re.compile( r'([^#]*)(#\\d+)([^#]*)' ) # consider comments separately to avoid matching '#' and # other special characters # comment_pat = re.compile( r'\\([^\\)]*\\)' ) line_no = 0 for line in sys.stdin: line_no += 1 line = line.rstrip() comments = \"\" for (comment) in re.findall(comment_pat, line): comments = comments + comment line = re.sub(comment_pat, '', line) m = re.match(var_decl_pat, line) if m: var_map[ \"#\" + str(m.group(1)) ] = str(m.group(2)) continue varsub_line = \"\" for (pfx, var_subs, sfx) in re.findall(var_sub_pat, line): if var_subs in var_map: pass else: print \" ERROR on line\", line_no, \", no variable mapping for\", var_subs sys.exit(1) continue varsub_line += pfx varsub_line += var_map[var_subs] varsub_line += sfx if varsub_line == \"\": varsub_line = line xpr_match = re.search(expr_pat, varsub_line) if not xpr_match: print varsub_line + comments continue cur_line = \"\" for (pfx, xpr, sfx) in re.findall(expr_pat, varsub_line): xpr_val = eval(xpr) cur_line += pfx + str(xpr_val) + sfx print cur_line + comments 2016-09-19","title":"GCode Conversion"},{"location":"GCode-Conversion.html#gcode-conversion-tools","text":"At one point, I had good success with an Inkscape plugin called Gcodetools but it seems to have succumbed to bit-rot and doesn't work on my current Ubuntu installation (16.04). I'm settling on a rough toolchain that takes some base format (PostScript/PDF/SVG/etc.), converts to \"GNUPlot format\" then converts to GCocde.","title":"GCode Conversion Tools"},{"location":"GCode-Conversion.html#basic-workflow","text":"Orig -> SVG Create object in whatever tool and export to SVG SVG -> PS Use rsvg-convert to convert from SVG to PostScript PS -> GP Use pstoedit to convert to \"gnuplot\" polygon format GP -> GCode Order the polygons properly, removing duplicate boundaries and convert to GCode using clipcli , convert from GNUPlot format to GCode using gp2ngc and then rescale using other cli GCode tools.","title":"Basic Workflow"},{"location":"GCode-Conversion.html#installation","text":"Some tools of relevance are: rsvg-convert pstoedit clipcli abes_cnc_utilities grecode Under Ubuntu, some of the tools can be installed via: sudo apt-get install pstoedit librsvg2-bin","title":"Installation"},{"location":"GCode-Conversion.html#conversion","text":"Though this is pretty hodge-podge, there are a few things to consider: pstoedit loses units when converting to RS274 GCode. I believe this only considers PostScript with \"pixel\" units, regardless of original units, then converts a pixel to 1/72 inches. A post scale has to be done if using pstoedit to rescale to the appropriate units Even if pstoedit is used, this creates a problem when trying to cut out shapes in the correct order. clipcli has an option to print out polygons in 'tree' order which should print the inner polygons first. I'll be using some of the tools that I've created below to rescale/etc. but in theory anything could be used, including (maybe the more standard and robust?) grecode as linked above. The following is an example script to convert an input SVG file into GCode: inpsvg=\"$1\" sf=`echo '72/25.4' | bc -l` premul=`echo 1000000 | bc -l` invmul=`echo \"1/$premul\" | bc -l` frapid=\"\" fslow=\"F800\" S=\"1.0\" if [[ \"$inpsvg\" == \"\" ]] ; then echo \"provide input svg\" exit 1 fi rawtype=`file $inpsvg` checktype=`file -b $inpsvg | cut -f1 -d' '` if [[ \"$checktype\" != \"SVG\" ]] ; then echo -e \"file $inpsvg is of type:\\n\\n$rawtype\\n\\nNnot an SVG file? Exiting.\\n\" exit 1 fi bn=`basename $inpsvg .svg` # causes duplicate paths otherwise # sed -i 's/fill=\"[^\"]*\"/fill=\"none\"/g' $inpsvg echo \"creating $bn.ps\" rsvg-convert -f ps -o $bn.ps $inpsvg pstoedit -f gnuplot $bn.ps $bn.gp clipcli -s $bn.gp -F -x $premul -T > ${bn}-ord.gp sfx_slow=\"$frapid S$S\" sfx_rapid=\"$fslow S0\" echo gp2ngc -i ${bn}-ord.gp -s \"$invmul\" --sfx-rapid \"$sfx_rapid\" --sfx-slow \"$sfx_slow\" -o ${bn}.ngc gp2ngc -i ${bn}-ord.gp --sfx-rapid \"$sfx_rapid\" --sfx-slow \"$sfx_slow\" | ngc_scale -s \"$invmul\" > ${bn}.ngc","title":"Conversion"},{"location":"GCode-Conversion.html#misc","text":"In theory, pstoedit can be used to create GCode but pstoedit converts to the RS274 standard. Among other things, the RS274 includes variables so a substitution step needs to be involved in order to \"normalize\" to something that other GCode interpreters can understand (for example, the smoothieboard or grbl). There's still the problem of polygon ordering but assuming that's not an issue, the following is a \"hacky\" script does the substitution (no nested expressions, no non-trivial functions, run at your own risk): #!/usr/bin/python # # regexp substitution of variables. # Uses Python's \"eval\" to evaluate interior # after variable substitution. # # AGPLv3 license # import sys import re var_map = {} # variable decleration # var_decl_pat = re.compile( r'\\s*#(\\d+)\\s*=\\s*([^\\s]+)\\s*(\\([^\\)]*\\))?\\s*$' ) # not [], [], not [] # expr_pat = re.compile( r'([^\\[]*)\\[([^\\]]*)\\]([^\\[]*)' ) # not #*, #\\d+, not #* # var_sub_pat = re.compile( r'([^#]*)(#\\d+)([^#]*)' ) # consider comments separately to avoid matching '#' and # other special characters # comment_pat = re.compile( r'\\([^\\)]*\\)' ) line_no = 0 for line in sys.stdin: line_no += 1 line = line.rstrip() comments = \"\" for (comment) in re.findall(comment_pat, line): comments = comments + comment line = re.sub(comment_pat, '', line) m = re.match(var_decl_pat, line) if m: var_map[ \"#\" + str(m.group(1)) ] = str(m.group(2)) continue varsub_line = \"\" for (pfx, var_subs, sfx) in re.findall(var_sub_pat, line): if var_subs in var_map: pass else: print \" ERROR on line\", line_no, \", no variable mapping for\", var_subs sys.exit(1) continue varsub_line += pfx varsub_line += var_map[var_subs] varsub_line += sfx if varsub_line == \"\": varsub_line = line xpr_match = re.search(expr_pat, varsub_line) if not xpr_match: print varsub_line + comments continue cur_line = \"\" for (pfx, xpr, sfx) in re.findall(expr_pat, varsub_line): xpr_val = eval(xpr) cur_line += pfx + str(xpr_val) + sfx print cur_line + comments","title":"Misc."},{"location":"GCode-Conversion.html#2016-09-19","text":"","title":"2016-09-19"},{"location":"GPG-Notes.html","text":"Install Dependencies sudo apt-get install gpa gnupg2 Generate Key Pair gpg --gen-key Suggestions Use RSA and RSA (the default) Use 4096 bits for the keysize Choose no expiration for key ( 0 option, default) Note: Real name , Email address , Comment and passphrase are needed. List Keys gpg --list-keys Export ASCII Public Key gpg --armor --export user@example.com Export Binary Public Key gpg --output user.gpg --export user@example.com Export Private Key (careful) gpg --export-secret-key -a user@example.net > private.key Add Binary Public Key gpg --import friend.gpg Add Private Key gpg --import private.key Encrypt Message gpg -e -u \"Sender ID\" -r \"Recipient ID\" plaintext-message 'human readable' ASCII armor encrypted message: gpg -a -e -u \"Sender ID\" -r \"Recipient ID\" plaintext-message Decrypt Message gpg -d encrypted-message Fin gpg --armor --export abetusk@mechaelephant.com -----BEGIN PGP PUBLIC KEY BLOCK----- Version: GnuPG v1 mQINBFic0RgBEACV328qdNksrQvAY/ilYxsgALaw96jDMBGQldvH9M/oWWs14ll/ p04QhSXuwuAQnhsvFvMrfvMRarsIkyDc3PUpBNh9PR39NvN0c1Lsq9hXq2qW1j3W zfqJp4rg9eYYLVATLm2aLb+cYwwavthwWfVI3egt3hCq+cP7nTHrH+yeT+lavSfX UF7q8a0Ku2Y5fZruc1sxggPNMrvPr2bss1QlpJ7d9HAjvtTqo9TXv9jutoDe1iYr XUcGFgAYgm+ZeCMTbBYn8KrpbS+OWjuZMs7PdkO/IfOdDvL0pYXCHgbj+4NL4i8v Q8KPJxShfBuMvOE1W6/VrkRKp3FzLiidi8lvpsB4pXXmUS+6Vl37MM74mFFltHbz FeojGwHMKGJY6vFrFfsIKXbggvVDyzs5y8WYd9MW473Oq4EHcb9M5HCrj+shZ0yP O/6uB/5OFhV+gk6FWtQtJAlnxkVGtIoj5zWZEAqMGOl1/i5Fk4uqSeffBFhW0l3r ndYuaXv67tJ0xwUKlOsd515gvIS26gGaqB/u1zfRT7Va7xtGlY35wflmnmnKdpQ8 8t4OFhPu7u7fB68S+8U4hAMaQsPCt7NmNKM041gN0/SlqcOkap9FShoomVg46ZRd kSLbohnS6N784La/Zr8aShWatn6DOO+MalltAaOeUxPbLACbwkdt/groUwARAQAB tCRBYmUgVHVzayA8YWJldHVza0BtZWNoYWVsZXBoYW50LmNvbT6JAjgEEwECACIF Alic0RgCGwMGCwkIBwMCBhUIAgkKCwQWAgMBAh4BAheAAAoJEDSaujvXa/13c0UP /jr1srV/EAvBYWlVn/Tyoz2vyg0KCNo+WWqEjgX8LbWa/K9Bo46F8hiVQCy3ApGI 8QEaV5FkFKYZ5+tVA8ytHdhHDdQhEPFwFwOknf+Myklkj83TIwT1oj68aBDuKF/o 61x52B9gyDk4W1lbfeNv+AT0GBCV84cRBwdoz2u22wlwJZQk7roUznGWjbae0iT/ XPESF5MAHSl5AQnScewHSOmJFIbCwCjtIVUdY0Dg+KeH4XoUNjGcgWKQDLIo1YY2 ygEk8XRlQDkhN+chEpjfWtlGwDKvg3+ARvCn5luSuQRR/q9Gs0ErY8NBdUq80SLa 2VNqrKkM5U7ATG7t+fpysrpSjPRw7ijD/eMj4JMKn481sMH/990hm6WDv1UsYPLC A+RzJu8JDws3jJn1jQlOxNJ1LXaXTpi7IaKgHAdLYNyXVVFWs+T9V03VkvITQEfz 0zanPRUjbIMBq47Fz62OTAFKXfhyZkfGugUUehdx9WDm34bjs2/U/d84A++ZeXOR ZQISO3GUiiUz7d1FN8FxLe3TfB0Q+skYqDmTDqqXLtn/WnZjrh21ad4J3AZjoPRp +OoLZOXY1LLCPMi5YLMO3Vavcp3Hwr3H625wmyJEdLBl0vDhAHd39iwdvEairhJx MtF+7j7cekQmA44FtQOsJcppN1aELWiSGDv6xl1muqvsuQINBFic0RgBEACgP21H ICKFLbTe+eiZCKbFklgctx6S9JlwYHXqSp4ft1kHfIvovH/Rax3kETC+w3YRBbws 4pVBnojz1+Osh7xj0VgT+paUrP9clQgLsP5bUZfTxtR3EDcEKx3rxKRGtD66r5xp 3Gb6lCFM5ZJ03wHD1hq5kRjumu+gncbGPIKeTrfGltgMvZusUeM9avuQHFdU7xw5 GFQaTHN5bS/EeVRcBVHrW+Hrxjh+OcogScqsk9juZt2g8mSwoQGATIgGtXPHeex7 qmThCVWUqhu22wVrU/M2+f+67RXvQBYAFHb9JOsrNQxQ1Bezr5kKn9EsoV2VBfdO lCJoLpcSfNjb2S33I9DUJ+P7ASNafiBvB6PXjiy1esWeEe6X9ma7wficy6n63WHt YYd3+j/ur1iwi11ikc5FyGEnRf2gt+hVStqYjVTyFQ5yPMpEqkWtg526rt1jLkMU xZwMXUyWIKo9ie+Y2Kw8mJ3ToAMoei52L3+V3CZk7oC6M2TlHf3uSmLvfJg+dz8p s8xlUQxngDxj+c0deWvbiN29QfBdWNHU+SFb+mXGxQHMkgMAkxGETPs09XrAorCK dMosPKUA/NmCDE1UIFOOXFSy+DBZqToUtYvj0SgH1wJhcFyT8fgABozne7gpi3Is WuCQRQx9NFZ5QVthPq0PAGrAziH8guilAoDS6wARAQABiQIfBBgBAgAJBQJYnNEY AhsMAAoJEDSaujvXa/137c4P/1nMdqi21mw/cqP3Y/yPdjnUYiajcEV5O9jV5bRl 2OEW7sFD22EKd4mH4e8jC2FPQJnKnDcMwyMhs2rUHHopMRs9y8cVK/UUH4hlSHmf YWKuK+iwfvKkAkokCKTe0Svw4+LA7lDMb5XUNQF+A5otT0M6AjKe9FPBKruuxGc+ VQF6MtRQ+xb7JXRCPy4Ad0bO+K7I9dQg7sxxpM2Ooc1Q6PdtfN1NDD7Nw+XxDnVA DWxXfs4fisFFOe3WpghEcHVw6Xsk08mNf4JU/KG5fuA6chmumwt32SQsWV7vtkv4 kG3fxNOz3aifXXFdmmF4MLYDmPE/Gm++Ae4YK1wjYDgqQckhOja9u0Ywiz4NhyVC vrJdsIsLZycwLH+qqPYbhklH2VFWoeEIVg5bJbKMbKxLV2vrIFduq7f/3eVkGfh2 yze2cSwag/lpy+SUSwnjOl3y9cOyr1K1SAQYAw2j1VQThRhtPEJCVacjTMuZ/Vsd F1aWO3d0MOhJrz6wWG+6BJQSnmsedbzsT/TGhdGZ9dZAAwBvJZNF59xO9hMQPd54 5bATkkQLs4gjpDcbEm6xK03YhcDFB8p/3CfBaWYD9wGjv23RNJZbkv+V1L8xaNWG R67Rw7s+5nTClV2WU7YHy7vefmGosCNdmi4lUETfx838oyNLOuPzc21Rwvtby6tF IPY2 =tdhS -----END PGP PUBLIC KEY BLOCK----- 2017-02-09","title":"GPG Notes"},{"location":"GPG-Notes.html#install-dependencies","text":"sudo apt-get install gpa gnupg2","title":"Install Dependencies"},{"location":"GPG-Notes.html#generate-key-pair","text":"gpg --gen-key","title":"Generate Key Pair"},{"location":"GPG-Notes.html#suggestions","text":"Use RSA and RSA (the default) Use 4096 bits for the keysize Choose no expiration for key ( 0 option, default) Note: Real name , Email address , Comment and passphrase are needed.","title":"Suggestions"},{"location":"GPG-Notes.html#list-keys","text":"gpg --list-keys","title":"List Keys"},{"location":"GPG-Notes.html#export-ascii-public-key","text":"gpg --armor --export user@example.com","title":"Export ASCII Public Key"},{"location":"GPG-Notes.html#export-binary-public-key","text":"gpg --output user.gpg --export user@example.com","title":"Export Binary Public Key"},{"location":"GPG-Notes.html#export-private-key-careful","text":"gpg --export-secret-key -a user@example.net > private.key","title":"Export Private Key (careful)"},{"location":"GPG-Notes.html#add-binary-public-key","text":"gpg --import friend.gpg","title":"Add Binary Public Key"},{"location":"GPG-Notes.html#add-private-key","text":"gpg --import private.key","title":"Add Private Key"},{"location":"GPG-Notes.html#encrypt-message","text":"gpg -e -u \"Sender ID\" -r \"Recipient ID\" plaintext-message 'human readable' ASCII armor encrypted message: gpg -a -e -u \"Sender ID\" -r \"Recipient ID\" plaintext-message","title":"Encrypt Message"},{"location":"GPG-Notes.html#decrypt-message","text":"gpg -d encrypted-message","title":"Decrypt Message"},{"location":"GPG-Notes.html#fin","text":"gpg --armor --export abetusk@mechaelephant.com -----BEGIN PGP PUBLIC KEY BLOCK----- Version: GnuPG v1 mQINBFic0RgBEACV328qdNksrQvAY/ilYxsgALaw96jDMBGQldvH9M/oWWs14ll/ p04QhSXuwuAQnhsvFvMrfvMRarsIkyDc3PUpBNh9PR39NvN0c1Lsq9hXq2qW1j3W zfqJp4rg9eYYLVATLm2aLb+cYwwavthwWfVI3egt3hCq+cP7nTHrH+yeT+lavSfX UF7q8a0Ku2Y5fZruc1sxggPNMrvPr2bss1QlpJ7d9HAjvtTqo9TXv9jutoDe1iYr XUcGFgAYgm+ZeCMTbBYn8KrpbS+OWjuZMs7PdkO/IfOdDvL0pYXCHgbj+4NL4i8v Q8KPJxShfBuMvOE1W6/VrkRKp3FzLiidi8lvpsB4pXXmUS+6Vl37MM74mFFltHbz FeojGwHMKGJY6vFrFfsIKXbggvVDyzs5y8WYd9MW473Oq4EHcb9M5HCrj+shZ0yP O/6uB/5OFhV+gk6FWtQtJAlnxkVGtIoj5zWZEAqMGOl1/i5Fk4uqSeffBFhW0l3r ndYuaXv67tJ0xwUKlOsd515gvIS26gGaqB/u1zfRT7Va7xtGlY35wflmnmnKdpQ8 8t4OFhPu7u7fB68S+8U4hAMaQsPCt7NmNKM041gN0/SlqcOkap9FShoomVg46ZRd kSLbohnS6N784La/Zr8aShWatn6DOO+MalltAaOeUxPbLACbwkdt/groUwARAQAB tCRBYmUgVHVzayA8YWJldHVza0BtZWNoYWVsZXBoYW50LmNvbT6JAjgEEwECACIF Alic0RgCGwMGCwkIBwMCBhUIAgkKCwQWAgMBAh4BAheAAAoJEDSaujvXa/13c0UP /jr1srV/EAvBYWlVn/Tyoz2vyg0KCNo+WWqEjgX8LbWa/K9Bo46F8hiVQCy3ApGI 8QEaV5FkFKYZ5+tVA8ytHdhHDdQhEPFwFwOknf+Myklkj83TIwT1oj68aBDuKF/o 61x52B9gyDk4W1lbfeNv+AT0GBCV84cRBwdoz2u22wlwJZQk7roUznGWjbae0iT/ XPESF5MAHSl5AQnScewHSOmJFIbCwCjtIVUdY0Dg+KeH4XoUNjGcgWKQDLIo1YY2 ygEk8XRlQDkhN+chEpjfWtlGwDKvg3+ARvCn5luSuQRR/q9Gs0ErY8NBdUq80SLa 2VNqrKkM5U7ATG7t+fpysrpSjPRw7ijD/eMj4JMKn481sMH/990hm6WDv1UsYPLC A+RzJu8JDws3jJn1jQlOxNJ1LXaXTpi7IaKgHAdLYNyXVVFWs+T9V03VkvITQEfz 0zanPRUjbIMBq47Fz62OTAFKXfhyZkfGugUUehdx9WDm34bjs2/U/d84A++ZeXOR ZQISO3GUiiUz7d1FN8FxLe3TfB0Q+skYqDmTDqqXLtn/WnZjrh21ad4J3AZjoPRp +OoLZOXY1LLCPMi5YLMO3Vavcp3Hwr3H625wmyJEdLBl0vDhAHd39iwdvEairhJx MtF+7j7cekQmA44FtQOsJcppN1aELWiSGDv6xl1muqvsuQINBFic0RgBEACgP21H ICKFLbTe+eiZCKbFklgctx6S9JlwYHXqSp4ft1kHfIvovH/Rax3kETC+w3YRBbws 4pVBnojz1+Osh7xj0VgT+paUrP9clQgLsP5bUZfTxtR3EDcEKx3rxKRGtD66r5xp 3Gb6lCFM5ZJ03wHD1hq5kRjumu+gncbGPIKeTrfGltgMvZusUeM9avuQHFdU7xw5 GFQaTHN5bS/EeVRcBVHrW+Hrxjh+OcogScqsk9juZt2g8mSwoQGATIgGtXPHeex7 qmThCVWUqhu22wVrU/M2+f+67RXvQBYAFHb9JOsrNQxQ1Bezr5kKn9EsoV2VBfdO lCJoLpcSfNjb2S33I9DUJ+P7ASNafiBvB6PXjiy1esWeEe6X9ma7wficy6n63WHt YYd3+j/ur1iwi11ikc5FyGEnRf2gt+hVStqYjVTyFQ5yPMpEqkWtg526rt1jLkMU xZwMXUyWIKo9ie+Y2Kw8mJ3ToAMoei52L3+V3CZk7oC6M2TlHf3uSmLvfJg+dz8p s8xlUQxngDxj+c0deWvbiN29QfBdWNHU+SFb+mXGxQHMkgMAkxGETPs09XrAorCK dMosPKUA/NmCDE1UIFOOXFSy+DBZqToUtYvj0SgH1wJhcFyT8fgABozne7gpi3Is WuCQRQx9NFZ5QVthPq0PAGrAziH8guilAoDS6wARAQABiQIfBBgBAgAJBQJYnNEY AhsMAAoJEDSaujvXa/137c4P/1nMdqi21mw/cqP3Y/yPdjnUYiajcEV5O9jV5bRl 2OEW7sFD22EKd4mH4e8jC2FPQJnKnDcMwyMhs2rUHHopMRs9y8cVK/UUH4hlSHmf YWKuK+iwfvKkAkokCKTe0Svw4+LA7lDMb5XUNQF+A5otT0M6AjKe9FPBKruuxGc+ VQF6MtRQ+xb7JXRCPy4Ad0bO+K7I9dQg7sxxpM2Ooc1Q6PdtfN1NDD7Nw+XxDnVA DWxXfs4fisFFOe3WpghEcHVw6Xsk08mNf4JU/KG5fuA6chmumwt32SQsWV7vtkv4 kG3fxNOz3aifXXFdmmF4MLYDmPE/Gm++Ae4YK1wjYDgqQckhOja9u0Ywiz4NhyVC vrJdsIsLZycwLH+qqPYbhklH2VFWoeEIVg5bJbKMbKxLV2vrIFduq7f/3eVkGfh2 yze2cSwag/lpy+SUSwnjOl3y9cOyr1K1SAQYAw2j1VQThRhtPEJCVacjTMuZ/Vsd F1aWO3d0MOhJrz6wWG+6BJQSnmsedbzsT/TGhdGZ9dZAAwBvJZNF59xO9hMQPd54 5bATkkQLs4gjpDcbEm6xK03YhcDFB8p/3CfBaWYD9wGjv23RNJZbkv+V1L8xaNWG R67Rw7s+5nTClV2WU7YHy7vefmGosCNdmi4lUETfx838oyNLOuPzc21Rwvtby6tF IPY2 =tdhS -----END PGP PUBLIC KEY BLOCK-----","title":"Fin"},{"location":"GPG-Notes.html#2017-02-09","text":"","title":"2017-02-09"},{"location":"Git-Notes.html","text":"Git User Settings Local: $ git config user.name abevoid $ git config user.email abevoid@abevoid.com $ git config user.name abetusk abetusk $ git config user.email abetusk@mechaelephant.com abetusk@mechaelephant.com $ git config user.name abetusk $ git config user.email abetusk@mechaelephant.com Global: $ git config --global user.name abevoid $ git config --global user.email abevoid@abevoid.com $ git config --global user.name abetusk abetusk $ git config --global user.email abetusk@mechaelephant.com abetusk@mechaelephant.com $ git config --global user.name abetusk $ git config --global user.email abetusk@mechaelephant.com Git Log git log --decorate=full --graph Merge Branches and Keep DAG History git checkout -b alt-branch ... git commit git checkout fin-branch git merge --no-ff alt-branch Adding Upstream Branch git remote -v git remote add upstream {remote.git} Merging/Syncing Forked Repository git fetch upstream git checkout main git merge upstream/main Creating and Deleting Branches git checkout -b x-branch ... git commit git push -u origin x-branch git checkout release git merge --no-ff x-branch git commit git push ... git branch -d x-branch git push origin :x-branch Destroy Local Changes To Files git fetch ; git reset --hard origin/release Move Local Changes to New Branch git branch newbranch git reset --hard origin/master git checkout newbranch Create a branch but don't switch to it. Remove local commits back to origin/master . Now switch to the new branch and continue work. Comparing Changes Local changes since last commit: git diff Two commits ago: git diff HEAD^^ or: git diff HEAD@{2} Restore File Single file: git checkout -- fn Everything in a repo: #!/bin/bash for f in `git ls-files -d` do echo restoring $f git checkout -- $f done Caching Git Password git config --global credentail.helper 'cache --timeout=3600' Clear Git Credential Cache (Clear Git Cached Password) git credential-cache exit Tracking Remote Branch git remote add origin https://github.com/user/repo.git git remote -v Checking Out Submodules git clone --recursive https://github.com/user/repo git submodule update --init --recursive Applying Inverse Create (add) a new commit that applies the inverse operation of the given <SHA> . git revert <SHA> References How to undo (almost) anything with Git 2017-02-10","title":"Git Notes"},{"location":"Git-Notes.html#git-user-settings","text":"Local: $ git config user.name abevoid $ git config user.email abevoid@abevoid.com $ git config user.name abetusk abetusk $ git config user.email abetusk@mechaelephant.com abetusk@mechaelephant.com $ git config user.name abetusk $ git config user.email abetusk@mechaelephant.com Global: $ git config --global user.name abevoid $ git config --global user.email abevoid@abevoid.com $ git config --global user.name abetusk abetusk $ git config --global user.email abetusk@mechaelephant.com abetusk@mechaelephant.com $ git config --global user.name abetusk $ git config --global user.email abetusk@mechaelephant.com","title":"Git User Settings"},{"location":"Git-Notes.html#git-log","text":"git log --decorate=full --graph","title":"Git Log"},{"location":"Git-Notes.html#merge-branches-and-keep-dag-history","text":"git checkout -b alt-branch ... git commit git checkout fin-branch git merge --no-ff alt-branch","title":"Merge Branches and Keep DAG History"},{"location":"Git-Notes.html#adding-upstream-branch","text":"git remote -v git remote add upstream {remote.git}","title":"Adding Upstream Branch"},{"location":"Git-Notes.html#mergingsyncing-forked-repository","text":"git fetch upstream git checkout main git merge upstream/main","title":"Merging/Syncing Forked Repository"},{"location":"Git-Notes.html#creating-and-deleting-branches","text":"git checkout -b x-branch ... git commit git push -u origin x-branch git checkout release git merge --no-ff x-branch git commit git push ... git branch -d x-branch git push origin :x-branch","title":"Creating and Deleting Branches"},{"location":"Git-Notes.html#destroy-local-changes-to-files","text":"git fetch ; git reset --hard origin/release","title":"Destroy Local Changes To Files"},{"location":"Git-Notes.html#move-local-changes-to-new-branch","text":"git branch newbranch git reset --hard origin/master git checkout newbranch Create a branch but don't switch to it. Remove local commits back to origin/master . Now switch to the new branch and continue work.","title":"Move Local Changes to New Branch"},{"location":"Git-Notes.html#comparing-changes","text":"Local changes since last commit: git diff Two commits ago: git diff HEAD^^ or: git diff HEAD@{2}","title":"Comparing Changes"},{"location":"Git-Notes.html#restore-file","text":"Single file: git checkout -- fn Everything in a repo: #!/bin/bash for f in `git ls-files -d` do echo restoring $f git checkout -- $f done","title":"Restore File"},{"location":"Git-Notes.html#caching-git-password","text":"git config --global credentail.helper 'cache --timeout=3600'","title":"Caching Git Password"},{"location":"Git-Notes.html#clear-git-credential-cache-clear-git-cached-password","text":"git credential-cache exit","title":"Clear Git Credential Cache (Clear Git Cached Password)"},{"location":"Git-Notes.html#tracking-remote-branch","text":"git remote add origin https://github.com/user/repo.git git remote -v","title":"Tracking Remote Branch"},{"location":"Git-Notes.html#checking-out-submodules","text":"git clone --recursive https://github.com/user/repo git submodule update --init --recursive","title":"Checking Out Submodules"},{"location":"Git-Notes.html#applying-inverse","text":"Create (add) a new commit that applies the inverse operation of the given <SHA> . git revert <SHA>","title":"Applying Inverse"},{"location":"Git-Notes.html#references","text":"How to undo (almost) anything with Git","title":"References"},{"location":"Git-Notes.html#2017-02-10","text":"","title":"2017-02-10"},{"location":"Git-Rename-Master.html","text":"Renaming master branch to release From Adam Dymitruk on SO git checkout -b release master # create and switch to the release branch git push -u origin release # push the release branch to the remote and track it git branch -d master # delete local master If you're using GitHub (as I am), issuing the next needed command ( git push --delete origin master ) will fail because GitHub won't let you delete the default branch, which is stil master . In order to successfully be able to delete the remote master branch, you have to set the default branch on GitHub to be the newly created branch (i.e. release ). After the default branch has been changed to the newly created branch ( release in this case), issuing the following commands will now work: git push --delete origin master # delete remote master git remote prune origin # delete the remote tracking branch NOTE Apparantly GitHub does not allow wikis to be anything other than the master branch. This is why I had to move to my own hosting service to continue this dev blog. 2016-09-21","title":"Git Rename Master"},{"location":"Git-Rename-Master.html#renaming-master-branch-to-release","text":"From Adam Dymitruk on SO git checkout -b release master # create and switch to the release branch git push -u origin release # push the release branch to the remote and track it git branch -d master # delete local master If you're using GitHub (as I am), issuing the next needed command ( git push --delete origin master ) will fail because GitHub won't let you delete the default branch, which is stil master . In order to successfully be able to delete the remote master branch, you have to set the default branch on GitHub to be the newly created branch (i.e. release ). After the default branch has been changed to the newly created branch ( release in this case), issuing the following commands will now work: git push --delete origin master # delete remote master git remote prune origin # delete the remote tracking branch","title":"Renaming master branch to release"},{"location":"Git-Rename-Master.html#note","text":"Apparantly GitHub does not allow wikis to be anything other than the master branch. This is why I had to move to my own hosting service to continue this dev blog.","title":"NOTE"},{"location":"Git-Rename-Master.html#2016-09-21","text":"","title":"2016-09-21"},{"location":"Halting-Problem.html","text":"Halting Problem The Halting Problem asks whether there exists a program that takes other programs as input and determines whether they loop forever or halt. Assume such a program exists and call it HP . The program HP(P,X) assumes: HP is a finite program HP stops in finite time HP takes as input a program, P , with input, X , both of finite length HP can access arbitrarily long memory such as a tape in a Turing Machine model. Though a bit far afield from the current topic, there also needs to be constraints on the time it takes to access distant memory so as not to 'hide' computation in memory access. For example, assuming memory is a linear tape and the time to reach a distance, d , from the current location takes time proportional to d . Consider the program SPITE: SPITE(P) { if (HP(P,P) reports P halts with P as input) { while (true) {} // loop forever } else if (HP(P,P) reports P loops forever with P as input) { halt // return } } When we run SPITE(SPITE) ( SPITE with itself as input), there are two cases: SPITE(SPITE) halts, in which case the first condition would have been hit, contradicting the subsequent action of looping forever. SPITE(SPITE) loops forever, in which case the second condition would have been hit, contradicting the subsequent action of halting. No matter the path we take, we get a contradiction, proving the impossibility of HP existing with our given assumptions. A quick note as it applies to statement verification. The proof is very much the same. To proceed via a proof by contradiction, we assume a function called Verifier , which takes in a statement and returns whether that statement is true or false. We have to allow for functions ( Verifier is one after all) and so we need to differentiate between function evaluation and writing out a statement without evaluation. Below, the convention {.} will be used to denote the \"source\" of a statement rather than it's evaluation. That is, {f(s)} is the statement f(s) rather than the evaluation of the function f on statement s . We construct the following functions: func FunctionVerifier(f,s) if Verifier( { f(s) } ) return True else return False func Spite(f) if FunctionVerifier(f,f) return False else return True We then get a contradiction by asking what the return value of Spite(Spite) is. The above is a bit sloppy because of the differentiation between function, function evaluation, statement, etc. and is why I prefer the Turing Machine model (or any more formal programming language model) but hopefully the intent should be clear. 2018-06-13","title":"Halting Problem"},{"location":"Halting-Problem.html#halting-problem","text":"The Halting Problem asks whether there exists a program that takes other programs as input and determines whether they loop forever or halt. Assume such a program exists and call it HP . The program HP(P,X) assumes: HP is a finite program HP stops in finite time HP takes as input a program, P , with input, X , both of finite length HP can access arbitrarily long memory such as a tape in a Turing Machine model. Though a bit far afield from the current topic, there also needs to be constraints on the time it takes to access distant memory so as not to 'hide' computation in memory access. For example, assuming memory is a linear tape and the time to reach a distance, d , from the current location takes time proportional to d . Consider the program SPITE: SPITE(P) { if (HP(P,P) reports P halts with P as input) { while (true) {} // loop forever } else if (HP(P,P) reports P loops forever with P as input) { halt // return } } When we run SPITE(SPITE) ( SPITE with itself as input), there are two cases: SPITE(SPITE) halts, in which case the first condition would have been hit, contradicting the subsequent action of looping forever. SPITE(SPITE) loops forever, in which case the second condition would have been hit, contradicting the subsequent action of halting. No matter the path we take, we get a contradiction, proving the impossibility of HP existing with our given assumptions. A quick note as it applies to statement verification. The proof is very much the same. To proceed via a proof by contradiction, we assume a function called Verifier , which takes in a statement and returns whether that statement is true or false. We have to allow for functions ( Verifier is one after all) and so we need to differentiate between function evaluation and writing out a statement without evaluation. Below, the convention {.} will be used to denote the \"source\" of a statement rather than it's evaluation. That is, {f(s)} is the statement f(s) rather than the evaluation of the function f on statement s . We construct the following functions: func FunctionVerifier(f,s) if Verifier( { f(s) } ) return True else return False func Spite(f) if FunctionVerifier(f,f) return False else return True We then get a contradiction by asking what the return value of Spite(Spite) is. The above is a bit sloppy because of the differentiation between function, function evaluation, statement, etc. and is why I prefer the Turing Machine model (or any more formal programming language model) but hopefully the intent should be clear.","title":"Halting Problem"},{"location":"Halting-Problem.html#2018-06-13","text":"","title":"2018-06-13"},{"location":"History.html","text":"History Years Ago Event $13.7 \\cdot 10^9$ Beginning of universe $4.6 \\cdot 10^9$ Sun Formation $4.5 \\cdot 10^9$ Earth formation $4.4 \\cdot 10^9$ Water on Earth $3.77 \\cdot 10^9$ Earliest life on Earth $3.5 \\cdot 10^9$ Oxygen on Earth $440 \\cdot 10^6$ First land animals $230 \\cdot 10^6$ First dinosaurs $200 \\cdot 10^6$ First mammals $180 \\cdot 10^6$ First flowers $2.6 \\cdot 10^6$ Start of last ice age $12 \\cdot 10^4 - 35 \\cdot 10^3$ Neanderthals and Homo sapiens sapiens appear $11.7 \\cdot 10^3$ End of last ice age $11.5 \\cdot 10^3$ First agriculture $-8000$ First African civilization appear $-3100$ King Menes founds Memphis in Egypt $-3000$ First Mayan civilizations (central America) $-3000 - -1100$ Bronze age $-1100 - -6000$ Iron age $-1240$ Moses creates the ten commandments $-27 - 476$ Roman Empire $-500 - -300 Aristotle, Plato, Socrates $780 - 850$ Muhammad ibn Musa al-Khwazirmi $800 - 1400$ Islamic Golden Age $1066$ Battle of Hastings $1725$ Invention of punch cards $1904$ Invention of the vacuum tube $1914 - 1918$ First world war $1926$ Invention of transistor $1939 - 1945$ Second world war","title":"History"},{"location":"History.html#history","text":"Years Ago Event $13.7 \\cdot 10^9$ Beginning of universe $4.6 \\cdot 10^9$ Sun Formation $4.5 \\cdot 10^9$ Earth formation $4.4 \\cdot 10^9$ Water on Earth $3.77 \\cdot 10^9$ Earliest life on Earth $3.5 \\cdot 10^9$ Oxygen on Earth $440 \\cdot 10^6$ First land animals $230 \\cdot 10^6$ First dinosaurs $200 \\cdot 10^6$ First mammals $180 \\cdot 10^6$ First flowers $2.6 \\cdot 10^6$ Start of last ice age $12 \\cdot 10^4 - 35 \\cdot 10^3$ Neanderthals and Homo sapiens sapiens appear $11.7 \\cdot 10^3$ End of last ice age $11.5 \\cdot 10^3$ First agriculture $-8000$ First African civilization appear $-3100$ King Menes founds Memphis in Egypt $-3000$ First Mayan civilizations (central America) $-3000 - -1100$ Bronze age $-1100 - -6000$ Iron age $-1240$ Moses creates the ten commandments $-27 - 476$ Roman Empire $-500 - -300 Aristotle, Plato, Socrates $780 - 850$ Muhammad ibn Musa al-Khwazirmi $800 - 1400$ Islamic Golden Age $1066$ Battle of Hastings $1725$ Invention of punch cards $1904$ Invention of the vacuum tube $1914 - 1918$ First world war $1926$ Invention of transistor $1939 - 1945$ Second world war","title":"History"},{"location":"Human-Entropy.html","text":"Human Entropy Information Item Value Unit Reference(s) Brain Capacity 2.5 Petabytes Scientific American, May 1 2010, Paul Reber, \"What is the Memory Capacity of the Human Brain.\" (*) Human Eye 324 Megapixels Notes on the Resolution and Other Details of the Human Eye (*) - (100 * 10^9 neurons) * (10k avg. connections / neuron) $= 10^15$. References","title":"Human Entropy"},{"location":"Human-Entropy.html#human-entropy-information","text":"Item Value Unit Reference(s) Brain Capacity 2.5 Petabytes Scientific American, May 1 2010, Paul Reber, \"What is the Memory Capacity of the Human Brain.\" (*) Human Eye 324 Megapixels Notes on the Resolution and Other Details of the Human Eye (*) - (100 * 10^9 neurons) * (10k avg. connections / neuron) $= 10^15$.","title":"Human Entropy Information"},{"location":"Human-Entropy.html#references","text":"","title":"References"},{"location":"Image-Resize.html","text":"Resize with Imagemagick Using Imagemagick's convert tool, you can resize an image but Imagemagick does interpolation so the resulting image can look pretty bad. For example, using the input (animated) gif: And issuing the command to resize it by a factor of 4: $ convert tree_anim_inp.gif -resize 400% tree_anim_ugly.gif produces the interpolated (and ugly) picture: Instead, use the scale command: $ convert tree_anim_inp.gif -scale 400% tree_anim_out.gif Reference ImageMagick resize without interpolation Saving animated GIFs in Gimp Save as a .gif file extension. When exporting, there will be an option to save as an animation: Check it and you should have an animated GIF. 2015-10-27","title":"Image Resize"},{"location":"Image-Resize.html#resize-with-imagemagick","text":"Using Imagemagick's convert tool, you can resize an image but Imagemagick does interpolation so the resulting image can look pretty bad. For example, using the input (animated) gif: And issuing the command to resize it by a factor of 4: $ convert tree_anim_inp.gif -resize 400% tree_anim_ugly.gif produces the interpolated (and ugly) picture: Instead, use the scale command: $ convert tree_anim_inp.gif -scale 400% tree_anim_out.gif","title":"Resize with Imagemagick"},{"location":"Image-Resize.html#reference","text":"ImageMagick resize without interpolation","title":"Reference"},{"location":"Image-Resize.html#saving-animated-gifs-in-gimp","text":"Save as a .gif file extension. When exporting, there will be an option to save as an animation: Check it and you should have an animated GIF.","title":"Saving animated GIFs in Gimp"},{"location":"Image-Resize.html#2015-10-27","text":"","title":"2015-10-27"},{"location":"Is-It-Really-Open.html","text":"Is It Really Open From the Wikipedia entry on the Open Source Model : Open source promotes universal access via an open-source or free license to a product's design or blueprint, and universal redistribution of that design or blueprint. Project Open Source? Claim Individual/Organization Notes Air Quality Egg No \" ... a ... device that uses sensors to record changes in the levels of specified air contaminants.\" GitHub There are multiple references to it being \"open-source hardware\" on airquailityegg.com but the source and design files available on GitHub have no license. BCN3D yes \"A fully Open Source 3D printed robot arm\" source (GPLv3), hardware (CERN OHL), electronics (CERN OHL) DIY LilCNC yes \"... open-source set of plans for an inexpensive, fully functional 3-axis CNC ...\" DIYLilCNC Downloads Doti: Open Source Jacquard Loom No \"open source\" Doti From the website: \"Doti is an open source desktop jacquard loom ...\" yet no design files are available OpenEIT No \"open\" FarmBot yes \"Open Source CNC Farming\" source (MIT), design files (CC0) Index PnP yes \"Open Source pick and place machine\" source GPLv3 InMoov No \"InMoov is the first Open Source 3D printed life-size robot.\" Design files are under a non commercial license. Lasersaur yes \"open source laser cutter\" Lasersaur License ODrive yes \"... accurately driving brushless motors, for cheap.\" source hardware OpenBike No \"... Open source designs for all ...\" source All files are available under a CC-BY-NC-SA license OpenCat mixed \"A programmable and highly maneuverable robotic cat ...\" OpenCat Source files are under MIT license but design files for the robot are not available. OpenDog yes \"CAD and code for each episode of my open source dog series\" XRobots GPLv3 Open Desk mixed \"open\" OpenDesk.cc Though they don't claim anywhere that they're \"open source\", the name is misleading. Many of their furniture designs are free/libre (CC0, CC-BY) but many are under a non-commercial license OpenKnit No \"open source digital knitting\" OpenKnit No license on source , Instructables build under a non commercial license (CC-BY-NC-SA) OpenMower No \"Open Source\" GitHub The license is CC-BY-NC-SA 4.0 Open Press Project No \"Open Press\" Martin Schneider All design files are under a non commercial license (CC-BY-NC) OpenROV mixed \"OpenROV is a DIY telerobotics community centered around underwater exploration & adventure.\" Source code is under an MIT license but design files are under a non commercial license (CC-BY-NC-SA). OpenScanner No \"Open Hardware\" OpenScan.eu The project uses third party software to do the photogramatry, so the project consists of the PCB design files, Arduino code and 3D design files all which have no license information or are under a non-commercial license . Open Source Fashion unknown \"Open Source\" in name Open Source Fashion I don't know what's \"open source\" about their site. It looks to be just a blog. Free Sewing yes \"an open source platform for made-to-measure sewing patterns\" Free Sewing Source is free/libre and other content is under CC-BY Openwear yes * ( now defunct ) \"Openwear is an open-source brand concept ...\" Openwear Pattern files look to be under free/libre licenses (CC-BY-SA etc.) OSLoom No \"open source jacquard loom\" OS Loom Though there are claims of the source being under GPL and the hardware being under OHL, I can't find any design files. Other project design files, when available, are all under a non commercial license (CC-BY-NC-SA). Poppy Humanoid yes \"... an open-source and 3D printed humanoid robot.\" source Seamly2D / Valentina yes \"Open source patternmaking software.\" Seamly2D Pattern making software STM32 Open Source Multimeter No \"Open source multimeter\".\" EmbedBlog All source , case design files and documentation are under a non-commercial license (CC-BY-NC-SA) Thor yes \"DIY 3D Printable Robotic Arm\" source (CC-BY-SA) Comments From the Wikipedia entry on Open Source Software : Open-source software (OSS) is computer software with its source code made available with a license in which the copyright holder provides the rights to study, change, and distribute the software to anyone and for any purpose. This is an attempt to create a list of projects to reconcile claims of openness with whether they actually are open. Often times companies claim that their projects are \"open source\", either computer source code, design files or other digital artifacts, but either fail to provide the source or fail to provide proper licensing. When talking about other digital assets, either art files, design files, documentation, etc., what constitutes the analog of \"open source\" can get confusing, but I will generally take it to mean that they provide the fundamental copyrightable material to help with or recreate the project under a license that respects the freedoms of being able to run or create, to study, to change and to distribute, including distribution for commercial purposes. 2019-01-10","title":"Is It Really Open"},{"location":"Is-It-Really-Open.html#is-it-really-open","text":"From the Wikipedia entry on the Open Source Model : Open source promotes universal access via an open-source or free license to a product's design or blueprint, and universal redistribution of that design or blueprint. Project Open Source? Claim Individual/Organization Notes Air Quality Egg No \" ... a ... device that uses sensors to record changes in the levels of specified air contaminants.\" GitHub There are multiple references to it being \"open-source hardware\" on airquailityegg.com but the source and design files available on GitHub have no license. BCN3D yes \"A fully Open Source 3D printed robot arm\" source (GPLv3), hardware (CERN OHL), electronics (CERN OHL) DIY LilCNC yes \"... open-source set of plans for an inexpensive, fully functional 3-axis CNC ...\" DIYLilCNC Downloads Doti: Open Source Jacquard Loom No \"open source\" Doti From the website: \"Doti is an open source desktop jacquard loom ...\" yet no design files are available OpenEIT No \"open\" FarmBot yes \"Open Source CNC Farming\" source (MIT), design files (CC0) Index PnP yes \"Open Source pick and place machine\" source GPLv3 InMoov No \"InMoov is the first Open Source 3D printed life-size robot.\" Design files are under a non commercial license. Lasersaur yes \"open source laser cutter\" Lasersaur License ODrive yes \"... accurately driving brushless motors, for cheap.\" source hardware OpenBike No \"... Open source designs for all ...\" source All files are available under a CC-BY-NC-SA license OpenCat mixed \"A programmable and highly maneuverable robotic cat ...\" OpenCat Source files are under MIT license but design files for the robot are not available. OpenDog yes \"CAD and code for each episode of my open source dog series\" XRobots GPLv3 Open Desk mixed \"open\" OpenDesk.cc Though they don't claim anywhere that they're \"open source\", the name is misleading. Many of their furniture designs are free/libre (CC0, CC-BY) but many are under a non-commercial license OpenKnit No \"open source digital knitting\" OpenKnit No license on source , Instructables build under a non commercial license (CC-BY-NC-SA) OpenMower No \"Open Source\" GitHub The license is CC-BY-NC-SA 4.0 Open Press Project No \"Open Press\" Martin Schneider All design files are under a non commercial license (CC-BY-NC) OpenROV mixed \"OpenROV is a DIY telerobotics community centered around underwater exploration & adventure.\" Source code is under an MIT license but design files are under a non commercial license (CC-BY-NC-SA). OpenScanner No \"Open Hardware\" OpenScan.eu The project uses third party software to do the photogramatry, so the project consists of the PCB design files, Arduino code and 3D design files all which have no license information or are under a non-commercial license . Open Source Fashion unknown \"Open Source\" in name Open Source Fashion I don't know what's \"open source\" about their site. It looks to be just a blog. Free Sewing yes \"an open source platform for made-to-measure sewing patterns\" Free Sewing Source is free/libre and other content is under CC-BY Openwear yes * ( now defunct ) \"Openwear is an open-source brand concept ...\" Openwear Pattern files look to be under free/libre licenses (CC-BY-SA etc.) OSLoom No \"open source jacquard loom\" OS Loom Though there are claims of the source being under GPL and the hardware being under OHL, I can't find any design files. Other project design files, when available, are all under a non commercial license (CC-BY-NC-SA). Poppy Humanoid yes \"... an open-source and 3D printed humanoid robot.\" source Seamly2D / Valentina yes \"Open source patternmaking software.\" Seamly2D Pattern making software STM32 Open Source Multimeter No \"Open source multimeter\".\" EmbedBlog All source , case design files and documentation are under a non-commercial license (CC-BY-NC-SA) Thor yes \"DIY 3D Printable Robotic Arm\" source (CC-BY-SA)","title":"Is It Really Open"},{"location":"Is-It-Really-Open.html#comments","text":"From the Wikipedia entry on Open Source Software : Open-source software (OSS) is computer software with its source code made available with a license in which the copyright holder provides the rights to study, change, and distribute the software to anyone and for any purpose. This is an attempt to create a list of projects to reconcile claims of openness with whether they actually are open. Often times companies claim that their projects are \"open source\", either computer source code, design files or other digital artifacts, but either fail to provide the source or fail to provide proper licensing. When talking about other digital assets, either art files, design files, documentation, etc., what constitutes the analog of \"open source\" can get confusing, but I will generally take it to mean that they provide the fundamental copyrightable material to help with or recreate the project under a license that respects the freedoms of being able to run or create, to study, to change and to distribute, including distribution for commercial purposes.","title":"Comments"},{"location":"Is-It-Really-Open.html#2019-01-10","text":"","title":"2019-01-10"},{"location":"JavaScript-Cheat-Sheet.html","text":"JavaScript Cheat Sheet array. indexOf var a = ['a', 'x', 'z']; console.log( a.indexOf('x'), a.indexOf('.') ); 1 -1 forEach ['a','b','c'].forEach( function(ele,idx,arr) { console.log(ele,idx,arr); } ); a 0 [ 'a', 'b', 'c' ] b 1 [ 'a', 'b', 'c' ] c 2 [ 'a', 'b', 'c' ] let and var var let scoped to immediate function body scoped to immediate enclosing block hoisted unhoisted global declaration attaches to global object redeclaration allowed no redeclaration allowed scope function f() { { var v=\"v\"; let l=\"l\"; } console.log(v); console.log(l); } f(); v ReferenceError: l is not defined hoisting function f() { console.log(u); } function g() { console.log(v); console.log(l); var v = \"v\"; let l = \"l\"; } f(); g(); ReferenceError: u is not defined undefined ReferenceError: l is not defined global attachment var v = \"v\"; let l = \"l\"; console.log(this.v, this.l); v undefined redeclaration function f() { var v = \"v\"; let l = \"l\"; var v = \"a\"; let l = \"e\"; } f(); SyntaxError: Identifier 'l' has already been declared const const c = \"c\"; == vs === == does implicit type conversion when comparing === matches only when the values are equal as our their types if (0 == '') { console.log(\"0 == ''\"); } else if (0 != '') { console.log(\"0 != ''\"); } if (0 === '') { console.log(\"0 === ''\"); } else if (0 !== '') { console.log(\"0 !== ''\"); } 0 == '' 0 !== '' ?? let a, b = \"b\"; let u = ( a ?? \"u\"), v = ( b ?? \"v\"); console.log(a,b,u,v); undefined b u b => let f = () => console.log(\"f()\"), g = (u) => console.log(\"g(\", u, \")\"), h = ( (v,w) => { console.log(\"h\"); console.log(v,w); } ); let _f = function() { console.log(\"_f()\"); }, _g = function(u) { console.log(\"_g(\", u, \")\"); }, _h = function(v,w) { console.log(\"h\"); console.log(v,w); }; f(); g('.'); h('.', '..'); _f(); _g('.'); _h('.', '..'); f() g( . ) h . .. _f() _g( . ) h . .. todo new function.(bind|call|apply) anonymous function callback pattern array.(map|slice|push|pop|fill|findIndex|indexOf) 2020-12-08","title":"JavaScript Cheat Sheet"},{"location":"JavaScript-Cheat-Sheet.html#javascript-cheat-sheet","text":"","title":"JavaScript Cheat Sheet"},{"location":"JavaScript-Cheat-Sheet.html#array","text":"","title":"array."},{"location":"JavaScript-Cheat-Sheet.html#indexof","text":"var a = ['a', 'x', 'z']; console.log( a.indexOf('x'), a.indexOf('.') ); 1 -1","title":"indexOf"},{"location":"JavaScript-Cheat-Sheet.html#_1","text":"","title":""},{"location":"JavaScript-Cheat-Sheet.html#foreach","text":"['a','b','c'].forEach( function(ele,idx,arr) { console.log(ele,idx,arr); } ); a 0 [ 'a', 'b', 'c' ] b 1 [ 'a', 'b', 'c' ] c 2 [ 'a', 'b', 'c' ]","title":"forEach"},{"location":"JavaScript-Cheat-Sheet.html#let-and-var","text":"var let scoped to immediate function body scoped to immediate enclosing block hoisted unhoisted global declaration attaches to global object redeclaration allowed no redeclaration allowed","title":"let and var"},{"location":"JavaScript-Cheat-Sheet.html#scope","text":"function f() { { var v=\"v\"; let l=\"l\"; } console.log(v); console.log(l); } f(); v ReferenceError: l is not defined","title":"scope"},{"location":"JavaScript-Cheat-Sheet.html#hoisting","text":"function f() { console.log(u); } function g() { console.log(v); console.log(l); var v = \"v\"; let l = \"l\"; } f(); g(); ReferenceError: u is not defined undefined ReferenceError: l is not defined","title":"hoisting"},{"location":"JavaScript-Cheat-Sheet.html#global-attachment","text":"var v = \"v\"; let l = \"l\"; console.log(this.v, this.l); v undefined","title":"global attachment"},{"location":"JavaScript-Cheat-Sheet.html#redeclaration","text":"function f() { var v = \"v\"; let l = \"l\"; var v = \"a\"; let l = \"e\"; } f(); SyntaxError: Identifier 'l' has already been declared","title":"redeclaration"},{"location":"JavaScript-Cheat-Sheet.html#const","text":"const c = \"c\";","title":"const"},{"location":"JavaScript-Cheat-Sheet.html#vs","text":"== does implicit type conversion when comparing === matches only when the values are equal as our their types if (0 == '') { console.log(\"0 == ''\"); } else if (0 != '') { console.log(\"0 != ''\"); } if (0 === '') { console.log(\"0 === ''\"); } else if (0 !== '') { console.log(\"0 !== ''\"); } 0 == '' 0 !== ''","title":"== vs ==="},{"location":"JavaScript-Cheat-Sheet.html#_2","text":"let a, b = \"b\"; let u = ( a ?? \"u\"), v = ( b ?? \"v\"); console.log(a,b,u,v); undefined b u b","title":"??"},{"location":"JavaScript-Cheat-Sheet.html#_3","text":"let f = () => console.log(\"f()\"), g = (u) => console.log(\"g(\", u, \")\"), h = ( (v,w) => { console.log(\"h\"); console.log(v,w); } ); let _f = function() { console.log(\"_f()\"); }, _g = function(u) { console.log(\"_g(\", u, \")\"); }, _h = function(v,w) { console.log(\"h\"); console.log(v,w); }; f(); g('.'); h('.', '..'); _f(); _g('.'); _h('.', '..'); f() g( . ) h . .. _f() _g( . ) h . ..","title":"=&gt;"},{"location":"JavaScript-Cheat-Sheet.html#todo","text":"new function.(bind|call|apply) anonymous function callback pattern array.(map|slice|push|pop|fill|findIndex|indexOf)","title":"todo"},{"location":"JavaScript-Cheat-Sheet.html#2020-12-08","text":"","title":"2020-12-08"},{"location":"Kelly-Criterion.html","text":"Kelly Criterion The Wikipedia article on the Kelly criterion might say more, but here is a simple derivation. Assuming you have a coin with probability $p$ of coming up heads and odds of $b:1$, the Kelly criterion states: $$ f^* = \\frac{bp-q}{b} $$ Where $f^*$ is the fraction of your money pot the Kelly criterion tells you to bet and $q=1-p$. That is: $$ f^* = \\frac{bp-(1-p)}{b} \\ = \\frac{p(b+1)-1}{b} $$ Assuming the strategy is to bet a fraction of your bank roll every round, with $W_0$ as the initial bank roll, $n$ time units and $W_n$ as your winnings at time $n$: $$ W_n = (1 + br)^{pn} (1 - r)^{(1-p)n} W_0 $$ Taking logarithms, setting the derivative with respect to $r$ and solving: $$ \\begin{array} . & \\frac{d}{dr} \\ln(W_n) &= \\frac{d}{dr} ( pn \\ln(1+br) + (1-p)n \\ln(1-r) + \\ln(W_0) ) \\\\ \\to & 0 &= \\frac{pnb}{1+br} - \\frac{(1-p)n}{1-r} \\\\ \\to & \\frac{(1-p)n}{1-r} &= \\frac{pnb}{1+br} \\\\ \\to & \\frac{1-p}{1-r} &= \\frac{pb}{1+br} \\\\ \\to & (1-p) (1+br) &= pb (1-r) \\\\ \\to & 1 - p + br - pbr &= pb - pbr \\\\ \\to & 1 - p + br &= pb \\\\ \\to & r &= \\frac{pb + p - 1}{b} \\\\ \\to & r &= \\frac{b(p + 1) - 1}{b} \\\\ \\end{array} $$ 2017-08-18","title":"Kelly Criterion"},{"location":"Kelly-Criterion.html#kelly-criterion","text":"The Wikipedia article on the Kelly criterion might say more, but here is a simple derivation. Assuming you have a coin with probability $p$ of coming up heads and odds of $b:1$, the Kelly criterion states: $$ f^* = \\frac{bp-q}{b} $$ Where $f^*$ is the fraction of your money pot the Kelly criterion tells you to bet and $q=1-p$. That is: $$ f^* = \\frac{bp-(1-p)}{b} \\ = \\frac{p(b+1)-1}{b} $$ Assuming the strategy is to bet a fraction of your bank roll every round, with $W_0$ as the initial bank roll, $n$ time units and $W_n$ as your winnings at time $n$: $$ W_n = (1 + br)^{pn} (1 - r)^{(1-p)n} W_0 $$ Taking logarithms, setting the derivative with respect to $r$ and solving: $$ \\begin{array} . & \\frac{d}{dr} \\ln(W_n) &= \\frac{d}{dr} ( pn \\ln(1+br) + (1-p)n \\ln(1-r) + \\ln(W_0) ) \\\\ \\to & 0 &= \\frac{pnb}{1+br} - \\frac{(1-p)n}{1-r} \\\\ \\to & \\frac{(1-p)n}{1-r} &= \\frac{pnb}{1+br} \\\\ \\to & \\frac{1-p}{1-r} &= \\frac{pb}{1+br} \\\\ \\to & (1-p) (1+br) &= pb (1-r) \\\\ \\to & 1 - p + br - pbr &= pb - pbr \\\\ \\to & 1 - p + br &= pb \\\\ \\to & r &= \\frac{pb + p - 1}{b} \\\\ \\to & r &= \\frac{b(p + 1) - 1}{b} \\\\ \\end{array} $$","title":"Kelly Criterion"},{"location":"Kelly-Criterion.html#2017-08-18","text":"","title":"2017-08-18"},{"location":"Kullback-Leibler-Divergence.html","text":"Kullback-Leibler Divergence Here is a short discussion of the Kullback-Leibler divergence that helps give some motivation for what it is an where it comes from. Given two (discrete) probability distribution functions, $p(\\cdot)$ and $q(\\cdot)$, the Kullback-Leibler divergence is defined as: $$ D_{KL}(p || q) = - \\sum_{x} p(x) \\ln(\\frac{p(x)}{q(x)}) $$ The Kullback-Leibler divergence, in some intuitive sense, measures the difference between probability distributions. As motivation, consider a situation where you are providing internet service to a business. This business wants to compress their data but has, incorrectly, guessed at a distribution $q(x)$ for the symbols it's trying to transmit. If you've determined the true distribution to be $p(x)$ the question is, how much is the gain you get by, say, charging the business for compressing their data with the probability distribution $q(x)$ when you have knowledge and can use the true distribution $p(x)$. The business will encode it's messages with an average of $$ -\\sum_{x} p(x) \\lg(q(x)) $$ bits, where the $\\lg(q(x))$ is the encoded bit size of the token $x$ but will only happen with the true underlying distribution $p(x)$. If you take the messages and re-encode them properly, the average bit size will be: $$ -\\sum_{x} p(x) \\lg(p(x)) $$ Taking the difference: $$ \\begin{array}{cl} & -\\sum_{x} p(x) \\lg(p(x)) - [-\\sum_{x} p(x) \\lg(q(x) ] \\\\ \\to & -\\sum_{x} p(x) \\lg( \\frac{p(x)}{q(x)}) \\end{array} $$ On average, $D_{KL}(p || q)$ bits per token are saved. If the business is willing to pay rates based on the bits they see going out, namely $-\\sum_{x} p(x) \\lg(q(x))$, but you're paying only for $-\\sum_{x} p(x) \\lg(p(x))$ bits going out, then there's potential profit on the difference to be made. The cross entropy is a concept closely connected to the Kullback-Leibler distribution. The cross entropy is defined as: $$ H(p,q) = -\\sum_{x} p(x) \\lg(q(x)) $$ The Kullback-Leibler distribution can be seen as the difference of the entropy of the distribution $p(\\cdot)$ and the cross entropy of $p(\\cdot)$ and $q(\\cdot)$: $$ \\begin{array}{cl} D_{KL} & = H(p) - H(p,q) \\\\ & = -\\sum_{x} p(x) \\lg(\\frac{p(x)}{q(x)}) \\end{array} $$ In the above example, the cross entropy is how many bits the business is transmitting under their fallacious assumption of $q(\\cdot)$ as the distribution and the entropy of $p(\\cdot)$ is the \"optimized\" bits you transmit with knowledge of the underlying distribution. When given a \"guess\" distribution, $q(x)$, one can compare how \"bad\" it is compared with a \"true\" or underlying distribution, $p(x)$. 2023-08-22","title":"Kullback Leibler Divergence"},{"location":"Kullback-Leibler-Divergence.html#kullback-leibler-divergence","text":"Here is a short discussion of the Kullback-Leibler divergence that helps give some motivation for what it is an where it comes from. Given two (discrete) probability distribution functions, $p(\\cdot)$ and $q(\\cdot)$, the Kullback-Leibler divergence is defined as: $$ D_{KL}(p || q) = - \\sum_{x} p(x) \\ln(\\frac{p(x)}{q(x)}) $$ The Kullback-Leibler divergence, in some intuitive sense, measures the difference between probability distributions. As motivation, consider a situation where you are providing internet service to a business. This business wants to compress their data but has, incorrectly, guessed at a distribution $q(x)$ for the symbols it's trying to transmit. If you've determined the true distribution to be $p(x)$ the question is, how much is the gain you get by, say, charging the business for compressing their data with the probability distribution $q(x)$ when you have knowledge and can use the true distribution $p(x)$. The business will encode it's messages with an average of $$ -\\sum_{x} p(x) \\lg(q(x)) $$ bits, where the $\\lg(q(x))$ is the encoded bit size of the token $x$ but will only happen with the true underlying distribution $p(x)$. If you take the messages and re-encode them properly, the average bit size will be: $$ -\\sum_{x} p(x) \\lg(p(x)) $$ Taking the difference: $$ \\begin{array}{cl} & -\\sum_{x} p(x) \\lg(p(x)) - [-\\sum_{x} p(x) \\lg(q(x) ] \\\\ \\to & -\\sum_{x} p(x) \\lg( \\frac{p(x)}{q(x)}) \\end{array} $$ On average, $D_{KL}(p || q)$ bits per token are saved. If the business is willing to pay rates based on the bits they see going out, namely $-\\sum_{x} p(x) \\lg(q(x))$, but you're paying only for $-\\sum_{x} p(x) \\lg(p(x))$ bits going out, then there's potential profit on the difference to be made. The cross entropy is a concept closely connected to the Kullback-Leibler distribution. The cross entropy is defined as: $$ H(p,q) = -\\sum_{x} p(x) \\lg(q(x)) $$ The Kullback-Leibler distribution can be seen as the difference of the entropy of the distribution $p(\\cdot)$ and the cross entropy of $p(\\cdot)$ and $q(\\cdot)$: $$ \\begin{array}{cl} D_{KL} & = H(p) - H(p,q) \\\\ & = -\\sum_{x} p(x) \\lg(\\frac{p(x)}{q(x)}) \\end{array} $$ In the above example, the cross entropy is how many bits the business is transmitting under their fallacious assumption of $q(\\cdot)$ as the distribution and the entropy of $p(\\cdot)$ is the \"optimized\" bits you transmit with knowledge of the underlying distribution. When given a \"guess\" distribution, $q(x)$, one can compare how \"bad\" it is compared with a \"true\" or underlying distribution, $p(x)$.","title":"Kullback-Leibler Divergence"},{"location":"Kullback-Leibler-Divergence.html#2023-08-22","text":"","title":"2023-08-22"},{"location":"Lambda-Calculus.html","text":"Lambda Calculus Functional programming: Making you feel clever by allowing you to solve problems that nobody else even knew existed, in order to let you do what everyone else could do from the start. -- taneq ( HN comment ) Syntax Name Description Example $(\\lambda x . M)$ function definition / lambda abstraction Function $M$ taking $x$ as input $(\\lambda x . x^2)$ $(M x)$ function evaluation / application Function $M$ executed on input $x$ $(x^2 3)$ $ \\lambda x . y z = \\lambda x . (y z) $ $ x y z = (x y) z $ Recursion From [6]: Lambda calculus, which is the core of functional languages, only supports function literals and function application. So the definition above is not a valid lambda term ... it involves a recursive binding: the definition ... refers to itself! function fact_base(recur, x) { return function() { if (x==0) { return 1; } return x * (recur(recur, x-1)()); }; } fact = function(x) { return fact_base(fact_base, x)(); } Now > fact(5) 120 Currying The technique of translating the evaluation of a function that takes multiple arguments into evaluating a sequence of functions, each with a single argument. From [6]: Functions with several parameters can be encoded in the lambda calculus via currying, whereby a function taking two parameters is turned into a function that takes the first parameter and returns a function that takes the second parameter. var X = function(recurse, n) { if (0==n) { return 1; } return n * recurse(recurse, n-1); } var Y = function(builder, n) { return builder(builder, n); } var res = Y(X,6); console.log(res); //---- var X1 = function (recurse) { return function(n) { if (0==n) { return 1; } return n * (recurse(recurse))(n-1); }}; var Y1 = function(builder) { return function(n) { return (builder(builder))(n); }}; res = Y1(X1)(6); console.log(res); //--- var res = (function(builder) { return function(n) { return (builder(builder))(n); }})(function (recurse) { return function(n) { if (0==n) { return 1; } return n * (recurse(recurse))(n-1); }})(6); console.log(res); Produces: 720 720 720 Y Combinator $$ Y = \\lambda f . (\\lambda x . f(x x)) (\\lambda x . f(x x)) $$ References Re: Boston.pm perl6/pugs SO: What is a Y-combinator? SO: Clear intuitive derivation of the fixed-point combinator (Y combinator) Wikipedia: Currying Wikipedia: Lambda Calculus The Simple Essence of the Y Combinator (Explained in Python) 2018-09-10","title":"Lambda Calculus"},{"location":"Lambda-Calculus.html#lambda-calculus","text":"Functional programming: Making you feel clever by allowing you to solve problems that nobody else even knew existed, in order to let you do what everyone else could do from the start. -- taneq ( HN comment ) Syntax Name Description Example $(\\lambda x . M)$ function definition / lambda abstraction Function $M$ taking $x$ as input $(\\lambda x . x^2)$ $(M x)$ function evaluation / application Function $M$ executed on input $x$ $(x^2 3)$ $ \\lambda x . y z = \\lambda x . (y z) $ $ x y z = (x y) z $","title":"Lambda Calculus"},{"location":"Lambda-Calculus.html#recursion","text":"From [6]: Lambda calculus, which is the core of functional languages, only supports function literals and function application. So the definition above is not a valid lambda term ... it involves a recursive binding: the definition ... refers to itself! function fact_base(recur, x) { return function() { if (x==0) { return 1; } return x * (recur(recur, x-1)()); }; } fact = function(x) { return fact_base(fact_base, x)(); } Now > fact(5) 120","title":"Recursion"},{"location":"Lambda-Calculus.html#currying","text":"The technique of translating the evaluation of a function that takes multiple arguments into evaluating a sequence of functions, each with a single argument. From [6]: Functions with several parameters can be encoded in the lambda calculus via currying, whereby a function taking two parameters is turned into a function that takes the first parameter and returns a function that takes the second parameter. var X = function(recurse, n) { if (0==n) { return 1; } return n * recurse(recurse, n-1); } var Y = function(builder, n) { return builder(builder, n); } var res = Y(X,6); console.log(res); //---- var X1 = function (recurse) { return function(n) { if (0==n) { return 1; } return n * (recurse(recurse))(n-1); }}; var Y1 = function(builder) { return function(n) { return (builder(builder))(n); }}; res = Y1(X1)(6); console.log(res); //--- var res = (function(builder) { return function(n) { return (builder(builder))(n); }})(function (recurse) { return function(n) { if (0==n) { return 1; } return n * (recurse(recurse))(n-1); }})(6); console.log(res); Produces: 720 720 720","title":"Currying"},{"location":"Lambda-Calculus.html#y-combinator","text":"$$ Y = \\lambda f . (\\lambda x . f(x x)) (\\lambda x . f(x x)) $$","title":"Y Combinator"},{"location":"Lambda-Calculus.html#references","text":"Re: Boston.pm perl6/pugs SO: What is a Y-combinator? SO: Clear intuitive derivation of the fixed-point combinator (Y combinator) Wikipedia: Currying Wikipedia: Lambda Calculus The Simple Essence of the Y Combinator (Explained in Python)","title":"References"},{"location":"Lambda-Calculus.html#2018-09-10","text":"","title":"2018-09-10"},{"location":"Laser-Cutter-Notes.html","text":"Laser Cutter Notes Material Elastic Modulus (GPa) Yield Strength (GPa) Acrylic 3.2 0.072 Delrin 1.5 0.099 MDF 4 0.02 Rubber 0.01-0.1 0.001-0.007 Wood (along grain) Douglas Fir 13 0.03-0.05 Oak 11 0.05-0.1 Pine 9 0.05-0.1 ... The elastic modulus is a measure of stress (force over a given area) over strain (extension over original length) and therefore has pressure units (GPa in the table). A smaller value indicates that the material is more flexible/compliant ... The yield strength of a material is the stress at which a material begins to deform plastically (i.e. it will not return to its original configuration). ... If a material deforms plastically, it is no longer considered compliant When selecting a material, look for a proportionally low elastic modulus and high yield strength. (taken from 1 ) References Laser Cut Like a Boss: Complient Joints Universal Laser Systems, Materials Library, Plastics 2019-09-24","title":"Laser Cutter Notes"},{"location":"Laser-Cutter-Notes.html#laser-cutter-notes","text":"Material Elastic Modulus (GPa) Yield Strength (GPa) Acrylic 3.2 0.072 Delrin 1.5 0.099 MDF 4 0.02 Rubber 0.01-0.1 0.001-0.007 Wood (along grain) Douglas Fir 13 0.03-0.05 Oak 11 0.05-0.1 Pine 9 0.05-0.1 ... The elastic modulus is a measure of stress (force over a given area) over strain (extension over original length) and therefore has pressure units (GPa in the table). A smaller value indicates that the material is more flexible/compliant ... The yield strength of a material is the stress at which a material begins to deform plastically (i.e. it will not return to its original configuration). ... If a material deforms plastically, it is no longer considered compliant When selecting a material, look for a proportionally low elastic modulus and high yield strength. (taken from 1 )","title":"Laser Cutter Notes"},{"location":"Laser-Cutter-Notes.html#references","text":"Laser Cut Like a Boss: Complient Joints Universal Laser Systems, Materials Library, Plastics","title":"References"},{"location":"Laser-Cutter-Notes.html#2019-09-24","text":"","title":"2019-09-24"},{"location":"Littlewood-Polynomials-Notes.html","text":"Littlewood Polynomial Notes Littlewood polynomials are polynomials whose coeffients are drawn from a finite set of ${-1,1}$. Plotting the roots of augmented Littlewood polynomials (allowing for 0 as a coefficient) for a restricted degree $n$ yields pretty pictures. Each point represents a complex root of a polynomial $p(z) = \\sum_{k=0}^{11} c_k z^k$, where $c_k \\in { -1, 0, 1 }$. Claim : $$ \\begin{align} & c_k \\in {-1, 0, 1} \\\\ p(z) & = \\sum c_k z^k \\\\ p(z_0) & = 0 & \\\\ \\to |z_{0}| & \\ge \\frac{1}{2} \\text{ or } z_{0} = 0 \\\\ \\end{align} $$ Proof (by contradiction): $$ \\begin{align} & c_0 \\ne 0 \\\\ \\to p(z_0) & = c_0 + \\sum_{k=1}^{n} c_k z_{0}^k \\\\ \\to 1 & = | \\sum_{k=1}^{n} c_k z_{0}^k | \\\\ \\to 1 & < \\sum_{k=1}^{n} | \\frac{1}{2} |^k < 1 \\\\ \\end{align} $$ source This is sloppy and doesn't take into account degree, $n$, of the polynomial. \"In the limit\", this is true, but this is violated for smaller degree polynomials. In general, we can ask what are the symmetries of transformations on $z$ that will still admit Littlewood polynomials. Some candidates for $p(z)$ are: $p(z^{-1})$ $p(z^{\\dagger})$ $p(-z)$ The complex conjugate gives the mirror symmetry about the x-axis. The negation and complex conjugation gives the symmetry about the y-axis ($p(\\alpha + i \\beta) = 0 \\to q(-\\alpha + i \\beta)=0$). The inversion gives a kind of projection onto the Riemann sphere and/or Mobius transformation like symmetry, giving a projection symmetry about the circle/sphere at radius 1. $$ \\begin{align} & c_k \\in {-1, 0, 1} \\\\ p(z) & = \\sum c_k z^k \\\\ \\end{align} $$ y-axis symmetry ($x \\to -x$): $$ \\begin{align} p_{-}(z) & = \\sum_{k=0}^n (-1)^k c_{k} z^k \\\\ p(z) & = p_{-}(-z) \\\\ \\to p(z_0) & = p_{-}(-z_0) = 0 \\end{align} $$ x-axis symmetry ($x \\to x^{ \\dagger }$): $$ \\begin{align} p_{\\dagger}(z) & = \\sum_{k=0}^n c_{k} ( z^{ \\dagger })^k \\\\ p(z) & = p_{ \\dagger }( z^{ \\dagger } ) \\\\ \\to p(z_0) & = p_{ \\dagger }( z_0^{ \\dagger } ) = 0 \\\\ \\end{align} $$ Since the coefficients are real, $p_{\\dagger}(z)$ has the same roots. Inversion symmetry ($x \\to \\frac{1}{x}$): $$ \\begin{align} p_{-1}(z) & = \\sum_{k=0}^n c_{n-k} z^k \\\\ p(z) & = z^n p_{-1}(z^{-1}) \\\\ \\to p(z_{\\dagger}) & = z_{\\dagger}^n p_{-1}(z_{\\dagger}^{-1}) = 0 \\\\ \\end{align} $$ Noting that $z^n p_{-1}(\\frac{1}{z})$ is also a Littlewood polynomial. All of the above create a situation that for every $p(z)=0$, there's a corresponding $p_{-}(-z)=0$, $p_{\\dagger}(z^{\\dagger})=0$ and $p_{-1}(z^{-1})=0, (z \\ne 0)$, establishing the gross level symmetry. An explanation of how Dragon curves show up: The idea is that the dragon curve can be created through in iterated function system (IFS) by choosing randomly between $f_{-,z}(x)=1-zx$ and $f_{+,z}(x)=1+zx$ for some complex z and initial $x=0$. The iteration of ${f_{-,z}, f_{+,z}}$ trace out all Littlewood polynomials for initial values of $f_{-,z}(0)$ and $f_{+,z}(0)$. Here's what I think the argument is in the category cafe thread: $p(z_0)=0$ is the result of a finite cutoff IFS from above. If the IFS is a nice fractal, that means other polynomials from the IFS evaluated at the source point $z_0$ will trace out a fractal (dragon-like curve) around the image point 0. Since we're dealing with a small neighborhood, we can heuristically use linearity to shift the image points not landing on 0 to 0, moving their source points along with them. The source points will then create a dragon-like fractal curve in the source map, giving us the dragon curves that we see in zoomed in snapshots of the root map. Also note that the 'twist' in the dragon-like curves might be explained with another heuristic argument that goes as follows: Take $p(z_0)=0$ and a 'similar' polynomial Littlewood $q(z_0 + \\delta) = 0$. $p'(z_0) \\sim q'(z_0)$ (and perhaps even more so than $p \\sim q$?), and $q(z_0 + \\delta) \\sim q(z_0) + \\delta q'(z_0)$, we can try solving for $q(z_0) + \\delta q'(z_0) = 0 \\to \\delta = -\\frac{q(z_0)}{q'(z_0)} \\sim -\\frac{q(z_0)}{p'(z_0)}$. So the 'correction' source change for $q(z)$ has the factor of $p'(z_0)$ which might induce a sort of 'twist' to the curve. Attempts at explaining the holes around the unit circle: The holes on the unit disc look to have bigger gaps in a Farey like sequence. Take the upper quadrent of the roots ($z = a+ib, a>0, b>0$) to remove as much symmetry as possible and then take the complex logarithm. The points below the unit disc could also be discarded but the gaps are easier to see when they're left in, which introduces a mirror symmetry in the complex logarithm plot. The lines correspond to the first few Farey sequence numbers with a factor of $\\pi$. One observation is that if we have a polynomial, $p_0(z)$, of $\\deg(p_0) = d$, with root $p_0(z_0)=0$, then this implies the existence of another Littlewood polynomial, $p_1(z)$ with $\\deg(p_1) = \\frac{d}{\\alpha}$ (for $\\frac{d}{\\alpha} \\in \\mathbb{Z}$) and $p_1(z_0^{\\alpha}) = 0$. For example, consider: $$ \\begin{align} p_0(z) & = 1 + z \\\\ p_0(z_0) & = 0 \\\\ \\to p_1(z) & = 1 + z^2 \\\\ \\to p_1(z_0^{\\frac{1}{2}}) & = 1 + (z_0^{\\frac{1}{1}})^2 \\\\ & = 1 + z_0 = 0 \\end{align} $$ If we collect roots for Littlewood polynomials of finite degree $n$, this means that for all polynomials of $\\lfloor \\frac{\\deg(d)}{2} \\rfloor$ ($d \\le n$), there is a corresponding Littlewood polynomial of $deg(d)$ that share half of its roots under the $(\\cdot)^\\frac{1}{2}$ transformation. The same with $\\lfloor \\frac{\\deg(d)}{3} \\rfloor$, $\\lfloor \\frac{\\deg(d)}{5} \\rfloor$, $\\lfloor \\frac{\\deg(d)}{7} \\rfloor$, etc. The roots of $p_0(z)$ get 'squashed' by a factor of $\\alpha$ when they become roots of $p_1(z)$, giving the plot another more subtle symmetry. That is, for every root (of unit length) $e^{i \\theta}$, there are other roots $e^{ i \\alpha \\theta }$ and $e^{ - k \\alpha \\theta }$, for $k$ cycling through all the possibilities. If the \"dead zone\" of roots around ${ 1, -1 }$ is taken as a given, this explains the other \"holes\" around the $|z| = 1$ circle, as the roots fan out to the other locations along the circle and why it's a Farey sequence. The number of polynomials that can be scaled up by $2$ is more than can be scaled up by $3$ which is more than can be scaled up by $5$, etc. Where there's a \"dead zone\" around the ${ 1, -1 }$ points ist still something that I don't understand. As some guy on the street notices, if $p(z)$ and $q(z)$ are Littlewood polynomials, with $\\deg(q) = d-1$, then $q(z) \\cdot p(z^d)$ is also Littlewood. Some questions I have (they might be known, I just don't know them) are how what are the sizes of the dead zone holes and how do they scale as the degree increases. One way to attack this is to plot the minimum distance between roots that fall along the Farey sequence angle proportions (excluding purely real roots) and/or to plot the frequency/falloff. Most likely, once the initial dead zone around $z = 1$ is understood, then the rest should follow, though this would need confirmation. In terms of an initial heuristic argument for the size of the dead zone around $z=1$, the root $z_0=1$ might be privileged because of the way the coefficients are chosen. Any polynomial which has an equal number of coefficients equal to 1 as -1 must have a root at $z=1$. Doing a quick estimate with Stirling's approximation tells us that there should roughly be $O(\\frac{1}{\\sqrt{d}})$ of these polynomials for a given degree $d$. This, combined with the fact that roots tend to \"repel\", by some heuristic measure, might give us enough information to get an understanding of the dead zone around $z=1$. References Baez: The Beauty of Roots n-Category Cafe Bohemian Matrices ( wiki ) Iterated Function System Heighway dragon mpsolve Zeros of Polynomials with 0,1 Coefficients by Odlyzko and Poonen 2022-01-20","title":"Littlewood Polynomials Notes"},{"location":"Littlewood-Polynomials-Notes.html#littlewood-polynomial-notes","text":"Littlewood polynomials are polynomials whose coeffients are drawn from a finite set of ${-1,1}$. Plotting the roots of augmented Littlewood polynomials (allowing for 0 as a coefficient) for a restricted degree $n$ yields pretty pictures. Each point represents a complex root of a polynomial $p(z) = \\sum_{k=0}^{11} c_k z^k$, where $c_k \\in { -1, 0, 1 }$. Claim : $$ \\begin{align} & c_k \\in {-1, 0, 1} \\\\ p(z) & = \\sum c_k z^k \\\\ p(z_0) & = 0 & \\\\ \\to |z_{0}| & \\ge \\frac{1}{2} \\text{ or } z_{0} = 0 \\\\ \\end{align} $$ Proof (by contradiction): $$ \\begin{align} & c_0 \\ne 0 \\\\ \\to p(z_0) & = c_0 + \\sum_{k=1}^{n} c_k z_{0}^k \\\\ \\to 1 & = | \\sum_{k=1}^{n} c_k z_{0}^k | \\\\ \\to 1 & < \\sum_{k=1}^{n} | \\frac{1}{2} |^k < 1 \\\\ \\end{align} $$ source This is sloppy and doesn't take into account degree, $n$, of the polynomial. \"In the limit\", this is true, but this is violated for smaller degree polynomials. In general, we can ask what are the symmetries of transformations on $z$ that will still admit Littlewood polynomials. Some candidates for $p(z)$ are: $p(z^{-1})$ $p(z^{\\dagger})$ $p(-z)$ The complex conjugate gives the mirror symmetry about the x-axis. The negation and complex conjugation gives the symmetry about the y-axis ($p(\\alpha + i \\beta) = 0 \\to q(-\\alpha + i \\beta)=0$). The inversion gives a kind of projection onto the Riemann sphere and/or Mobius transformation like symmetry, giving a projection symmetry about the circle/sphere at radius 1. $$ \\begin{align} & c_k \\in {-1, 0, 1} \\\\ p(z) & = \\sum c_k z^k \\\\ \\end{align} $$ y-axis symmetry ($x \\to -x$): $$ \\begin{align} p_{-}(z) & = \\sum_{k=0}^n (-1)^k c_{k} z^k \\\\ p(z) & = p_{-}(-z) \\\\ \\to p(z_0) & = p_{-}(-z_0) = 0 \\end{align} $$ x-axis symmetry ($x \\to x^{ \\dagger }$): $$ \\begin{align} p_{\\dagger}(z) & = \\sum_{k=0}^n c_{k} ( z^{ \\dagger })^k \\\\ p(z) & = p_{ \\dagger }( z^{ \\dagger } ) \\\\ \\to p(z_0) & = p_{ \\dagger }( z_0^{ \\dagger } ) = 0 \\\\ \\end{align} $$ Since the coefficients are real, $p_{\\dagger}(z)$ has the same roots. Inversion symmetry ($x \\to \\frac{1}{x}$): $$ \\begin{align} p_{-1}(z) & = \\sum_{k=0}^n c_{n-k} z^k \\\\ p(z) & = z^n p_{-1}(z^{-1}) \\\\ \\to p(z_{\\dagger}) & = z_{\\dagger}^n p_{-1}(z_{\\dagger}^{-1}) = 0 \\\\ \\end{align} $$ Noting that $z^n p_{-1}(\\frac{1}{z})$ is also a Littlewood polynomial. All of the above create a situation that for every $p(z)=0$, there's a corresponding $p_{-}(-z)=0$, $p_{\\dagger}(z^{\\dagger})=0$ and $p_{-1}(z^{-1})=0, (z \\ne 0)$, establishing the gross level symmetry. An explanation of how Dragon curves show up: The idea is that the dragon curve can be created through in iterated function system (IFS) by choosing randomly between $f_{-,z}(x)=1-zx$ and $f_{+,z}(x)=1+zx$ for some complex z and initial $x=0$. The iteration of ${f_{-,z}, f_{+,z}}$ trace out all Littlewood polynomials for initial values of $f_{-,z}(0)$ and $f_{+,z}(0)$. Here's what I think the argument is in the category cafe thread: $p(z_0)=0$ is the result of a finite cutoff IFS from above. If the IFS is a nice fractal, that means other polynomials from the IFS evaluated at the source point $z_0$ will trace out a fractal (dragon-like curve) around the image point 0. Since we're dealing with a small neighborhood, we can heuristically use linearity to shift the image points not landing on 0 to 0, moving their source points along with them. The source points will then create a dragon-like fractal curve in the source map, giving us the dragon curves that we see in zoomed in snapshots of the root map. Also note that the 'twist' in the dragon-like curves might be explained with another heuristic argument that goes as follows: Take $p(z_0)=0$ and a 'similar' polynomial Littlewood $q(z_0 + \\delta) = 0$. $p'(z_0) \\sim q'(z_0)$ (and perhaps even more so than $p \\sim q$?), and $q(z_0 + \\delta) \\sim q(z_0) + \\delta q'(z_0)$, we can try solving for $q(z_0) + \\delta q'(z_0) = 0 \\to \\delta = -\\frac{q(z_0)}{q'(z_0)} \\sim -\\frac{q(z_0)}{p'(z_0)}$. So the 'correction' source change for $q(z)$ has the factor of $p'(z_0)$ which might induce a sort of 'twist' to the curve. Attempts at explaining the holes around the unit circle: The holes on the unit disc look to have bigger gaps in a Farey like sequence. Take the upper quadrent of the roots ($z = a+ib, a>0, b>0$) to remove as much symmetry as possible and then take the complex logarithm. The points below the unit disc could also be discarded but the gaps are easier to see when they're left in, which introduces a mirror symmetry in the complex logarithm plot. The lines correspond to the first few Farey sequence numbers with a factor of $\\pi$. One observation is that if we have a polynomial, $p_0(z)$, of $\\deg(p_0) = d$, with root $p_0(z_0)=0$, then this implies the existence of another Littlewood polynomial, $p_1(z)$ with $\\deg(p_1) = \\frac{d}{\\alpha}$ (for $\\frac{d}{\\alpha} \\in \\mathbb{Z}$) and $p_1(z_0^{\\alpha}) = 0$. For example, consider: $$ \\begin{align} p_0(z) & = 1 + z \\\\ p_0(z_0) & = 0 \\\\ \\to p_1(z) & = 1 + z^2 \\\\ \\to p_1(z_0^{\\frac{1}{2}}) & = 1 + (z_0^{\\frac{1}{1}})^2 \\\\ & = 1 + z_0 = 0 \\end{align} $$ If we collect roots for Littlewood polynomials of finite degree $n$, this means that for all polynomials of $\\lfloor \\frac{\\deg(d)}{2} \\rfloor$ ($d \\le n$), there is a corresponding Littlewood polynomial of $deg(d)$ that share half of its roots under the $(\\cdot)^\\frac{1}{2}$ transformation. The same with $\\lfloor \\frac{\\deg(d)}{3} \\rfloor$, $\\lfloor \\frac{\\deg(d)}{5} \\rfloor$, $\\lfloor \\frac{\\deg(d)}{7} \\rfloor$, etc. The roots of $p_0(z)$ get 'squashed' by a factor of $\\alpha$ when they become roots of $p_1(z)$, giving the plot another more subtle symmetry. That is, for every root (of unit length) $e^{i \\theta}$, there are other roots $e^{ i \\alpha \\theta }$ and $e^{ - k \\alpha \\theta }$, for $k$ cycling through all the possibilities. If the \"dead zone\" of roots around ${ 1, -1 }$ is taken as a given, this explains the other \"holes\" around the $|z| = 1$ circle, as the roots fan out to the other locations along the circle and why it's a Farey sequence. The number of polynomials that can be scaled up by $2$ is more than can be scaled up by $3$ which is more than can be scaled up by $5$, etc. Where there's a \"dead zone\" around the ${ 1, -1 }$ points ist still something that I don't understand. As some guy on the street notices, if $p(z)$ and $q(z)$ are Littlewood polynomials, with $\\deg(q) = d-1$, then $q(z) \\cdot p(z^d)$ is also Littlewood. Some questions I have (they might be known, I just don't know them) are how what are the sizes of the dead zone holes and how do they scale as the degree increases. One way to attack this is to plot the minimum distance between roots that fall along the Farey sequence angle proportions (excluding purely real roots) and/or to plot the frequency/falloff. Most likely, once the initial dead zone around $z = 1$ is understood, then the rest should follow, though this would need confirmation. In terms of an initial heuristic argument for the size of the dead zone around $z=1$, the root $z_0=1$ might be privileged because of the way the coefficients are chosen. Any polynomial which has an equal number of coefficients equal to 1 as -1 must have a root at $z=1$. Doing a quick estimate with Stirling's approximation tells us that there should roughly be $O(\\frac{1}{\\sqrt{d}})$ of these polynomials for a given degree $d$. This, combined with the fact that roots tend to \"repel\", by some heuristic measure, might give us enough information to get an understanding of the dead zone around $z=1$.","title":"Littlewood Polynomial Notes"},{"location":"Littlewood-Polynomials-Notes.html#references","text":"Baez: The Beauty of Roots n-Category Cafe Bohemian Matrices ( wiki ) Iterated Function System Heighway dragon mpsolve Zeros of Polynomials with 0,1 Coefficients by Odlyzko and Poonen","title":"References"},{"location":"Littlewood-Polynomials-Notes.html#2022-01-20","text":"","title":"2022-01-20"},{"location":"Liveplotting-With-Gnuplot.html","text":"Liveplotting with Gnuplot liveplot.gnuplot set view map set tic scale 0 splot 'map.gp' matrix with image pause 1 reread gen-map #!/usr/bin/python3 import os, time, random n_col, n_row = 128, 128 while True: with open(\"map.tmp\", \"w\") as fp: for _col in range(n_col): line = [] for _row in range(n_row): line.append( str(random.random()) ) fp.write(\" \".join(line) + \"\\n\") os.replace(\"map.tmp\", \"map.gp\") time.sleep(1) 2023-09-28","title":"Liveplotting With Gnuplot"},{"location":"Liveplotting-With-Gnuplot.html#liveplotting-with-gnuplot","text":"liveplot.gnuplot set view map set tic scale 0 splot 'map.gp' matrix with image pause 1 reread gen-map #!/usr/bin/python3 import os, time, random n_col, n_row = 128, 128 while True: with open(\"map.tmp\", \"w\") as fp: for _col in range(n_col): line = [] for _row in range(n_row): line.append( str(random.random()) ) fp.write(\" \".join(line) + \"\\n\") os.replace(\"map.tmp\", \"map.gp\") time.sleep(1)","title":"Liveplotting with Gnuplot"},{"location":"Liveplotting-With-Gnuplot.html#2023-09-28","text":"","title":"2023-09-28"},{"location":"Local-First-Software.html","text":"Local First Software Speed Synchronized Detachable Collaborative Access Secure Control Blog post and original paper . Description Speed Operations should be done on local data as much as possible. Synchronization to off-device storage should be done in the background. Synchronized Data should be synchronized across applications that use it. Detachable The application should be available for use, as much as possible, even when the network isn't present. If local peers are available, data should be synchronizable between the local peers or local network. Collaborative When possible, the application should be able to be used in collaboration with others. Access Data should always be accessible and exportable at all times. Secure End-to-end encryption should be used. Central server storage should have data encrypted at rest. Control The end user is in control of their data and should have full writes over it without synthetic or unwanted restrictions. Implementation Distributed Data Structure Conflict Free Replicated Data Types ( CRDT ) are an option for data synchronization. Operation-based (Commutative Replicated Data Types (CmRDT)) and state-based (Convergent Replicated Data Type (CvRDT)) are two options: Approach Advantages Disadvantages Constraints CmRDT Delta updates Required Communication Guarantees Commutative CvRDT Simplicity Full State Update Commutative, Associative, Idempotent Access Text centric file formats should be preferred such as JSON or CSV. Secure Here are some thoughts on providing secure access options to the application on a device: Last Use Timeout - provide password after a preset time of last use Periodic Timeout - provide password after preset time, regardless of last use Password on Session Start - enter password when entering into a new session or after a session sign off Control It's hard to talk about control without some sort of underlying free software assumption. References Local-First Software: You Own Your Data, in spite of the Cloud by Martin Kleppmann, Adam Wiggins, Peter van Hardenberg and Mark McGranaghan ( link ). 2019-11-20","title":"Local First Software"},{"location":"Local-First-Software.html#local-first-software","text":"Speed Synchronized Detachable Collaborative Access Secure Control Blog post and original paper .","title":"Local First Software"},{"location":"Local-First-Software.html#description","text":"","title":"Description"},{"location":"Local-First-Software.html#speed","text":"Operations should be done on local data as much as possible. Synchronization to off-device storage should be done in the background.","title":"Speed"},{"location":"Local-First-Software.html#synchronized","text":"Data should be synchronized across applications that use it.","title":"Synchronized"},{"location":"Local-First-Software.html#detachable","text":"The application should be available for use, as much as possible, even when the network isn't present. If local peers are available, data should be synchronizable between the local peers or local network.","title":"Detachable"},{"location":"Local-First-Software.html#collaborative","text":"When possible, the application should be able to be used in collaboration with others.","title":"Collaborative"},{"location":"Local-First-Software.html#access","text":"Data should always be accessible and exportable at all times.","title":"Access"},{"location":"Local-First-Software.html#secure","text":"End-to-end encryption should be used. Central server storage should have data encrypted at rest.","title":"Secure"},{"location":"Local-First-Software.html#control","text":"The end user is in control of their data and should have full writes over it without synthetic or unwanted restrictions.","title":"Control"},{"location":"Local-First-Software.html#implementation","text":"","title":"Implementation"},{"location":"Local-First-Software.html#distributed-data-structure","text":"Conflict Free Replicated Data Types ( CRDT ) are an option for data synchronization. Operation-based (Commutative Replicated Data Types (CmRDT)) and state-based (Convergent Replicated Data Type (CvRDT)) are two options: Approach Advantages Disadvantages Constraints CmRDT Delta updates Required Communication Guarantees Commutative CvRDT Simplicity Full State Update Commutative, Associative, Idempotent","title":"Distributed Data Structure"},{"location":"Local-First-Software.html#access_1","text":"Text centric file formats should be preferred such as JSON or CSV.","title":"Access"},{"location":"Local-First-Software.html#secure_1","text":"Here are some thoughts on providing secure access options to the application on a device: Last Use Timeout - provide password after a preset time of last use Periodic Timeout - provide password after preset time, regardless of last use Password on Session Start - enter password when entering into a new session or after a session sign off","title":"Secure"},{"location":"Local-First-Software.html#control_1","text":"It's hard to talk about control without some sort of underlying free software assumption.","title":"Control"},{"location":"Local-First-Software.html#references","text":"Local-First Software: You Own Your Data, in spite of the Cloud by Martin Kleppmann, Adam Wiggins, Peter van Hardenberg and Mark McGranaghan ( link ).","title":"References"},{"location":"Local-First-Software.html#2019-11-20","text":"","title":"2019-11-20"},{"location":"Makefile-cheatsheet.html","text":"Makefile Cheatsheet .PHONY Targets without explicit target .PHONY: clean clean: rm -f *.o 2019-09-24","title":"Makefile cheatsheet"},{"location":"Makefile-cheatsheet.html#makefile-cheatsheet","text":"","title":"Makefile Cheatsheet"},{"location":"Makefile-cheatsheet.html#phony","text":"Targets without explicit target .PHONY: clean clean: rm -f *.o","title":".PHONY"},{"location":"Makefile-cheatsheet.html#2019-09-24","text":"","title":"2019-09-24"},{"location":"Material-Charts.html","text":"Material Charts Centipoise (cps, $ \\frac{N \\cdot s}{10^3 \\cdot m^2} = \\frac{kg}{10^3 \\cdot m \\cdot s} $ ), a measurement of viscosity . 0 water 4,000 honey 15,000 chocolate syrup 60,000 yellow mustard Hardness ( shore durometer ) 20 rubber hand 40 pencil eraser 70 rubber ducky 95 shopping card wheel","title":"Material Charts"},{"location":"Material-Charts.html#material-charts","text":"Centipoise (cps, $ \\frac{N \\cdot s}{10^3 \\cdot m^2} = \\frac{kg}{10^3 \\cdot m \\cdot s} $ ), a measurement of viscosity . 0 water 4,000 honey 15,000 chocolate syrup 60,000 yellow mustard Hardness ( shore durometer ) 20 rubber hand 40 pencil eraser 70 rubber ducky 95 shopping card wheel","title":"Material Charts"},{"location":"Math-Exercises.html","text":"Math Exercises Balls and urns counting (from src ): Balls Urns unrestricted max(1) min(1) l l $u^b$ $(u)_b$ $u! { b \\brace u }$ u l $\\left({u \\choose b }\\right)$ $u \\choose b$ $\\left({u \\choose b-u }\\right)$ l u $\\sum_{i=0}^u { b \\brace i }$ $[ b \\le u ]$ ${ b \\brace u }$ u u $\\sum_{i=1}^u p_i(b) $ $[ b \\le u ]$ $p_u(b)$ l = labelled u = unlabelled $(u) b = \\prod {k=0}^{b-1} (u-k)$ ${ b \\brace u}$ - Stirling's number of the second kind (${ n \\brace k } = S(n,k) = k S(n-1,k) + S(n-1,k-1)$) $ \\left({u \\choose b }\\right) = { n - (k-1) \\choose k }$, \"with replacement\" $ [ b \\le u ]$ = indicator function $ p_k(n) = p_{k-1}(n-1) + p_k(n-k) $, number of ways to partition $n$ into $k$ parts ($n$ as sum of $k$ integers) ll* Keep throwing balls without care about occupancy, you get $u^b$. ll? $u$ choices for the first ball, $(u-1)$ choices for the second ball, etc, until $(u)_b$. ll+ One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. For example, $S(3,2) = { 3 \\brace 2 } = #{ {1} \\cup {2,3}, {2} \\cup {1,3}, {3} \\cup {1,2} }$. The minimum 1 ball per urn, mans that we have a minimum of $u$ subsets. By definition (?) the urns act as sets themselves, so the order of balls within urns doesn't matter. We now want to partition $b$ balls into $u$ sets, which is ${ b \\brace u }$. Since the urns are labelled, we need to multiply by an extra $u!$ factor, which gives: $$ u! { b \\brace u } $$ ul* A stars and bars argument gives the result. Consider $u$ urns, represented by $u+1$ 'bars' and $b$ balls represented as stars. For example ||***|*|||**|*|...|| . There is a constraint that the ends have bars, so there are $u-1 + b$ symbols in the middle. Choosing all configurations gives ${ u-1+b \\choose b }$ configurations, which is $\\left({u \\choose b }\\right$. ul? Since balls are unlabeled, we can divide out by the different labelled configurations. That is, the ll? case divided by $b!$: $$ \\frac{(u)_b}{b!} = { u \\choose b } $$ ul+ This reduces to the unrestricted case ( ul* ) with first laying down a single ball in each bin. After the initial outlay, we have $(b-u)$ balls left to put in the ul case, giving $\\left({u \\choose b-u }\\right)$. lu* One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. For the unrestricted case, we can think of trying to put $b$ balls into some number of (unlabelled) urns running from $1$ to $u$. That is, for each $i$ from $1$ to $u$, find the number of ways of putting $b$ labels into $i$ subsets: $$ \\sum_{i=1}^u { b \\brace i } $$ lu? If there are fewer balls than urns, there is only one way to place the balls in unlabeled urns. lu+ One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. The minimum 1 ball per urn, mans that we have a minimum of $u$ subsets. By definition (?) the urns act as sets themselves, so the order of balls within urns doesn't matter. We now want to partition $b$ balls into $u$ (unlabelled) sets, which is: $$ { b \\brace u } $$ uu* Much like the uu+ case, since the balls and urns are unlabelled, we can think of it as how many ways can the integer $b$ be written as the sum of some number of integers. Since we have no restriction on the minimum number of balls in urns, we have to sum all the different ways to partition, giving: $$ \\sum_{i=1}^u p_i(b) $$ uu? If there are fewer balls than urns, there is only one way to place the balls, labelled or no, in unlabeled urns. uu+ Since both balls and urns are unlabelled, this is equivalent to asking how many ways can the integer $b$ be written as the sum of $u$ integers (partitions), which is $p_u(b)$.","title":"Math Exercises"},{"location":"Math-Exercises.html#math-exercises","text":"Balls and urns counting (from src ): Balls Urns unrestricted max(1) min(1) l l $u^b$ $(u)_b$ $u! { b \\brace u }$ u l $\\left({u \\choose b }\\right)$ $u \\choose b$ $\\left({u \\choose b-u }\\right)$ l u $\\sum_{i=0}^u { b \\brace i }$ $[ b \\le u ]$ ${ b \\brace u }$ u u $\\sum_{i=1}^u p_i(b) $ $[ b \\le u ]$ $p_u(b)$ l = labelled u = unlabelled $(u) b = \\prod {k=0}^{b-1} (u-k)$ ${ b \\brace u}$ - Stirling's number of the second kind (${ n \\brace k } = S(n,k) = k S(n-1,k) + S(n-1,k-1)$) $ \\left({u \\choose b }\\right) = { n - (k-1) \\choose k }$, \"with replacement\" $ [ b \\le u ]$ = indicator function $ p_k(n) = p_{k-1}(n-1) + p_k(n-k) $, number of ways to partition $n$ into $k$ parts ($n$ as sum of $k$ integers)","title":"Math Exercises"},{"location":"Math-Exercises.html#ll","text":"Keep throwing balls without care about occupancy, you get $u^b$.","title":"ll*"},{"location":"Math-Exercises.html#ll_1","text":"$u$ choices for the first ball, $(u-1)$ choices for the second ball, etc, until $(u)_b$.","title":"ll?"},{"location":"Math-Exercises.html#ll_2","text":"One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. For example, $S(3,2) = { 3 \\brace 2 } = #{ {1} \\cup {2,3}, {2} \\cup {1,3}, {3} \\cup {1,2} }$. The minimum 1 ball per urn, mans that we have a minimum of $u$ subsets. By definition (?) the urns act as sets themselves, so the order of balls within urns doesn't matter. We now want to partition $b$ balls into $u$ sets, which is ${ b \\brace u }$. Since the urns are labelled, we need to multiply by an extra $u!$ factor, which gives: $$ u! { b \\brace u } $$","title":"ll+"},{"location":"Math-Exercises.html#ul","text":"A stars and bars argument gives the result. Consider $u$ urns, represented by $u+1$ 'bars' and $b$ balls represented as stars. For example ||***|*|||**|*|...|| . There is a constraint that the ends have bars, so there are $u-1 + b$ symbols in the middle. Choosing all configurations gives ${ u-1+b \\choose b }$ configurations, which is $\\left({u \\choose b }\\right$.","title":"ul*"},{"location":"Math-Exercises.html#ul_1","text":"Since balls are unlabeled, we can divide out by the different labelled configurations. That is, the ll? case divided by $b!$: $$ \\frac{(u)_b}{b!} = { u \\choose b } $$","title":"ul?"},{"location":"Math-Exercises.html#ul_2","text":"This reduces to the unrestricted case ( ul* ) with first laying down a single ball in each bin. After the initial outlay, we have $(b-u)$ balls left to put in the ul case, giving $\\left({u \\choose b-u }\\right)$.","title":"ul+"},{"location":"Math-Exercises.html#lu","text":"One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. For the unrestricted case, we can think of trying to put $b$ balls into some number of (unlabelled) urns running from $1$ to $u$. That is, for each $i$ from $1$ to $u$, find the number of ways of putting $b$ labels into $i$ subsets: $$ \\sum_{i=1}^u { b \\brace i } $$","title":"lu*"},{"location":"Math-Exercises.html#lu_1","text":"If there are fewer balls than urns, there is only one way to place the balls in unlabeled urns.","title":"lu?"},{"location":"Math-Exercises.html#lu_2","text":"One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. The minimum 1 ball per urn, mans that we have a minimum of $u$ subsets. By definition (?) the urns act as sets themselves, so the order of balls within urns doesn't matter. We now want to partition $b$ balls into $u$ (unlabelled) sets, which is: $$ { b \\brace u } $$","title":"lu+"},{"location":"Math-Exercises.html#uu","text":"Much like the uu+ case, since the balls and urns are unlabelled, we can think of it as how many ways can the integer $b$ be written as the sum of some number of integers. Since we have no restriction on the minimum number of balls in urns, we have to sum all the different ways to partition, giving: $$ \\sum_{i=1}^u p_i(b) $$","title":"uu*"},{"location":"Math-Exercises.html#uu_1","text":"If there are fewer balls than urns, there is only one way to place the balls, labelled or no, in unlabeled urns.","title":"uu?"},{"location":"Math-Exercises.html#uu_2","text":"Since both balls and urns are unlabelled, this is equivalent to asking how many ways can the integer $b$ be written as the sum of $u$ integers (partitions), which is $p_u(b)$.","title":"uu+"},{"location":"Math-Notes.html","text":"Math Notes Tests for convergence If $\\sum u_n = S \\in \\mathbb{R}$ then $\\lim_{n \\to \\infty} = 0$. If $\\lim_{n \\to \\infty} \\ne 0$ then $\\sum u_n$ diverges. Note that this is a necessary but insufficient condition for convergence. Absolute Convergence If $\\sum u_n $ converges and $\\sum u_n$ converges Conditional Convergence If $\\sum u_n $ diverges but $\\sum u_n$ converges Integral Test Conditional Convergence If $\\sum u_n $ diverges but $\\sum u_n$ converges","title":"Math Notes"},{"location":"Math-Notes.html#math-notes","text":"","title":"Math Notes"},{"location":"Math-Notes.html#tests-for-convergence","text":"If $\\sum u_n = S \\in \\mathbb{R}$ then $\\lim_{n \\to \\infty} = 0$. If $\\lim_{n \\to \\infty} \\ne 0$ then $\\sum u_n$ diverges. Note that this is a necessary but insufficient condition for convergence. Absolute Convergence If $\\sum u_n $ converges and $\\sum u_n$ converges Conditional Convergence If $\\sum u_n $ diverges but $\\sum u_n$ converges Integral Test Conditional Convergence If $\\sum u_n $ diverges but $\\sum u_n$ converges","title":"Tests for convergence"},{"location":"Matrix-Cheat-Sheet.html","text":"Matrix Cheat Sheet Hermitian Matrix $$ \\begin{array}{c} A \\in \\text{Hermitian} \\ n \\in \\mathbb{Z^+}, \\ A = \\mathbb{C}^{n,n} \\ A = \\overline{ (A^T) } = A^H = A^* = A^{\\dagger} \\end{array} $$ $ A, B \\in \\text{Hermitian} $: $ A^ A = A A^ $ $ A_{j,j} \\in \\mathbb{R} $ $ A = U^* D U $, with $U$ unitary, $D$ diagonal $A B = B A \\to A B \\in \\mathbb{H}$ $A B A \\in \\mathbb{H} $ $ \\det(A) \\in \\mathbb{R} $ Hadamard Matrix $$ \\begin{array}{c} A \\in \\text{Hadamard} \\ n \\in \\mathbb{Z^+}, \\ A = {-1,1}^{n,n} \\ x = A_{i,\\cdot}, y = A_{j,\\cdot} (i \\ne j) \\ x \\cdot y^T = 0 \\end{array} $$ $ A \\in \\text{Hadamard} $: $ A A^T = n I_n $ $ \\det(A) = \\pm n^{\\frac{n}{2}} $ $ M \\in \\mathbb{C}^{n,n}, |M_{i,j}| \\le 1 \\to | \\det(M) | \\le n^{\\frac{n}{2}} $ Toeplitz Matrix $$ \\begin{array}{c} A \\in \\mathbb{C}^{n,n} \\ A_{ (i+1)%n, (j+1)%n } = A_{i,j} = a_{i-j} \\end{array} $$ Unitary Matrix $$ \\begin{array}{c} U \\in \\text{Unitary} \\ n \\in \\mathbb{Z^+}, \\ U \\in \\mathbb{C}^{n,n} \\ U U^ = U^ U = I \\end{array} $$ $ U \\in \\text{Unitary}$: $ \\det(U) = 1 $ $ U = V D V^* $, where $V \\in \\text{Unitary}$ and $D$ is diagonal and unitary $ H \\in text{Hermitian} \\to U = e^{i H} $ $ x,y \\in \\mathbb{C}^{n}, x y^T = (U x) \\cdot (U y)^T $ That is, unitary matricies are complex multidimensional analogues of rotations.","title":"Matrix Cheat Sheet"},{"location":"Matrix-Cheat-Sheet.html#matrix-cheat-sheet","text":"","title":"Matrix Cheat Sheet"},{"location":"Matrix-Cheat-Sheet.html#hermitian-matrix","text":"$$ \\begin{array}{c} A \\in \\text{Hermitian} \\ n \\in \\mathbb{Z^+}, \\ A = \\mathbb{C}^{n,n} \\ A = \\overline{ (A^T) } = A^H = A^* = A^{\\dagger} \\end{array} $$ $ A, B \\in \\text{Hermitian} $: $ A^ A = A A^ $ $ A_{j,j} \\in \\mathbb{R} $ $ A = U^* D U $, with $U$ unitary, $D$ diagonal $A B = B A \\to A B \\in \\mathbb{H}$ $A B A \\in \\mathbb{H} $ $ \\det(A) \\in \\mathbb{R} $","title":"Hermitian Matrix"},{"location":"Matrix-Cheat-Sheet.html#hadamard-matrix","text":"$$ \\begin{array}{c} A \\in \\text{Hadamard} \\ n \\in \\mathbb{Z^+}, \\ A = {-1,1}^{n,n} \\ x = A_{i,\\cdot}, y = A_{j,\\cdot} (i \\ne j) \\ x \\cdot y^T = 0 \\end{array} $$ $ A \\in \\text{Hadamard} $: $ A A^T = n I_n $ $ \\det(A) = \\pm n^{\\frac{n}{2}} $ $ M \\in \\mathbb{C}^{n,n}, |M_{i,j}| \\le 1 \\to | \\det(M) | \\le n^{\\frac{n}{2}} $","title":"Hadamard Matrix"},{"location":"Matrix-Cheat-Sheet.html#toeplitz-matrix","text":"$$ \\begin{array}{c} A \\in \\mathbb{C}^{n,n} \\ A_{ (i+1)%n, (j+1)%n } = A_{i,j} = a_{i-j} \\end{array} $$","title":"Toeplitz Matrix"},{"location":"Matrix-Cheat-Sheet.html#unitary-matrix","text":"$$ \\begin{array}{c} U \\in \\text{Unitary} \\ n \\in \\mathbb{Z^+}, \\ U \\in \\mathbb{C}^{n,n} \\ U U^ = U^ U = I \\end{array} $$ $ U \\in \\text{Unitary}$: $ \\det(U) = 1 $ $ U = V D V^* $, where $V \\in \\text{Unitary}$ and $D$ is diagonal and unitary $ H \\in text{Hermitian} \\to U = e^{i H} $ $ x,y \\in \\mathbb{C}^{n}, x y^T = (U x) \\cdot (U y)^T $ That is, unitary matricies are complex multidimensional analogues of rotations.","title":"Unitary Matrix"},{"location":"Metropolis-Hastings.html","text":"Metropolis-Hastings Based off of Gregory Gundersen's blog . We want to sample from a target distribution $\\pi(x)$.","title":"Metropolis Hastings"},{"location":"Metropolis-Hastings.html#metropolis-hastings","text":"Based off of Gregory Gundersen's blog . We want to sample from a target distribution $\\pi(x)$.","title":"Metropolis-Hastings"},{"location":"Misc-Math.html","text":"Misc. Math Landau Notation $$ f(x) = O(g(x)) \\iff \\limsup_{x \\to \\infty} \\frac{f(x)}{g(x)} < \\infty $$ $$ f(x) = o(g(x)) \\iff \\lim_{x \\to \\infty} \\frac{f(x)}{g(x)} = 0 $$ $$ f(x) = \\Omega(g(x)) \\iff \\liminf_{x \\to \\infty} \\frac{f(x)}{g(x)} > 0 $$ $$ f(x) = \\omega(g(x)) \\iff \\liminf_{x \\to \\infty} \\frac{f(x)}{g(x)} = \\infty $$ $$ f(x) = \\Theta(g(x)) \\iff f(x) = O(g(x)) \\ \\& \\ f(x) = \\Omega(g(x)) $$ $e$ is Irrational Proof by contradiction. Assume $e$ is rational. This means $e = \\frac{p}{v}$. We use the identity: $$ e = \\sum_{k=0}^{\\infty} \\frac{1}{k!} $$ With the usual convention that $0!=1$. $$ \\begin{array}{lcl} & e & = \\frac{p}{v} \\\\ & & = \\sum_{k=0}^{v} \\frac{1}{k!} + \\sum_{k=v+1}^{\\infty} \\frac{1}{k!} \\\\ \\to & (v!) e & = (v-1)! p \\\\ & & = \\sum_{k=0}^{v} \\frac{v!}{k!} + ( \\frac{1}{v+1} + \\frac{1}{(v+1)(v+2)} + \\cdots ) \\end{array} $$ By assumption, $(v!) e = (v-1)! p \\in \\mathbb{Z}$. We also have the first sum $\\sum_{k=0}^{v} \\frac{v!}{k!} \\in \\mathbb{Z}$. The second sum is greater than 0 and we can get bounds: $$ \\begin{array}{lrcl} & 0 & < \\frac{1}{v+1} + \\frac{1}{(v+1)(v+2)} + \\cdots & \\le \\frac{1}{v+1} \\sum_{k=0}^{\\infty} \\frac{1}{(v+1)^k} \\\\ \\to & 0 & < \\frac{1}{v+1} + \\frac{1}{(v+1)(v+2)} + \\cdots & \\le \\frac{1}{v} \\end{array} $$ Choose $v > 1$ and we have the second sum as non-integral, contradicting the assumption of rationality. Balls and Urns Balls and urns counting (from src ): Balls Urns unrestricted max(1) min(1) l l $u^b$ $(u)_b$ $u! { b \\brace u }$ u l $\\left({u \\choose b }\\right)$ $u \\choose b$ $\\left({u \\choose b-u }\\right)$ l u $\\sum_{i=0}^u { b \\brace i }$ $[ b \\le u ]$ ${ b \\brace u }$ u u $\\sum_{i=1}^u p_i(b) $ $[ b \\le u ]$ $p_u(b)$ l = labelled u = unlabelled $ (u)_b = \\prod_{k=0}^{b-1} (u-k) $ ${ b \\brace u}$ - Stirling's number of the second kind (${ n \\brace k } = S(n,k) = k S(n-1,k) + S(n-1,k-1)$) $ \\left({u \\choose b }\\right) = { n - (k-1) \\choose k }$, \"with replacement\" $ [ b \\le u ]$ = indicator function $ p_k(n) = p_{k-1}(n-1) + p_k(n-k) $, number of ways to partition $n$ into $k$ parts ($n$ as sum of $k$ integers) ll* Keep throwing balls without care about occupancy, you get $u^b$. ll? $u$ choices for the first ball, $(u-1)$ choices for the second ball, etc, until $(u)_b$. ll+ One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. For example, $S(3,2) = { 3 \\brace 2 } = | \\{ \\{ 1 \\} \\cup \\{2,3\\}, \\{2\\} \\cup \\{1,3\\}, \\{3\\} \\cup \\{1,2\\} \\} | $. The minimum 1 ball per urn, mans that we have a minimum of $u$ subsets. By definition (?) the urns act as sets themselves, so the order of balls within urns doesn't matter. We now want to partition $b$ balls into $u$ sets, which is ${ b \\brace u }$. Since the urns are labelled, we need to multiply by an extra $u!$ factor, which gives: $$ u! { b \\brace u } $$ ul* A stars and bars argument gives the result. Consider $u$ urns, represented by $u+1$ 'bars' and $b$ balls represented as stars. For example ||***|*|||**|*|...|| . There is a constraint that the ends have bars, so there are $u-1 + b$ symbols in the middle. Choosing all configurations gives ${ u-1+b \\choose b }$ configurations, which is $\\left({u \\choose b }\\right)$. ul? Since balls are unlabeled, we can divide out by the different labelled configurations. That is, the ll? case divided by $b!$: $$ \\frac{(u)_b}{b!} = { u \\choose b } $$ ul+ This reduces to the unrestricted case ( ul* ) with first laying down a single ball in each bin. After the initial outlay, we have $(b-u)$ balls left to put in the ul case, giving $\\left({u \\choose b-u }\\right)$. lu* One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. For the unrestricted case, we can think of trying to put $b$ balls into some number of (unlabelled) urns running from $1$ to $u$. That is, for each $i$ from $1$ to $u$, find the number of ways of putting $b$ labels into $i$ subsets: $$ \\sum_{i=1}^u { b \\brace i } $$ lu? If there are fewer balls than urns, there is only one way to place the balls in unlabeled urns. lu+ One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. The minimum 1 ball per urn, mans that we have a minimum of $u$ subsets. By definition (?) the urns act as sets themselves, so the order of balls within urns doesn't matter. We now want to partition $b$ balls into $u$ (unlabelled) sets, which is: $$ { b \\brace u } $$ uu* Much like the uu+ case, since the balls and urns are unlabelled, we can think of it as how many ways can the integer $b$ be written as the sum of some number of integers. Since we have no restriction on the minimum number of balls in urns, we have to sum all the different ways to partition, giving: $$ \\sum_{i=1}^u p_i(b) $$ uu? If there are fewer balls than urns, there is only one way to place the balls, labelled or no, in unlabeled urns. uu+ Since both balls and urns are unlabelled, this is equivalent to asking how many ways can the integer $b$ be written as the sum of $u$ integers (partitions), which is $p_u(b)$. 2021-08-30","title":"Misc Math"},{"location":"Misc-Math.html#misc-math","text":"","title":"Misc. Math"},{"location":"Misc-Math.html#landau-notation","text":"$$ f(x) = O(g(x)) \\iff \\limsup_{x \\to \\infty} \\frac{f(x)}{g(x)} < \\infty $$ $$ f(x) = o(g(x)) \\iff \\lim_{x \\to \\infty} \\frac{f(x)}{g(x)} = 0 $$ $$ f(x) = \\Omega(g(x)) \\iff \\liminf_{x \\to \\infty} \\frac{f(x)}{g(x)} > 0 $$ $$ f(x) = \\omega(g(x)) \\iff \\liminf_{x \\to \\infty} \\frac{f(x)}{g(x)} = \\infty $$ $$ f(x) = \\Theta(g(x)) \\iff f(x) = O(g(x)) \\ \\& \\ f(x) = \\Omega(g(x)) $$","title":"Landau Notation"},{"location":"Misc-Math.html#e-is-irrational","text":"Proof by contradiction. Assume $e$ is rational. This means $e = \\frac{p}{v}$. We use the identity: $$ e = \\sum_{k=0}^{\\infty} \\frac{1}{k!} $$ With the usual convention that $0!=1$. $$ \\begin{array}{lcl} & e & = \\frac{p}{v} \\\\ & & = \\sum_{k=0}^{v} \\frac{1}{k!} + \\sum_{k=v+1}^{\\infty} \\frac{1}{k!} \\\\ \\to & (v!) e & = (v-1)! p \\\\ & & = \\sum_{k=0}^{v} \\frac{v!}{k!} + ( \\frac{1}{v+1} + \\frac{1}{(v+1)(v+2)} + \\cdots ) \\end{array} $$ By assumption, $(v!) e = (v-1)! p \\in \\mathbb{Z}$. We also have the first sum $\\sum_{k=0}^{v} \\frac{v!}{k!} \\in \\mathbb{Z}$. The second sum is greater than 0 and we can get bounds: $$ \\begin{array}{lrcl} & 0 & < \\frac{1}{v+1} + \\frac{1}{(v+1)(v+2)} + \\cdots & \\le \\frac{1}{v+1} \\sum_{k=0}^{\\infty} \\frac{1}{(v+1)^k} \\\\ \\to & 0 & < \\frac{1}{v+1} + \\frac{1}{(v+1)(v+2)} + \\cdots & \\le \\frac{1}{v} \\end{array} $$ Choose $v > 1$ and we have the second sum as non-integral, contradicting the assumption of rationality.","title":"$e$ is Irrational"},{"location":"Misc-Math.html#balls-and-urns","text":"Balls and urns counting (from src ): Balls Urns unrestricted max(1) min(1) l l $u^b$ $(u)_b$ $u! { b \\brace u }$ u l $\\left({u \\choose b }\\right)$ $u \\choose b$ $\\left({u \\choose b-u }\\right)$ l u $\\sum_{i=0}^u { b \\brace i }$ $[ b \\le u ]$ ${ b \\brace u }$ u u $\\sum_{i=1}^u p_i(b) $ $[ b \\le u ]$ $p_u(b)$ l = labelled u = unlabelled $ (u)_b = \\prod_{k=0}^{b-1} (u-k) $ ${ b \\brace u}$ - Stirling's number of the second kind (${ n \\brace k } = S(n,k) = k S(n-1,k) + S(n-1,k-1)$) $ \\left({u \\choose b }\\right) = { n - (k-1) \\choose k }$, \"with replacement\" $ [ b \\le u ]$ = indicator function $ p_k(n) = p_{k-1}(n-1) + p_k(n-k) $, number of ways to partition $n$ into $k$ parts ($n$ as sum of $k$ integers)","title":"Balls and Urns"},{"location":"Misc-Math.html#ll","text":"Keep throwing balls without care about occupancy, you get $u^b$.","title":"ll*"},{"location":"Misc-Math.html#ll_1","text":"$u$ choices for the first ball, $(u-1)$ choices for the second ball, etc, until $(u)_b$.","title":"ll?"},{"location":"Misc-Math.html#ll_2","text":"One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. For example, $S(3,2) = { 3 \\brace 2 } = | \\{ \\{ 1 \\} \\cup \\{2,3\\}, \\{2\\} \\cup \\{1,3\\}, \\{3\\} \\cup \\{1,2\\} \\} | $. The minimum 1 ball per urn, mans that we have a minimum of $u$ subsets. By definition (?) the urns act as sets themselves, so the order of balls within urns doesn't matter. We now want to partition $b$ balls into $u$ sets, which is ${ b \\brace u }$. Since the urns are labelled, we need to multiply by an extra $u!$ factor, which gives: $$ u! { b \\brace u } $$","title":"ll+"},{"location":"Misc-Math.html#ul","text":"A stars and bars argument gives the result. Consider $u$ urns, represented by $u+1$ 'bars' and $b$ balls represented as stars. For example ||***|*|||**|*|...|| . There is a constraint that the ends have bars, so there are $u-1 + b$ symbols in the middle. Choosing all configurations gives ${ u-1+b \\choose b }$ configurations, which is $\\left({u \\choose b }\\right)$.","title":"ul*"},{"location":"Misc-Math.html#ul_1","text":"Since balls are unlabeled, we can divide out by the different labelled configurations. That is, the ll? case divided by $b!$: $$ \\frac{(u)_b}{b!} = { u \\choose b } $$","title":"ul?"},{"location":"Misc-Math.html#ul_2","text":"This reduces to the unrestricted case ( ul* ) with first laying down a single ball in each bin. After the initial outlay, we have $(b-u)$ balls left to put in the ul case, giving $\\left({u \\choose b-u }\\right)$.","title":"ul+"},{"location":"Misc-Math.html#lu","text":"One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. For the unrestricted case, we can think of trying to put $b$ balls into some number of (unlabelled) urns running from $1$ to $u$. That is, for each $i$ from $1$ to $u$, find the number of ways of putting $b$ labels into $i$ subsets: $$ \\sum_{i=1}^u { b \\brace i } $$","title":"lu*"},{"location":"Misc-Math.html#lu_1","text":"If there are fewer balls than urns, there is only one way to place the balls in unlabeled urns.","title":"lu?"},{"location":"Misc-Math.html#lu_2","text":"One interpretation of Stirling numbers of the second kind ${ n \\brace k } = S(n,k)$ are the way of partitioning $n$ numbers into $k$ subsets. The minimum 1 ball per urn, mans that we have a minimum of $u$ subsets. By definition (?) the urns act as sets themselves, so the order of balls within urns doesn't matter. We now want to partition $b$ balls into $u$ (unlabelled) sets, which is: $$ { b \\brace u } $$","title":"lu+"},{"location":"Misc-Math.html#uu","text":"Much like the uu+ case, since the balls and urns are unlabelled, we can think of it as how many ways can the integer $b$ be written as the sum of some number of integers. Since we have no restriction on the minimum number of balls in urns, we have to sum all the different ways to partition, giving: $$ \\sum_{i=1}^u p_i(b) $$","title":"uu*"},{"location":"Misc-Math.html#uu_1","text":"If there are fewer balls than urns, there is only one way to place the balls, labelled or no, in unlabeled urns.","title":"uu?"},{"location":"Misc-Math.html#uu_2","text":"Since both balls and urns are unlabelled, this is equivalent to asking how many ways can the integer $b$ be written as the sum of $u$ integers (partitions), which is $p_u(b)$.","title":"uu+"},{"location":"Misc-Math.html#2021-08-30","text":"","title":"2021-08-30"},{"location":"MkDocs-Quickstart.html","text":"MkDocs Static Site - Quickstart sudo pip install mkdocs mkdocs new $site cd $site Assume the following all work under $site . Create the directory structure: mkdir dev_theme dev_theme/css dev_theme/js wget https://raw.githubusercontent.com/abetusk/minimal/master/stylesheets/pygment_trac.css -O dev_theme/css/pygment_trac.css wget https://raw.githubusercontent.com/abetusk/minimal/master/stylesheets/styles.css -O dev_theme/css/styles.css wget https://raw.githubusercontent.com/abetusk/minimal/master/javascripts/scale.fix.js -O dev_theme/js/scale.fix.js mkdocs.yml using the minimal theme : site_name: dev pages: - index.md - \"Textile-Cheat-Sheet.md\" - \"Image-Resize.md\" - \"Screenshots-Screencasts-Animated-Gifs.md\" - \"ffmpeg-notes.md\" - \"Unix-y-notes.md\" - \"lattice-reduction.md\" - \"GCode-Conversion.md\" - \"Git-Rename-Master.md\" - \"MkDocs-Quickstart.md\" docs_dir: 'wiki' theme_dir: 'dev_theme' extra: base : \"/dev/\" MkDocs uses jinja2 for templates. To test (note, you need to change config.extra.base for this setup): mkdocs serve To build the static site in the site subdir: mkdocs build 2016-09-28","title":"MkDocs Quickstart"},{"location":"MkDocs-Quickstart.html#mkdocs-static-site-quickstart","text":"sudo pip install mkdocs mkdocs new $site cd $site Assume the following all work under $site . Create the directory structure: mkdir dev_theme dev_theme/css dev_theme/js wget https://raw.githubusercontent.com/abetusk/minimal/master/stylesheets/pygment_trac.css -O dev_theme/css/pygment_trac.css wget https://raw.githubusercontent.com/abetusk/minimal/master/stylesheets/styles.css -O dev_theme/css/styles.css wget https://raw.githubusercontent.com/abetusk/minimal/master/javascripts/scale.fix.js -O dev_theme/js/scale.fix.js mkdocs.yml using the minimal theme : site_name: dev pages: - index.md - \"Textile-Cheat-Sheet.md\" - \"Image-Resize.md\" - \"Screenshots-Screencasts-Animated-Gifs.md\" - \"ffmpeg-notes.md\" - \"Unix-y-notes.md\" - \"lattice-reduction.md\" - \"GCode-Conversion.md\" - \"Git-Rename-Master.md\" - \"MkDocs-Quickstart.md\" docs_dir: 'wiki' theme_dir: 'dev_theme' extra: base : \"/dev/\" MkDocs uses jinja2 for templates. To test (note, you need to change config.extra.base for this setup): mkdocs serve To build the static site in the site subdir: mkdocs build","title":"MkDocs Static Site - Quickstart"},{"location":"MkDocs-Quickstart.html#2016-09-28","text":"","title":"2016-09-28"},{"location":"My-Favorite-Puzzles.html","text":"My Favorite Logic Puzzles Solutions at the bottom. Monty Hall A game is played in the following order: The game host shows 3 doors, one of which has a prize behind it while the others do not The host secretly places the prize at random behind 1 of the 3 doors You pick a door After you pick a door, the host opens one door other than the one you picked to show no prize behind it The host then asks you whether you would like to switch or stay Note: The prize's location stays fixed initial placement. Q: What is the tactic (to switch or stay) that maximized probability of winning the prize? Alternate version With probability $\\frac{1}{2}$, a red or green ball is secretly placed in a bag A red ball is then placed in the bag A ball is chosen at random from the two balls in the bag and is discovered to be red Q: What is the probability that the other ball in the bag is red? Truthsayers and Liars You come to a fork in the road where one path leads to riches and the other to doom. A person stands at the crossroads who either always tells the truth or always lies. Q: What is one question that can be asked to make sure you go on the path to the prize? Prisoner's in a Line $N$ prisoners are placed in a line, all facing front A red or green hat is placed on each prisoner's head Prisoner at position $p$ ($0 \\le p < N$) can see all prisoners at positions $p+1$ to $N-1$ (that is, they can see all prisoners in front of them) Prisoner at position $p$ cannot see prisoners hat at positions $0$ to $p$ (that is, they cannot see their own hat or any prisoner's hat behind them) The warden starts at position $0$ and asks what the prisoner's hat is When asked about their hat color, prisoners can only respond \"red\" or \"green\" All prisoners in front can hear answers to the hat questions from prisoner's behind them Prisoners who guess their hat color correctly are freed while an incorrect guess leads to life imprisonment Note: prisoners are allowed to collude on a solution before the game starts but the warden is assumed to be listening in and can change the prisoner order or hat color placement depending on the prisoner's strategy. Q: What tactic should be employed by the prisoners to save the maximum number of people? Dominoes on a Mutilated Chessboard A chessboard has 64 squares on it (8 rows by 8 columns). A domino takes up 2 squares on a chessboard. If two squares are removed from the chessboard, one from the upper left corner and the other from the lower right corner, Can 31 dominoes be put on a chessboard to completely cover it? Green Eyed Islanders and the Outsider $N$ people on an island $0 < M \\le N$ have green eyes (that is, at least one islander has green eyes) while all others have red eyes Initially, no islander knows their own eye color If an islander figures out their eye color anytime during the day, they leave the island that night One morning, an outsider comes in and announces \"At least one of you has green eyes\" Note: islanders consider it rude to tell each other their eye color and do not look in mirrors. Q: How many islanders are left on the island after $N$ days? Numbered Boxes $N$ people have a unique number from $0$ to $(N-1)$ assigned to them $N$ boxes are labeled uniquely from $0$ to $(N-1)$ $N$ slips of paper have a unique number from $0$ to $(N-1)$ written on them and placed in a box, one slip of paper per box, randomly Each of the $N$ people is brought in and allowed to look at $\\lfloor N/2 \\rfloor$ boxes Once the game starts, no communication is allowed between players Q: What strategy maximizes each player finding their own number in a box? Drunk Passengers on a Plane There are $N$ seats on a plane There are $N$ passengers, each uniquely assigned one of the $N$ seats Each passenger will take their assigned seat if available, otherwise they will pick a seat at random Each passenger boards one by one The first passenger is drunk and sits in the incorrect seat Q: What is the probability that the last passenger will sit in their assigned seat? Duplicates in a List An array of length $N+1$ Elements are only drawn from $[1,N]$ Note: there is at least one duplicate entry in the array. Q: In $O(\\lg N)$ space and $O(N)$ time, find any duplicated entry Cat in a Box $N$ adjacent rooms (that is, an array of rooms where all but the ends rooms have access to the left and right room next to them) A cat exists in a room, initially placed at random Each round, the cat moves to an adjacent room and does not stay in a room that it is currently in Each round, you are allowed to open the door to a room Q: What is the strategy to maximize the chance of finding the cat? Prisoners and the Coin $N$ prisoners are separated and not allowed to communicate At the warden's prerogative, a prisoner is brought in and fed While the prisoner is eating, the prisoner is allowed to observe and flip a coin, if they so choose The coin is initially in a random state at the warden's whim No prisoner is allowed to starve At any point, any prisoner is allowed to say \"We've all been here\", at which point they will all be let go if correct or sentenced to life without parole if wrong Note: prisoners are allowed to collude on a solution before the game starts. Q: What strategy can the prisoners employ to maximize the chances for their freedom? The Devil and Coins on a Chessboard An $8x8$ chessboard has $64$ coins placed in each square The Devil chooses the initial configuration of coins You are brought and the following chain of events take place: The Devil points to one position on the chessboard You must flip any one coin, in any position on the chessboard You are escorted out Your partner is escorted in to observe the chessboard Your partner guesses which position the Devil pointed to Note: you and your partner are allowed to collude beforehand but no communication is allowed while playing. The Devil is assumed to be listening in while you and your partner collude. Q: What strategy can be employed to maximize the chances of your partner guessing the pointed to position correctly? Solutions Monty Hall Switching gives a $\\frac{2}{3}$ chance of winning whereas staying gives $\\frac{1}{3}$. One way to see this is to play with $N$ doors. After the initial pick by the player, the game show host will reveal $(N-2)$ doors, in effect giving you $(N-1)$ door reveals if you switch but only $1$ if you stay. From personal experience, only when people experience the loss do they recant on their initial $\\frac{1}{2}$ estimate. Here is a program to convince yourself should you be adamant about staying as a solution: #!/usr/bin/python3 import re,sys,random N = 100 total_wins=0 total_losses=0 while True: guess=-1 prize_door = int(float(N)*random.random()) closed_door = prize_door print(\"\\n---\\nThere are\", N, \"doors.\") sys.stdout.write(\"Door guess [0-\" + str(N-1) + \"] ('q' to quit): \") sys.stdout.flush() line = sys.stdin.readline() if re.match('\\d+', line): guess = int(line.strip()) else: break if (guess<0) or (guess>=N): guess = int(float(N)*random.random()) if guess == prize_door: closed_door = int(float(N)*random.random()) if closed_door >= guess: closed_door = (closed_door + 1) % N open_door = [] for idx in range(N): if (idx!=guess) and (idx!=closed_door): open_door.append(str(idx)) print(\"You picked\", guess) print(\"Doors\", \",\".join(open_door), \"are opened.\") print(\"Doors\", guess, closed_door, \"remain closed\") sys.stdout.write(\"Would you like to switch [y/n]? \") sys.stdout.flush() line = sys.stdin.readline() if (len(line) > 0) and ((line[0] =='y') or line[0] == 'Y'): guess = closed_door print(\"Your door is now\", guess) if guess == prize_door: total_wins+=1 print(\"Congratulations! You won!\") else: total_losses+=1 print(\"Sorry, the prize was behind door\", prize_door, \"(you guessed\", guess,\")\") print(\"\\nYour total wins are now:\", total_wins, \"and total losses are:\", total_losses) Truthsayers and Liars If I were to ask you which path is the road to the prize, what would you say? Prisoner's in a Line $(N-1)$ can be saved by having prisoner $0$ call out the parity of hat colors (suitably encoded as \"red\" or \"green\") of the $(N-1)$ prisoners in front of them. Each subsequent prisoner will have full information to guess their own hat color correctly. Dominoes on a Mutilated Chessboard No, by a parity argument. The 62 squares now have 30 white squares and 32 black squares. Since a domino must cover both a white and black square, a domino must cover two black squares, which is an impossibility. Green Eyed Islanders and the Outsider A proof by induction will show that no islanders will be left. For $M$ green eyed islanders, from the perspective of a red eyed islander, they should all leave after $M$ nights. If the green eyed islanders leave after $M$ nights, that means $N-M$ red eyed islanders remain, forcing them to leave the subsequent night. For $M$ green eyed islanders, from the perspective of a green eyed islander, if they don't leave after $(M-1)$ nights, that means they must have green eyes, forcing them to leave on the $M$'th night, then causing the red eyed islander to leave on the $(M+1)$st night. Numbered Boxes Each participant opens the box with their own label. They then jump to the box of the number that is written on the piece of paper. This becomes the probability that the maximum cycle in a random permutation is less than $\\frac{N}{2}$ Drunk Passengers on a Plane One \"quick and dirty\" way is to approximate by asking what the probability is of \"drawing\" a number from a dwindling pool of seats. $$ \\prod_{k=0}^{N-1} (1 - \\frac{1}{n-k}) \\approx \\prod_{k=0}^{N-1} (1 - \\frac{1}{n}) = (1 - \\frac{1}{n})^n \\to \\frac{1}{e} $$ Giving $1-e^{-1}$. This assumes independence and a lot of hand waiving. The more rigorous analysis... Duplicates in a List hmm... Cat in a Box Start from the left most room, then increment to the right by one at each round. Wait an extra round at the right most door, then start going from right to left by an increment of 1 at each round. Starting from the left and then moving right forces the cat's position to the left of the current position into box positions of the same parity. Once the right has been reached, moving back towards the left with the cat's position forced into room positions of the same parity ensures the cat can't \"skip\" over the current position. Prisoners and the Coin A designated prisoner is chosen to flip the coin from tails to heads should they see it in a tails state and count the number of times they do so. All other prisoners will flip the coin from heads to tails a maximum of two times, doing so every chance they get. Once the designated prisoner sees the coin flip $(2N-3)$ times, they can announce that they've all been in the room. If the initial state of the coin is known, the count can be reduced to $(N-1)$. The Devil and Coins on a Chessboard Unknown 2020-04-30","title":"My Favorite Puzzles"},{"location":"My-Favorite-Puzzles.html#my-favorite-logic-puzzles","text":"Solutions at the bottom.","title":"My Favorite Logic Puzzles"},{"location":"My-Favorite-Puzzles.html#monty-hall","text":"A game is played in the following order: The game host shows 3 doors, one of which has a prize behind it while the others do not The host secretly places the prize at random behind 1 of the 3 doors You pick a door After you pick a door, the host opens one door other than the one you picked to show no prize behind it The host then asks you whether you would like to switch or stay Note: The prize's location stays fixed initial placement. Q: What is the tactic (to switch or stay) that maximized probability of winning the prize?","title":"Monty Hall"},{"location":"My-Favorite-Puzzles.html#alternate-version","text":"With probability $\\frac{1}{2}$, a red or green ball is secretly placed in a bag A red ball is then placed in the bag A ball is chosen at random from the two balls in the bag and is discovered to be red Q: What is the probability that the other ball in the bag is red?","title":"Alternate version"},{"location":"My-Favorite-Puzzles.html#truthsayers-and-liars","text":"You come to a fork in the road where one path leads to riches and the other to doom. A person stands at the crossroads who either always tells the truth or always lies. Q: What is one question that can be asked to make sure you go on the path to the prize?","title":"Truthsayers and Liars"},{"location":"My-Favorite-Puzzles.html#prisoners-in-a-line","text":"$N$ prisoners are placed in a line, all facing front A red or green hat is placed on each prisoner's head Prisoner at position $p$ ($0 \\le p < N$) can see all prisoners at positions $p+1$ to $N-1$ (that is, they can see all prisoners in front of them) Prisoner at position $p$ cannot see prisoners hat at positions $0$ to $p$ (that is, they cannot see their own hat or any prisoner's hat behind them) The warden starts at position $0$ and asks what the prisoner's hat is When asked about their hat color, prisoners can only respond \"red\" or \"green\" All prisoners in front can hear answers to the hat questions from prisoner's behind them Prisoners who guess their hat color correctly are freed while an incorrect guess leads to life imprisonment Note: prisoners are allowed to collude on a solution before the game starts but the warden is assumed to be listening in and can change the prisoner order or hat color placement depending on the prisoner's strategy. Q: What tactic should be employed by the prisoners to save the maximum number of people?","title":"Prisoner's in a Line"},{"location":"My-Favorite-Puzzles.html#dominoes-on-a-mutilated-chessboard","text":"A chessboard has 64 squares on it (8 rows by 8 columns). A domino takes up 2 squares on a chessboard. If two squares are removed from the chessboard, one from the upper left corner and the other from the lower right corner, Can 31 dominoes be put on a chessboard to completely cover it?","title":"Dominoes on a Mutilated Chessboard"},{"location":"My-Favorite-Puzzles.html#green-eyed-islanders-and-the-outsider","text":"$N$ people on an island $0 < M \\le N$ have green eyes (that is, at least one islander has green eyes) while all others have red eyes Initially, no islander knows their own eye color If an islander figures out their eye color anytime during the day, they leave the island that night One morning, an outsider comes in and announces \"At least one of you has green eyes\" Note: islanders consider it rude to tell each other their eye color and do not look in mirrors. Q: How many islanders are left on the island after $N$ days?","title":"Green Eyed Islanders and the Outsider"},{"location":"My-Favorite-Puzzles.html#numbered-boxes","text":"$N$ people have a unique number from $0$ to $(N-1)$ assigned to them $N$ boxes are labeled uniquely from $0$ to $(N-1)$ $N$ slips of paper have a unique number from $0$ to $(N-1)$ written on them and placed in a box, one slip of paper per box, randomly Each of the $N$ people is brought in and allowed to look at $\\lfloor N/2 \\rfloor$ boxes Once the game starts, no communication is allowed between players Q: What strategy maximizes each player finding their own number in a box?","title":"Numbered Boxes"},{"location":"My-Favorite-Puzzles.html#drunk-passengers-on-a-plane","text":"There are $N$ seats on a plane There are $N$ passengers, each uniquely assigned one of the $N$ seats Each passenger will take their assigned seat if available, otherwise they will pick a seat at random Each passenger boards one by one The first passenger is drunk and sits in the incorrect seat Q: What is the probability that the last passenger will sit in their assigned seat?","title":"Drunk Passengers on a Plane"},{"location":"My-Favorite-Puzzles.html#duplicates-in-a-list","text":"An array of length $N+1$ Elements are only drawn from $[1,N]$ Note: there is at least one duplicate entry in the array. Q: In $O(\\lg N)$ space and $O(N)$ time, find any duplicated entry","title":"Duplicates in a List"},{"location":"My-Favorite-Puzzles.html#cat-in-a-box","text":"$N$ adjacent rooms (that is, an array of rooms where all but the ends rooms have access to the left and right room next to them) A cat exists in a room, initially placed at random Each round, the cat moves to an adjacent room and does not stay in a room that it is currently in Each round, you are allowed to open the door to a room Q: What is the strategy to maximize the chance of finding the cat?","title":"Cat in a Box"},{"location":"My-Favorite-Puzzles.html#prisoners-and-the-coin","text":"$N$ prisoners are separated and not allowed to communicate At the warden's prerogative, a prisoner is brought in and fed While the prisoner is eating, the prisoner is allowed to observe and flip a coin, if they so choose The coin is initially in a random state at the warden's whim No prisoner is allowed to starve At any point, any prisoner is allowed to say \"We've all been here\", at which point they will all be let go if correct or sentenced to life without parole if wrong Note: prisoners are allowed to collude on a solution before the game starts. Q: What strategy can the prisoners employ to maximize the chances for their freedom?","title":"Prisoners and the Coin"},{"location":"My-Favorite-Puzzles.html#the-devil-and-coins-on-a-chessboard","text":"An $8x8$ chessboard has $64$ coins placed in each square The Devil chooses the initial configuration of coins You are brought and the following chain of events take place: The Devil points to one position on the chessboard You must flip any one coin, in any position on the chessboard You are escorted out Your partner is escorted in to observe the chessboard Your partner guesses which position the Devil pointed to Note: you and your partner are allowed to collude beforehand but no communication is allowed while playing. The Devil is assumed to be listening in while you and your partner collude. Q: What strategy can be employed to maximize the chances of your partner guessing the pointed to position correctly?","title":"The Devil and Coins on a Chessboard"},{"location":"My-Favorite-Puzzles.html#solutions","text":"","title":"Solutions"},{"location":"My-Favorite-Puzzles.html#monty-hall_1","text":"Switching gives a $\\frac{2}{3}$ chance of winning whereas staying gives $\\frac{1}{3}$. One way to see this is to play with $N$ doors. After the initial pick by the player, the game show host will reveal $(N-2)$ doors, in effect giving you $(N-1)$ door reveals if you switch but only $1$ if you stay. From personal experience, only when people experience the loss do they recant on their initial $\\frac{1}{2}$ estimate. Here is a program to convince yourself should you be adamant about staying as a solution: #!/usr/bin/python3 import re,sys,random N = 100 total_wins=0 total_losses=0 while True: guess=-1 prize_door = int(float(N)*random.random()) closed_door = prize_door print(\"\\n---\\nThere are\", N, \"doors.\") sys.stdout.write(\"Door guess [0-\" + str(N-1) + \"] ('q' to quit): \") sys.stdout.flush() line = sys.stdin.readline() if re.match('\\d+', line): guess = int(line.strip()) else: break if (guess<0) or (guess>=N): guess = int(float(N)*random.random()) if guess == prize_door: closed_door = int(float(N)*random.random()) if closed_door >= guess: closed_door = (closed_door + 1) % N open_door = [] for idx in range(N): if (idx!=guess) and (idx!=closed_door): open_door.append(str(idx)) print(\"You picked\", guess) print(\"Doors\", \",\".join(open_door), \"are opened.\") print(\"Doors\", guess, closed_door, \"remain closed\") sys.stdout.write(\"Would you like to switch [y/n]? \") sys.stdout.flush() line = sys.stdin.readline() if (len(line) > 0) and ((line[0] =='y') or line[0] == 'Y'): guess = closed_door print(\"Your door is now\", guess) if guess == prize_door: total_wins+=1 print(\"Congratulations! You won!\") else: total_losses+=1 print(\"Sorry, the prize was behind door\", prize_door, \"(you guessed\", guess,\")\") print(\"\\nYour total wins are now:\", total_wins, \"and total losses are:\", total_losses)","title":"Monty Hall"},{"location":"My-Favorite-Puzzles.html#truthsayers-and-liars_1","text":"If I were to ask you which path is the road to the prize, what would you say?","title":"Truthsayers and Liars"},{"location":"My-Favorite-Puzzles.html#prisoners-in-a-line_1","text":"$(N-1)$ can be saved by having prisoner $0$ call out the parity of hat colors (suitably encoded as \"red\" or \"green\") of the $(N-1)$ prisoners in front of them. Each subsequent prisoner will have full information to guess their own hat color correctly.","title":"Prisoner's in a Line"},{"location":"My-Favorite-Puzzles.html#dominoes-on-a-mutilated-chessboard_1","text":"No, by a parity argument. The 62 squares now have 30 white squares and 32 black squares. Since a domino must cover both a white and black square, a domino must cover two black squares, which is an impossibility.","title":"Dominoes on a Mutilated Chessboard"},{"location":"My-Favorite-Puzzles.html#green-eyed-islanders-and-the-outsider_1","text":"A proof by induction will show that no islanders will be left. For $M$ green eyed islanders, from the perspective of a red eyed islander, they should all leave after $M$ nights. If the green eyed islanders leave after $M$ nights, that means $N-M$ red eyed islanders remain, forcing them to leave the subsequent night. For $M$ green eyed islanders, from the perspective of a green eyed islander, if they don't leave after $(M-1)$ nights, that means they must have green eyes, forcing them to leave on the $M$'th night, then causing the red eyed islander to leave on the $(M+1)$st night.","title":"Green Eyed Islanders and the Outsider"},{"location":"My-Favorite-Puzzles.html#numbered-boxes_1","text":"Each participant opens the box with their own label. They then jump to the box of the number that is written on the piece of paper. This becomes the probability that the maximum cycle in a random permutation is less than $\\frac{N}{2}$","title":"Numbered Boxes"},{"location":"My-Favorite-Puzzles.html#drunk-passengers-on-a-plane_1","text":"One \"quick and dirty\" way is to approximate by asking what the probability is of \"drawing\" a number from a dwindling pool of seats. $$ \\prod_{k=0}^{N-1} (1 - \\frac{1}{n-k}) \\approx \\prod_{k=0}^{N-1} (1 - \\frac{1}{n}) = (1 - \\frac{1}{n})^n \\to \\frac{1}{e} $$ Giving $1-e^{-1}$. This assumes independence and a lot of hand waiving. The more rigorous analysis...","title":"Drunk Passengers on a Plane"},{"location":"My-Favorite-Puzzles.html#duplicates-in-a-list_1","text":"hmm...","title":"Duplicates in a List"},{"location":"My-Favorite-Puzzles.html#cat-in-a-box_1","text":"Start from the left most room, then increment to the right by one at each round. Wait an extra round at the right most door, then start going from right to left by an increment of 1 at each round. Starting from the left and then moving right forces the cat's position to the left of the current position into box positions of the same parity. Once the right has been reached, moving back towards the left with the cat's position forced into room positions of the same parity ensures the cat can't \"skip\" over the current position.","title":"Cat in a Box"},{"location":"My-Favorite-Puzzles.html#prisoners-and-the-coin_1","text":"A designated prisoner is chosen to flip the coin from tails to heads should they see it in a tails state and count the number of times they do so. All other prisoners will flip the coin from heads to tails a maximum of two times, doing so every chance they get. Once the designated prisoner sees the coin flip $(2N-3)$ times, they can announce that they've all been in the room. If the initial state of the coin is known, the count can be reduced to $(N-1)$.","title":"Prisoners and the Coin"},{"location":"My-Favorite-Puzzles.html#the-devil-and-coins-on-a-chessboard_1","text":"Unknown","title":"The Devil and Coins on a Chessboard"},{"location":"My-Favorite-Puzzles.html#2020-04-30","text":"","title":"2020-04-30"},{"location":"Networking-Cheat-Sheat.html","text":"Networking Cheat Sheet DNS dig example.com dig -t NS example.com +short # query nameserver, return short description dig -x 8.8.8.8 # reverse lookup Interfaces ip link show ip -s link show ip addr show ip route Scanning nmap -sP ... # Ping Scan - Scan network and find out which devices are up and running nmap -sL ... # List targets in network nmap -O ... # OS detection nmap -p 22,21,80,8080 -sV ... # Detect version of service running on port nmap -A ... # OS detection, version detection, script scanning, and traceroute nmap -p 192.168.0.1 # Also '-p \"*\"' works for scanning all ports ( src )","title":"Networking Cheat Sheat"},{"location":"Networking-Cheat-Sheat.html#networking-cheat-sheet","text":"","title":"Networking Cheat Sheet"},{"location":"Networking-Cheat-Sheat.html#dns","text":"dig example.com dig -t NS example.com +short # query nameserver, return short description dig -x 8.8.8.8 # reverse lookup","title":"DNS"},{"location":"Networking-Cheat-Sheat.html#interfaces","text":"ip link show ip -s link show ip addr show ip route","title":"Interfaces"},{"location":"Networking-Cheat-Sheat.html#scanning","text":"nmap -sP ... # Ping Scan - Scan network and find out which devices are up and running nmap -sL ... # List targets in network nmap -O ... # OS detection nmap -p 22,21,80,8080 -sV ... # Detect version of service running on port nmap -A ... # OS detection, version detection, script scanning, and traceroute nmap -p 192.168.0.1 # Also '-p \"*\"' works for scanning all ports ( src )","title":"Scanning"},{"location":"Neural-Network-Notes.html","text":"Neural Network Notes $$ \\begin{align} x \\in \\mathbb{R} ^ n & - \\text{ input } \\ y \\in \\mathbb{R} ^ m & - \\text{ output (training) data } \\ C & - \\text{ loss function } \\ L & - \\text{ number of layers } \\ W ^ l = ( w ^ l _ { j, k } ) & - \\text{ weight } \\ f _ l & - \\text{ activation function } \\ z _ l & - \\text{ weighted input at level } l \\ a _ l & - \\text{ activation output at level } l \\ \\end{align} $$ Some common activation functions: $$ \\begin{align} \\text{ReLU}(x) & = \\max(0, x) \\ \\sigma(x) &= \\frac{1}{1 + e ^ { - \\beta \\cdot x } } \\ s(x) &= \\frac{1}{\\beta} \\ln(1 + e ^ { \\beta \\cdot x } ) \\ \\text{softmax}( \\vec{ x } ) &= [ \\frac{e ^ {x _ j}}{ \\sum ^ {n-1} _ {j=0} e ^ { x _ j } } ] \\end{align} $$ Where $\\beta$ controls the width or 'ramp up' region for ReLU and the sigmoid function. Call $g(x) = f _ { L-1 } ( W _ { L-1 } f _ { L-2 } ( W _ { L-2} \\cdots f _ 0 ( W _ 0 x ) \\cdots ) )$, then we want to minimize: $$ C(y, g(x)) $$ Backpropagation proceeds by updating weights based on gradient descent: $$ \\frac{ d C }{ d a _ { L-1 }} \\circ \\frac{ d a _ { L-1 } }{ d z _ { L-1 } } \\circ \\frac{ d z _ { L-1 } }{ d a _ { L-2 } } \\circ \\frac{ d a _ { L-2 } }{ d z _ { L-2 } } \\circ \\cdots \\circ \\frac{ d a _ 0 }{ d z _ 0 } \\cdot \\frac{ d z _ 0 } { d x } $$ Where $\\circ$ is the Hadamard product. TO BE CONTINUED... Autoencoders $$ \\begin{align} \\phi & : X \\to F \\ \\psi & : F \\to X \\ \\phi, \\psi & = \\text{argmin} _ { \\phi, \\psi } | X - ( \\psi \\circ \\phi ) X |^2 \\end{align} $$ $$ \\begin{align} z &= \\sigma( W x + b ) \\ x &= \\sigma'( W' z + b' ) \\ L(x,x') & = \\text{ loss function } \\ & = |x - \\sigma'(W'(\\sigma(Wx + b)) + b') |^2 \\ \\end{align} $$ Some common loss functions: $$ \\begin{align} L _ {MSE} (x,\\bar{x}) & = \\frac{1}{M} \\sum ^ {M-1} _ {i=0} | x_i - \\bar{x} _ i |^2 \\ L _ {CE} (x,\\bar{x}) & = - \\frac{1}{M} \\sum ^ {M-1} _ {i=0} \\sum ^ { N-1 } _ {j=0} [ x _ {j,i} \\ln( \\bar{x} _ {j,i} ) + (1-x _ {j,i}) \\ln( 1 - \\bar{x} _ {j,i} ) ] \\ \\end{align} $$ Where $F$ is called the \"latent space\". Regularization is a method to make sure answers meet some sparsity condition. Here are two: $$ \\begin{align} \\ell _ 2 \\text{ reg } & : L(x,\\bar{x}) + \\lambda \\sum _ i \\theta ^ 2 _ i \\ \\ell _ 1 \\text{ reg } & : L(x,\\bar{x}) + \\lambda \\sum _ i |a ^ h _ i | \\ \\end{align} $$ Where $\\theta$ are the presumably the weights and $a ^ h _ i$ are the activations of position $i$ at level $h$. References 0 1 2 3 2023-03-18","title":"Neural Network Notes"},{"location":"Neural-Network-Notes.html#neural-network-notes","text":"$$ \\begin{align} x \\in \\mathbb{R} ^ n & - \\text{ input } \\ y \\in \\mathbb{R} ^ m & - \\text{ output (training) data } \\ C & - \\text{ loss function } \\ L & - \\text{ number of layers } \\ W ^ l = ( w ^ l _ { j, k } ) & - \\text{ weight } \\ f _ l & - \\text{ activation function } \\ z _ l & - \\text{ weighted input at level } l \\ a _ l & - \\text{ activation output at level } l \\ \\end{align} $$ Some common activation functions: $$ \\begin{align} \\text{ReLU}(x) & = \\max(0, x) \\ \\sigma(x) &= \\frac{1}{1 + e ^ { - \\beta \\cdot x } } \\ s(x) &= \\frac{1}{\\beta} \\ln(1 + e ^ { \\beta \\cdot x } ) \\ \\text{softmax}( \\vec{ x } ) &= [ \\frac{e ^ {x _ j}}{ \\sum ^ {n-1} _ {j=0} e ^ { x _ j } } ] \\end{align} $$ Where $\\beta$ controls the width or 'ramp up' region for ReLU and the sigmoid function. Call $g(x) = f _ { L-1 } ( W _ { L-1 } f _ { L-2 } ( W _ { L-2} \\cdots f _ 0 ( W _ 0 x ) \\cdots ) )$, then we want to minimize: $$ C(y, g(x)) $$ Backpropagation proceeds by updating weights based on gradient descent: $$ \\frac{ d C }{ d a _ { L-1 }} \\circ \\frac{ d a _ { L-1 } }{ d z _ { L-1 } } \\circ \\frac{ d z _ { L-1 } }{ d a _ { L-2 } } \\circ \\frac{ d a _ { L-2 } }{ d z _ { L-2 } } \\circ \\cdots \\circ \\frac{ d a _ 0 }{ d z _ 0 } \\cdot \\frac{ d z _ 0 } { d x } $$ Where $\\circ$ is the Hadamard product. TO BE CONTINUED...","title":"Neural Network Notes"},{"location":"Neural-Network-Notes.html#autoencoders","text":"$$ \\begin{align} \\phi & : X \\to F \\ \\psi & : F \\to X \\ \\phi, \\psi & = \\text{argmin} _ { \\phi, \\psi } | X - ( \\psi \\circ \\phi ) X |^2 \\end{align} $$ $$ \\begin{align} z &= \\sigma( W x + b ) \\ x &= \\sigma'( W' z + b' ) \\ L(x,x') & = \\text{ loss function } \\ & = |x - \\sigma'(W'(\\sigma(Wx + b)) + b') |^2 \\ \\end{align} $$ Some common loss functions: $$ \\begin{align} L _ {MSE} (x,\\bar{x}) & = \\frac{1}{M} \\sum ^ {M-1} _ {i=0} | x_i - \\bar{x} _ i |^2 \\ L _ {CE} (x,\\bar{x}) & = - \\frac{1}{M} \\sum ^ {M-1} _ {i=0} \\sum ^ { N-1 } _ {j=0} [ x _ {j,i} \\ln( \\bar{x} _ {j,i} ) + (1-x _ {j,i}) \\ln( 1 - \\bar{x} _ {j,i} ) ] \\ \\end{align} $$ Where $F$ is called the \"latent space\". Regularization is a method to make sure answers meet some sparsity condition. Here are two: $$ \\begin{align} \\ell _ 2 \\text{ reg } & : L(x,\\bar{x}) + \\lambda \\sum _ i \\theta ^ 2 _ i \\ \\ell _ 1 \\text{ reg } & : L(x,\\bar{x}) + \\lambda \\sum _ i |a ^ h _ i | \\ \\end{align} $$ Where $\\theta$ are the presumably the weights and $a ^ h _ i$ are the activations of position $i$ at level $h$.","title":"Autoencoders"},{"location":"Neural-Network-Notes.html#references","text":"0 1 2 3","title":"References"},{"location":"Neural-Network-Notes.html#2023-03-18","text":"","title":"2023-03-18"},{"location":"Non-Violent-Communication-In-A-Nutshell.html","text":"Non-Violent Communication in a Nutshell src When [observation], I feel [emotion] because I need [need]. Would you be able to [request] Observation Strive for observations rather than evaluations Observe events rather than evaluate or interpret what happened An evaluation is an interpretation of events. Communicating observations is the difference between showing evidence and communicating conclusions. Examples \"You ignored me\" vs. \"You didn't respond to the three emails I sent\" \"You don't respect me\" vs. \"You didn't show up to our meeting on time\" \"Your work is sloppy\" vs. \"Your report was inaccurate\" Emotion Communicate emotions rather than thoughts Focus on what emotion you're feeling rather than an interpretation of what someone else is feeling If you can substitute 'I think...' then it's a thought, not an emotion. Examples \"I feel you aren't taking this seriously\" vs. \"I feel frustrated\" \"I feel blamed\" vs. \"I feel frustrated\" (\"I think you're blaming me\" vs. \"I'm frustrated\") \"I feel rejected\" vs \"I feel hurt\" (\"I think you're rejecting me\" vs. \"I'm hurt\") Need Declarative needs instead of communicating strategies Perhaps a form of \"declarative vs. imperative\". Communicate universal needs without including the other party Examples \"I need you to copy me on all communication\" vs. \"I need transparency\" Request Request rather than demand Make sure it's specific and actionable Communicate what you want, not what you don't want Examples \"Could you respect me?\" vs. \"Could you arrive on time to dinner?\" Keep the statements concise (\"40-word rule\") Understand how the other party will interpret what's presented Try to work towards common goals where both parties gain","title":"Non Violent Communication In A Nutshell"},{"location":"Non-Violent-Communication-In-A-Nutshell.html#non-violent-communication-in-a-nutshell","text":"src When [observation], I feel [emotion] because I need [need]. Would you be able to [request]","title":"Non-Violent Communication in a Nutshell"},{"location":"Non-Violent-Communication-In-A-Nutshell.html#observation","text":"Strive for observations rather than evaluations Observe events rather than evaluate or interpret what happened An evaluation is an interpretation of events. Communicating observations is the difference between showing evidence and communicating conclusions.","title":"Observation"},{"location":"Non-Violent-Communication-In-A-Nutshell.html#examples","text":"\"You ignored me\" vs. \"You didn't respond to the three emails I sent\" \"You don't respect me\" vs. \"You didn't show up to our meeting on time\" \"Your work is sloppy\" vs. \"Your report was inaccurate\"","title":"Examples"},{"location":"Non-Violent-Communication-In-A-Nutshell.html#emotion","text":"Communicate emotions rather than thoughts Focus on what emotion you're feeling rather than an interpretation of what someone else is feeling If you can substitute 'I think...' then it's a thought, not an emotion.","title":"Emotion"},{"location":"Non-Violent-Communication-In-A-Nutshell.html#examples_1","text":"\"I feel you aren't taking this seriously\" vs. \"I feel frustrated\" \"I feel blamed\" vs. \"I feel frustrated\" (\"I think you're blaming me\" vs. \"I'm frustrated\") \"I feel rejected\" vs \"I feel hurt\" (\"I think you're rejecting me\" vs. \"I'm hurt\")","title":"Examples"},{"location":"Non-Violent-Communication-In-A-Nutshell.html#need","text":"Declarative needs instead of communicating strategies Perhaps a form of \"declarative vs. imperative\". Communicate universal needs without including the other party","title":"Need"},{"location":"Non-Violent-Communication-In-A-Nutshell.html#examples_2","text":"\"I need you to copy me on all communication\" vs. \"I need transparency\"","title":"Examples"},{"location":"Non-Violent-Communication-In-A-Nutshell.html#request","text":"Request rather than demand Make sure it's specific and actionable Communicate what you want, not what you don't want","title":"Request"},{"location":"Non-Violent-Communication-In-A-Nutshell.html#examples_3","text":"\"Could you respect me?\" vs. \"Could you arrive on time to dinner?\" Keep the statements concise (\"40-word rule\") Understand how the other party will interpret what's presented Try to work towards common goals where both parties gain","title":"Examples"},{"location":"Notes-on-Economic-Stagnation.html","text":"Notes on Economic Stagnation Since the 1970s, the United States of America has experienced a decline in wages, a widening wealth gap and a decline of other positive social metrics for most of it's citizens. Many of the same declines are happening across European nations, so it's not just localized to the USA. Base Statistics World Population Evidence of Decline","title":"Notes on Economic Stagnation"},{"location":"Notes-on-Economic-Stagnation.html#notes-on-economic-stagnation","text":"Since the 1970s, the United States of America has experienced a decline in wages, a widening wealth gap and a decline of other positive social metrics for most of it's citizens. Many of the same declines are happening across European nations, so it's not just localized to the USA.","title":"Notes on Economic Stagnation"},{"location":"Notes-on-Economic-Stagnation.html#base-statistics","text":"World Population","title":"Base Statistics"},{"location":"Notes-on-Economic-Stagnation.html#evidence-of-decline","text":"","title":"Evidence of Decline"},{"location":"Notes-on-Gervais-Principle.html","text":"Notes on \"The Gervais Principle\" NOTE : The items on this page are my interpretation of what the Ribbon Farm site is talking about, not my opinions. Everything below should be read with a \"this is the argument that Ribbon Farm is presenting\" prefix. I disagree with many points below but I'm presenting them here to collect my thoughts about what the argument is. The Gervais Principle : Sociopaths, in their own best interests, knowingly promote over-performing losers into middle-management, groom under-performing losers into sociopaths, and leave the average bare-minimum-effort losers to fend for themselves Here, \"loser\" is in the economic sense of the word, in that they are giving more value to the company than they are extracting from it. I will try to refer to them as \"economic loser\" to try and disentangle the alternate social connotation of \"loser\". Some points: This creates a pyramid with 'sociopaths' at the top, the 'clueless' in the middle and 'economic losers' at the bottom The 'clueless' act as a kind of buffer to the 'sociopaths' to shield them from risk 'Sociopaths' at the beginning of an organization, recruits the 'economic losers' to build out the business idea and then, later in the lifecycle of the business, start making decisions, perhaps about mergers, cuts or layoffs, to maximize profit that other people are too compassionate or unaware to pursue Underperforming economic losers that do not have 'sociopathic' ambitions are fired There is a matrix of transition that can be created: \"From\" state \\ \"To\" state Unaffiliated Economic Loser Clueless Sociopath Unaffiliated n/a Entry level position Result of merger/acquisition or leverage from another position Result of merger/acquisition Economic Loser Under performing \"economic loser\" with no \"sociopathic\" ambitions (\"incompetent under performing economic loser\") Bare minimum performing \"economic loser\" Over performing \"economic loser\" Under performing \"economic loser\" with \"sociopathic\" ambitions Clueless Let go to shield a higher ranking \"sociopath\" Demoted to shield higher ranking \"sociopath\" Steady state n/a Sociopath Left because of lack of profitability or exited because of a failed power play Punitive measure or n/a Punitive measure or n/a Steady state The nature of promoting under performing \"economic losers\" to \"sociopath\" positions is a near necessity, as energy needs to be devoted for sociopathic ambitions A weaker argument might be that \"economic losers\" that under perform but have \"sociopath\" ambitions are signalling that they want the desired position Sociopaths are a near requirement to keep the business sustainable/profitable as their emotional detachment allows them to make decisions that keep the business profitable, sustainable or alive (sometimes at the expense of creating a moral hazard in other ways) Economic Losers These are four states of the economic losers: Underperforming with no sociopathic ambitions The exit path is being fired Underperforming with sociopathic ambitions Underperformance is not sustainable so if they aren't promoted, they are fired Bare-minimum performing The vast majority of people (bottom of the pyramid) Have made a trade of security for lack of career power or career freedom Devote ambitions towards other aspect of society life (raising family, hobbies, etc.) Over-performing Give the company more value than they extract Are promoted to \"clueless\" to use as pawns for higher ranking sociopaths Clueless The clueless are used by sociopaths as pawns in their maneuvers for power and profit and can act as a buffer for the sociopaths to shield them from risk. While providing a consistent economic value as an over performing economic loser, they are valued more highly as part of the clueless by sociopaths as they can be used as pawns in their maneuvers for power and profit. [Rao] says this succinctly: ... [Sociopaths] look for ways to systematically claim paternity for successes, and orphan failures. An example given is if a sociopath is leading a potentially highly profitable strategic gamble (Ribbon Farm uses Ryan's 'line sales channel operation' as the example). If the gamble pays off, the sociopath is set to receive the reward. If it becomes clear the gamble is about to fail, the sociopath can promote a person in the clueless position to take the sociopaths place, thus setting the clueless up for failure and shielding the sociopath from the fallout that will happen when the project fails. The clueless will be grateful for the perceived act of giving them power and, presumably, will be more likely to take credit for the failure than to shift blame to the sociopath that initiated the endeavour. The clueless are giving the perception of power while being denied pathways to the upper echelons of power (by becoming a \"sociopath\") Though [Rao] claims that the clueless can't ascend to the sociopath tier, there's some hints that this might not be so clear cut. [Rao] gives two examples of Wallace (the sociopath) coming to talk to Michael (the clueless) because of various successes or profits. [Rao] discusses these interactions in terms of highlighting how the clueless lack any currency in power and \"The Office\" highlights them for comedic effect highlighting the same lack of power but Wallace, at least in one interaction, starts with powertalk. The reason from Wallace's powertalk is to start the negotiation with Michael and because Wallace thought there might be an actual opportunity to exchange power, something that only a sociopath or aspiring sociopath would be in a position to negotiate for. Sociopath Sociopaths are at the top of the power pyramid and are the source of power in the company. The term 'sociopath' itself denotes that the sociopath pursues power and profit over empathy. For a successful company, the sociopath needs to delegate both to other sociopaths that have real power and to subordinates like the clueless to direct the economic losers. There are different stages of a company and a sociopath at the beginning of a company life cycle might be more focused on creating a product or filling a societal niche by some 'value-added' aspect of the business. Later in a businesses life cycle, it's more profitable to leverage the companies wealth and assets to mergers, acquisitions, layoffs and other more financial and business strategies that aren't directly tied to production of value for a society. This is why the term 'sociopath' is used: sociopaths engage, in all phases of the business life cycle, exploitative behavior, extracting as much value from employees while giving the bare minimum back. The implication is that sociopaths add value to society because their pursuit of power and wealth leads them to create incidental artifacts that benefit society. Any moral hazard avoided by sociopaths are done because of systems in place to punish bad behavior on the sociopaths part or because the avoidance of a moral hazard happens to be in line with the pursuit of profit or power. In other words, the sociopath has no vested interest in exploiting workers or creating moral hazards, these just happen to be consequences of their pursuit of power and profit. A particular tactic that [Rao] goes into is called \"Hanlon's Dodge\". \"Hanlon's Razor\" is: Never attribute to malice what can be adequately explained by incompetence. The major tactic that's used is to always have incompetence as a defense, both for themselves if things go wrong or to point the fingers at subordinates. By focusing on incompetence, rather than malice, the punishment for failure is lessoned and cost of failure reduced. [Rao] goes into two major Hanlon dodges: Feigned incompetence - the sociopath acts incompetent for an end goal or to reduce punishment Execution ambiguity - the sociopath, through vague or indirect language, implies that an agent should use unscrupulous means, maintaining plausible deniability and allowing for an accusation of incompetence for the person carrying out the deed To engineer failure, [Rao] writes: When you genuinely want to give reports responsibilities that help them grow, you give them autonomy where they are strong. When you want to use them in engineered \u201cfailures\u201d that give you the outcomes you want, you give them autonomy in areas where they are weak. [Rao] gives a \"Heads I Win, Tails You Lose\" name to it: Benefiting from the success while externalizing the cost of failure. Sociopaths will use a 'divide-and-conquer' approach to manipulate the clueless and the economic losers. Though the social hierarchy is encouraged to be ambiguous (see below), the sociopaths almost surely enforce this behavior to make sure fault lines exist in order to exploit. If necessary, sociopaths can generate fault lines that can be later exploited ([Rao] gives the example of providing commission incentives to the sales team and not the support team). Communication The communication is split into four main classifications: Gametalk : The talk of economic losers among themselves Babytalk : The talk of the economic losers and sociopaths to the clueless, often placating in nature Posturetalk : The talk of the clueless to all other layers Powertalk : The talk of the sociopaths among themselves Of these, powertalk is the only one that significantly changes dynamics and has consequences. The requirement of powertalk is to hold actual power. The purpose of powertalk is to shift power through the exchange of information that is actionable. For example, an exchange of information between two sociopaths of equal value, or the elaboration of information at the expense of someone else. The analogy given in [Rao] uses is one of poker: Gametalk, babytalk and posture talk are like a game of poker with no stakes, powertalk is a game of poker with real money. Miscellaneous Notes Status illegibility, the inability to quantify rank, is a necessary ambiguity to keep the social dynamic stable ( link ) By allowing people to think they might be higher tiered, this allows them to operate under a delusion or to ascend A more complete description might be that the rank can be quantified but that the cost of examination is costly and discouraged so as to keep the farce of ambiguity in place There's probably a parallel to the social taboo of sharing salary and income Almost surely encouraged (or not discouraged) by sociopaths to maintain fertile ground of fault lines to exploit for manipulation \"The Curse of Development\", where a slightly more developed individual loses out to the under developed one, comes into play with the dynamics of how the economic losers and sociopaths deal with the clueless An example is when your pet coerces you to feed them The development of the clueless is proportional to their rank (re-enforcing the idea that the less developed individual gains) Babytalk is used by the economic losers to placate the clueless, effectively making the economic losers incur the mental cost of having to manage the situation Babytalk is used by the sociopaths to manipulate the clueless, thus surpassing the threshold the losses incurred by the curse of development would incur, presumably of the sociopaths access to power rather than anything intrinsic to the sociopath (though there could be an argument that the lack of empathy allows sociopaths to avoid the mental tax of wanting to placate the clueless for sympathetic reasons) This is why, paradoxically, collectivist philosophies that value equality must necessarily value diversity. Nobody wants to equally average. Everybody must be given a chance to be equally above average. Ancillary Notes Media often focuses on the \"loser\" narrative (in the social sense), whereas the clueless narrative (over performing \"loser\" that still doesn't understand how the world works) is what \"The Office\" explores. The \"sociopath\" narrative is a power fantasy with either a 'complex genius' or 'power corrupts' narrative","title":"Notes on Gervais Principle"},{"location":"Notes-on-Gervais-Principle.html#notes-on-the-gervais-principle","text":"NOTE : The items on this page are my interpretation of what the Ribbon Farm site is talking about, not my opinions. Everything below should be read with a \"this is the argument that Ribbon Farm is presenting\" prefix. I disagree with many points below but I'm presenting them here to collect my thoughts about what the argument is. The Gervais Principle : Sociopaths, in their own best interests, knowingly promote over-performing losers into middle-management, groom under-performing losers into sociopaths, and leave the average bare-minimum-effort losers to fend for themselves Here, \"loser\" is in the economic sense of the word, in that they are giving more value to the company than they are extracting from it. I will try to refer to them as \"economic loser\" to try and disentangle the alternate social connotation of \"loser\". Some points: This creates a pyramid with 'sociopaths' at the top, the 'clueless' in the middle and 'economic losers' at the bottom The 'clueless' act as a kind of buffer to the 'sociopaths' to shield them from risk 'Sociopaths' at the beginning of an organization, recruits the 'economic losers' to build out the business idea and then, later in the lifecycle of the business, start making decisions, perhaps about mergers, cuts or layoffs, to maximize profit that other people are too compassionate or unaware to pursue Underperforming economic losers that do not have 'sociopathic' ambitions are fired There is a matrix of transition that can be created: \"From\" state \\ \"To\" state Unaffiliated Economic Loser Clueless Sociopath Unaffiliated n/a Entry level position Result of merger/acquisition or leverage from another position Result of merger/acquisition Economic Loser Under performing \"economic loser\" with no \"sociopathic\" ambitions (\"incompetent under performing economic loser\") Bare minimum performing \"economic loser\" Over performing \"economic loser\" Under performing \"economic loser\" with \"sociopathic\" ambitions Clueless Let go to shield a higher ranking \"sociopath\" Demoted to shield higher ranking \"sociopath\" Steady state n/a Sociopath Left because of lack of profitability or exited because of a failed power play Punitive measure or n/a Punitive measure or n/a Steady state The nature of promoting under performing \"economic losers\" to \"sociopath\" positions is a near necessity, as energy needs to be devoted for sociopathic ambitions A weaker argument might be that \"economic losers\" that under perform but have \"sociopath\" ambitions are signalling that they want the desired position Sociopaths are a near requirement to keep the business sustainable/profitable as their emotional detachment allows them to make decisions that keep the business profitable, sustainable or alive (sometimes at the expense of creating a moral hazard in other ways)","title":"Notes on \"The Gervais Principle\""},{"location":"Notes-on-Gervais-Principle.html#economic-losers","text":"These are four states of the economic losers: Underperforming with no sociopathic ambitions The exit path is being fired Underperforming with sociopathic ambitions Underperformance is not sustainable so if they aren't promoted, they are fired Bare-minimum performing The vast majority of people (bottom of the pyramid) Have made a trade of security for lack of career power or career freedom Devote ambitions towards other aspect of society life (raising family, hobbies, etc.) Over-performing Give the company more value than they extract Are promoted to \"clueless\" to use as pawns for higher ranking sociopaths","title":"Economic Losers"},{"location":"Notes-on-Gervais-Principle.html#clueless","text":"The clueless are used by sociopaths as pawns in their maneuvers for power and profit and can act as a buffer for the sociopaths to shield them from risk. While providing a consistent economic value as an over performing economic loser, they are valued more highly as part of the clueless by sociopaths as they can be used as pawns in their maneuvers for power and profit. [Rao] says this succinctly: ... [Sociopaths] look for ways to systematically claim paternity for successes, and orphan failures. An example given is if a sociopath is leading a potentially highly profitable strategic gamble (Ribbon Farm uses Ryan's 'line sales channel operation' as the example). If the gamble pays off, the sociopath is set to receive the reward. If it becomes clear the gamble is about to fail, the sociopath can promote a person in the clueless position to take the sociopaths place, thus setting the clueless up for failure and shielding the sociopath from the fallout that will happen when the project fails. The clueless will be grateful for the perceived act of giving them power and, presumably, will be more likely to take credit for the failure than to shift blame to the sociopath that initiated the endeavour. The clueless are giving the perception of power while being denied pathways to the upper echelons of power (by becoming a \"sociopath\") Though [Rao] claims that the clueless can't ascend to the sociopath tier, there's some hints that this might not be so clear cut. [Rao] gives two examples of Wallace (the sociopath) coming to talk to Michael (the clueless) because of various successes or profits. [Rao] discusses these interactions in terms of highlighting how the clueless lack any currency in power and \"The Office\" highlights them for comedic effect highlighting the same lack of power but Wallace, at least in one interaction, starts with powertalk. The reason from Wallace's powertalk is to start the negotiation with Michael and because Wallace thought there might be an actual opportunity to exchange power, something that only a sociopath or aspiring sociopath would be in a position to negotiate for.","title":"Clueless"},{"location":"Notes-on-Gervais-Principle.html#sociopath","text":"Sociopaths are at the top of the power pyramid and are the source of power in the company. The term 'sociopath' itself denotes that the sociopath pursues power and profit over empathy. For a successful company, the sociopath needs to delegate both to other sociopaths that have real power and to subordinates like the clueless to direct the economic losers. There are different stages of a company and a sociopath at the beginning of a company life cycle might be more focused on creating a product or filling a societal niche by some 'value-added' aspect of the business. Later in a businesses life cycle, it's more profitable to leverage the companies wealth and assets to mergers, acquisitions, layoffs and other more financial and business strategies that aren't directly tied to production of value for a society. This is why the term 'sociopath' is used: sociopaths engage, in all phases of the business life cycle, exploitative behavior, extracting as much value from employees while giving the bare minimum back. The implication is that sociopaths add value to society because their pursuit of power and wealth leads them to create incidental artifacts that benefit society. Any moral hazard avoided by sociopaths are done because of systems in place to punish bad behavior on the sociopaths part or because the avoidance of a moral hazard happens to be in line with the pursuit of profit or power. In other words, the sociopath has no vested interest in exploiting workers or creating moral hazards, these just happen to be consequences of their pursuit of power and profit. A particular tactic that [Rao] goes into is called \"Hanlon's Dodge\". \"Hanlon's Razor\" is: Never attribute to malice what can be adequately explained by incompetence. The major tactic that's used is to always have incompetence as a defense, both for themselves if things go wrong or to point the fingers at subordinates. By focusing on incompetence, rather than malice, the punishment for failure is lessoned and cost of failure reduced. [Rao] goes into two major Hanlon dodges: Feigned incompetence - the sociopath acts incompetent for an end goal or to reduce punishment Execution ambiguity - the sociopath, through vague or indirect language, implies that an agent should use unscrupulous means, maintaining plausible deniability and allowing for an accusation of incompetence for the person carrying out the deed To engineer failure, [Rao] writes: When you genuinely want to give reports responsibilities that help them grow, you give them autonomy where they are strong. When you want to use them in engineered \u201cfailures\u201d that give you the outcomes you want, you give them autonomy in areas where they are weak. [Rao] gives a \"Heads I Win, Tails You Lose\" name to it: Benefiting from the success while externalizing the cost of failure. Sociopaths will use a 'divide-and-conquer' approach to manipulate the clueless and the economic losers. Though the social hierarchy is encouraged to be ambiguous (see below), the sociopaths almost surely enforce this behavior to make sure fault lines exist in order to exploit. If necessary, sociopaths can generate fault lines that can be later exploited ([Rao] gives the example of providing commission incentives to the sales team and not the support team).","title":"Sociopath"},{"location":"Notes-on-Gervais-Principle.html#communication","text":"The communication is split into four main classifications: Gametalk : The talk of economic losers among themselves Babytalk : The talk of the economic losers and sociopaths to the clueless, often placating in nature Posturetalk : The talk of the clueless to all other layers Powertalk : The talk of the sociopaths among themselves Of these, powertalk is the only one that significantly changes dynamics and has consequences. The requirement of powertalk is to hold actual power. The purpose of powertalk is to shift power through the exchange of information that is actionable. For example, an exchange of information between two sociopaths of equal value, or the elaboration of information at the expense of someone else. The analogy given in [Rao] uses is one of poker: Gametalk, babytalk and posture talk are like a game of poker with no stakes, powertalk is a game of poker with real money.","title":"Communication"},{"location":"Notes-on-Gervais-Principle.html#miscellaneous-notes","text":"Status illegibility, the inability to quantify rank, is a necessary ambiguity to keep the social dynamic stable ( link ) By allowing people to think they might be higher tiered, this allows them to operate under a delusion or to ascend A more complete description might be that the rank can be quantified but that the cost of examination is costly and discouraged so as to keep the farce of ambiguity in place There's probably a parallel to the social taboo of sharing salary and income Almost surely encouraged (or not discouraged) by sociopaths to maintain fertile ground of fault lines to exploit for manipulation \"The Curse of Development\", where a slightly more developed individual loses out to the under developed one, comes into play with the dynamics of how the economic losers and sociopaths deal with the clueless An example is when your pet coerces you to feed them The development of the clueless is proportional to their rank (re-enforcing the idea that the less developed individual gains) Babytalk is used by the economic losers to placate the clueless, effectively making the economic losers incur the mental cost of having to manage the situation Babytalk is used by the sociopaths to manipulate the clueless, thus surpassing the threshold the losses incurred by the curse of development would incur, presumably of the sociopaths access to power rather than anything intrinsic to the sociopath (though there could be an argument that the lack of empathy allows sociopaths to avoid the mental tax of wanting to placate the clueless for sympathetic reasons) This is why, paradoxically, collectivist philosophies that value equality must necessarily value diversity. Nobody wants to equally average. Everybody must be given a chance to be equally above average.","title":"Miscellaneous Notes"},{"location":"Notes-on-Gervais-Principle.html#ancillary-notes","text":"Media often focuses on the \"loser\" narrative (in the social sense), whereas the clueless narrative (over performing \"loser\" that still doesn't understand how the world works) is what \"The Office\" explores. The \"sociopath\" narrative is a power fantasy with either a 'complex genius' or 'power corrupts' narrative","title":"Ancillary Notes"},{"location":"Number-Partition-Problem-Phase-Transition.html","text":"Notes on Phase Transitions for the Number Partition Problem The Number Partition Problem (NPP) is, given $n$ integers, find a partition such that the sum of each is equal. Another way to formulate: $$ \\begin{align} & n, m, a_k \\in \\mathbb{Z}, \\ M = 2^m \\\\ & \\exists \\ \\ \\sigma_k \\in { 0,1 } \\ s.t.\\\\ & \\sum_{k=0}^{n-1} \\sigma_k a_k = 0 \\end{align} $$ If we take an ensemble to be all instances for a given $n$ and $a_k$ chosen in the uniform interval from $[1, 2, \\dots, M]$, then this is the \"uniform random NPP\" (UR-NPP) problem. Gent and Walsh observed that the probability of a number of $m$ bits long being all $0$ is $2^{-m}$ whereas the number of configurations is $2^n$. If we ignore carry, then we can get intuition about the expected number of solutions by observing $ \\frac{2^m}{2^n} > 0 $ when $ m > n $. In other words, for a random instance of the uniform NPP, we would expect to see a transition when the number of bits exceeds the list length. If we want to get intuition about the probability of a solution existing, we can pretend each configuration has no carry and is independent to notice that $(1-2^{-m})^{2^n}$ is the probability that no instance will have a solution. If we take $ m = \\kappa_n n $, then, with the above caveats and some questionable manipulation, the probability becomes: $$ \\begin{align} & 1 - (1 - \\frac{1}{2^{m}})^{2^{n}} \\\\ = & 1 - (1 - \\frac{1}{2^{m}})^{2^{m + n - m}} \\\\ \\approx & 1 - \\exp( - 2^{n-m}) \\\\ = & 1 - \\exp( -2^{-\\kappa_n (n-1)}) \\end{align} $$ Borgs, Chayes, and Pittel (BCP) provide rigour to the uniform random NPP. They introduce a parameter, $\\lamba_n$: $$ \\begin{align} & \\lambda_n = m - n + \\frac{lg(n)}{2n} \\\\ = & \\kappa_n n - n + \\frac{lg(n)}{2n} \\end{align} $$ Where $m = \\kappa_n n$ as above. In some vague sense, the extra $\\frac{lg(n)}{2n}$ is a term accounting for the carry from the addition. BCP use an integral representation for the number of solutions, where $X_k$ are independent, identically distributed random variables chosen over the integral range of $1$ to $M = 2^m$: $$ Z_{n} = \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} \\sum_{k=0}^{n-1} (e^{-X_k \\theta} + e^{X_k \\theta}) d\\theta $$ For a perfect partition, the integral over the expanded summation will be non zero iff there is a perfect partition. Taking expectations, fiddling and noticing independence: $$ \\begin{align} E[Z_{n}] = & E[ \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} \\sum_{k=0}^{n-1} (e^{- X_k i \\theta} + e^{ X_k i \\theta})d\\theta ] \\\\ = & \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} (E[e^{- X i \\theta} + e^{ X i \\theta}])^nd\\theta \\\\ = & \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} ( \\frac{2}{M}\\sum_{k=0}^{M-1} \\cos( k \\theta))^n d\\theta \\ = & \\frac{2^{n(1-m)}}{2\\pi} \\int_{-\\pi}^{\\pi} ( \\frac{\\sin((M+\\frac{1}{2})\\theta)}{2\\sin(\\theta/2)} - \\frac{1}{2} )^n d\\theta \\ \\end{align} $$ References C. Borgs, J. Chayes, B. Pittel, Phase Transition and Finite-Size Scaling for the Integer Partitioning Problem, p I.P. Gent, T. Walsh, Phase Transitions and Annealed Theories: Number Partitioning as a Case Study, p","title":"Number Partition Problem Phase Transition"},{"location":"Number-Partition-Problem-Phase-Transition.html#notes-on-phase-transitions-for-the-number-partition-problem","text":"The Number Partition Problem (NPP) is, given $n$ integers, find a partition such that the sum of each is equal. Another way to formulate: $$ \\begin{align} & n, m, a_k \\in \\mathbb{Z}, \\ M = 2^m \\\\ & \\exists \\ \\ \\sigma_k \\in { 0,1 } \\ s.t.\\\\ & \\sum_{k=0}^{n-1} \\sigma_k a_k = 0 \\end{align} $$ If we take an ensemble to be all instances for a given $n$ and $a_k$ chosen in the uniform interval from $[1, 2, \\dots, M]$, then this is the \"uniform random NPP\" (UR-NPP) problem. Gent and Walsh observed that the probability of a number of $m$ bits long being all $0$ is $2^{-m}$ whereas the number of configurations is $2^n$. If we ignore carry, then we can get intuition about the expected number of solutions by observing $ \\frac{2^m}{2^n} > 0 $ when $ m > n $. In other words, for a random instance of the uniform NPP, we would expect to see a transition when the number of bits exceeds the list length. If we want to get intuition about the probability of a solution existing, we can pretend each configuration has no carry and is independent to notice that $(1-2^{-m})^{2^n}$ is the probability that no instance will have a solution. If we take $ m = \\kappa_n n $, then, with the above caveats and some questionable manipulation, the probability becomes: $$ \\begin{align} & 1 - (1 - \\frac{1}{2^{m}})^{2^{n}} \\\\ = & 1 - (1 - \\frac{1}{2^{m}})^{2^{m + n - m}} \\\\ \\approx & 1 - \\exp( - 2^{n-m}) \\\\ = & 1 - \\exp( -2^{-\\kappa_n (n-1)}) \\end{align} $$ Borgs, Chayes, and Pittel (BCP) provide rigour to the uniform random NPP. They introduce a parameter, $\\lamba_n$: $$ \\begin{align} & \\lambda_n = m - n + \\frac{lg(n)}{2n} \\\\ = & \\kappa_n n - n + \\frac{lg(n)}{2n} \\end{align} $$ Where $m = \\kappa_n n$ as above. In some vague sense, the extra $\\frac{lg(n)}{2n}$ is a term accounting for the carry from the addition. BCP use an integral representation for the number of solutions, where $X_k$ are independent, identically distributed random variables chosen over the integral range of $1$ to $M = 2^m$: $$ Z_{n} = \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} \\sum_{k=0}^{n-1} (e^{-X_k \\theta} + e^{X_k \\theta}) d\\theta $$ For a perfect partition, the integral over the expanded summation will be non zero iff there is a perfect partition. Taking expectations, fiddling and noticing independence: $$ \\begin{align} E[Z_{n}] = & E[ \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} \\sum_{k=0}^{n-1} (e^{- X_k i \\theta} + e^{ X_k i \\theta})d\\theta ] \\\\ = & \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} (E[e^{- X i \\theta} + e^{ X i \\theta}])^nd\\theta \\\\ = & \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} ( \\frac{2}{M}\\sum_{k=0}^{M-1} \\cos( k \\theta))^n d\\theta \\ = & \\frac{2^{n(1-m)}}{2\\pi} \\int_{-\\pi}^{\\pi} ( \\frac{\\sin((M+\\frac{1}{2})\\theta)}{2\\sin(\\theta/2)} - \\frac{1}{2} )^n d\\theta \\ \\end{align} $$","title":"Notes on Phase Transitions for the Number Partition Problem"},{"location":"Number-Partition-Problem-Phase-Transition.html#references","text":"C. Borgs, J. Chayes, B. Pittel, Phase Transition and Finite-Size Scaling for the Integer Partitioning Problem, p I.P. Gent, T. Walsh, Phase Transitions and Annealed Theories: Number Partitioning as a Case Study, p","title":"References"},{"location":"Number-Theory-Notes.html","text":"Number Theory Notes Wilson's Theorem $p$ prime $$ \\prod_{i=1}^{p-1} (i) \\mod p \\equiv -1 \\mod p $$ proof : $$ \\begin{equation} \\label{eq1} \\begin{split} \\prod_{i=1}^{p-1} (i) & = 1 \\cdot 2 \\cdot 3 \\cdots (p-1) \\\\ & = (a_0 a_0') (a_1 a_1^{-1}) \\cdots (a_{\\frac{p-1}{2}} a_{\\frac{p-1}{2}}^{-1}) \\\\ & = [1] \\end{split} \\end{equation} $$ Where $a_0=1$ and $a_0' = -1$. lemma : The only number whose inverse is itself is $(-1)$ lemma proof : $$ \\begin{split} x^2 & = 1 \\mod p \\\\ & = \\pm 1 \\mod p \\end{split} $$ Since $p$ is prime, there are only two solutions. proof (cont'd) : $$ \\begin{split} \\to [1] & = (1 \\cdot -1) (a_1 a_1^{-1}) (a_2 a_2^{-1}) \\cdots (a_{\\frac{p-1}{2}} a_{\\frac{p-1}{2}}^{-1}) \\\\ & = (1 \\cdot -1) (1) (1) \\cdots (1) \\\\ & = -1 \\end{split} $$ Fermat's Theorem $p$ prime $$ \\begin{split} x^{p-1} = 1 \\mod p \\end{split} $$ proof : $a \\ne 0$ $$ \\begin{align} & & -1 & = 1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdots (p-1) \\mod p \\\\ \\to & & -a^{p-1} & = (a \\cdot 1) (a \\cdot 2) (a \\cdot 3) \\cdots (a \\cdot (p-1)) \\mod p \\\\ \\to & & -a^{p-1} & = 1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdots (p-1) \\mod p \\\\ \\to & & -a^{p-1} & = -1 \\mod p \\\\ \\to & & a^{p-1} & = 1 \\mod p \\end{align} $$ Since $a \\cdot x$ is 1-1 and onto for prime $p$ (with $a,x \\ne 0$). Legendre Symbol $p$ prime $$ \\left( \\dfrac{a}{p} \\right) = \\begin{cases} 1, & \\text{ if } \\sqrt{a} \\text{ exists } \\\\ 0, & \\text{ if } \\gcd(a,p) \\ne 1 \\\\ -1, & \\text{ if } \\sqrt{a} \\text{ does not exist } \\end{cases} $$ notes : $g$ a generator of $p$, consider $a = g^\\beta$ $\\beta$ even: $$ \\begin{align} & & g^\\beta & = a \\\\ \\to & & ( g^{\\frac{p-1}{2}} ) )^\\beta & = ( a^{\\frac{p-1}{a}} ) \\\\ \\to & & (-1)^\\beta & = a^{\\frac{p-1}{2}} \\\\ \\to & & a^{\\frac{p-1}{2}} & = 1 \\\\ \\end{align} $$ $\\beta$ odd: $$ \\begin{align} \\to & & (-1)^\\beta & = a^{\\frac{p-1}{2}} \\\\ \\to & & a^{\\frac{p-1}{2}} & = -1 \\\\ \\end{align} $$ So: $$ \\left( \\dfrac{a}{p} \\right) = a^{\\frac{p-1}{2}} $$ Zeta Function $$ \\begin{split} \\sum_{n=1}^{\\infty} \\frac{1}{n^s} & = \\prod_{p \\text{ prime}}^{\\infty} ( \\frac{1}{1 - p^{-s}} ) \\end{split} $$ proof : $$ \\begin{align} & & \\sum_{i=1}^{\\infty} a^i & = S \\\\ & & \\sum_{i=0}^{\\infty} a^{i+1} & = \\sum_{i=1}^{\\infty} a^i \\\\ & & & = S a \\\\ & & & \\\\ & & S - S a & = 1 \\\\ \\to & & S & = \\frac{1}{1-a} \\end{align} $$ Write out the product of infinit series of primes, $p_i$: $$ \\begin{align} & ( 1 + p_0^{-s} + p_0^{-2s} + p_0^{-3s} + \\cdots ) \\\\ & ( 1 + p_1^{-s} + p_1^{-2s} + p_1^{-3s} + \\cdots ) \\\\ & \\cdots \\\\ & = \\prod_{p \\text{ prime}}^{\\infty} [ \\sum_{i=0}^{\\infty} \\frac{1}{p^{i s}} ] \\\\ & = \\prod_{p \\text{ prime}}^{\\infty} ( \\frac{1}{1-p^{-s}} ) \\end{align} $$ Any choice of terms in the product of infinite sums of primes will yield a value in the original $\\sum \\frac{1}{n^s}$. 2019-01-10","title":"Number Theory Notes"},{"location":"Number-Theory-Notes.html#number-theory-notes","text":"","title":"Number Theory Notes"},{"location":"Number-Theory-Notes.html#wilsons-theorem","text":"$p$ prime $$ \\prod_{i=1}^{p-1} (i) \\mod p \\equiv -1 \\mod p $$ proof : $$ \\begin{equation} \\label{eq1} \\begin{split} \\prod_{i=1}^{p-1} (i) & = 1 \\cdot 2 \\cdot 3 \\cdots (p-1) \\\\ & = (a_0 a_0') (a_1 a_1^{-1}) \\cdots (a_{\\frac{p-1}{2}} a_{\\frac{p-1}{2}}^{-1}) \\\\ & = [1] \\end{split} \\end{equation} $$ Where $a_0=1$ and $a_0' = -1$. lemma : The only number whose inverse is itself is $(-1)$ lemma proof : $$ \\begin{split} x^2 & = 1 \\mod p \\\\ & = \\pm 1 \\mod p \\end{split} $$ Since $p$ is prime, there are only two solutions. proof (cont'd) : $$ \\begin{split} \\to [1] & = (1 \\cdot -1) (a_1 a_1^{-1}) (a_2 a_2^{-1}) \\cdots (a_{\\frac{p-1}{2}} a_{\\frac{p-1}{2}}^{-1}) \\\\ & = (1 \\cdot -1) (1) (1) \\cdots (1) \\\\ & = -1 \\end{split} $$","title":"Wilson's Theorem"},{"location":"Number-Theory-Notes.html#fermats-theorem","text":"$p$ prime $$ \\begin{split} x^{p-1} = 1 \\mod p \\end{split} $$ proof : $a \\ne 0$ $$ \\begin{align} & & -1 & = 1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdots (p-1) \\mod p \\\\ \\to & & -a^{p-1} & = (a \\cdot 1) (a \\cdot 2) (a \\cdot 3) \\cdots (a \\cdot (p-1)) \\mod p \\\\ \\to & & -a^{p-1} & = 1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdots (p-1) \\mod p \\\\ \\to & & -a^{p-1} & = -1 \\mod p \\\\ \\to & & a^{p-1} & = 1 \\mod p \\end{align} $$ Since $a \\cdot x$ is 1-1 and onto for prime $p$ (with $a,x \\ne 0$).","title":"Fermat's Theorem"},{"location":"Number-Theory-Notes.html#legendre-symbol","text":"$p$ prime $$ \\left( \\dfrac{a}{p} \\right) = \\begin{cases} 1, & \\text{ if } \\sqrt{a} \\text{ exists } \\\\ 0, & \\text{ if } \\gcd(a,p) \\ne 1 \\\\ -1, & \\text{ if } \\sqrt{a} \\text{ does not exist } \\end{cases} $$ notes : $g$ a generator of $p$, consider $a = g^\\beta$ $\\beta$ even: $$ \\begin{align} & & g^\\beta & = a \\\\ \\to & & ( g^{\\frac{p-1}{2}} ) )^\\beta & = ( a^{\\frac{p-1}{a}} ) \\\\ \\to & & (-1)^\\beta & = a^{\\frac{p-1}{2}} \\\\ \\to & & a^{\\frac{p-1}{2}} & = 1 \\\\ \\end{align} $$ $\\beta$ odd: $$ \\begin{align} \\to & & (-1)^\\beta & = a^{\\frac{p-1}{2}} \\\\ \\to & & a^{\\frac{p-1}{2}} & = -1 \\\\ \\end{align} $$ So: $$ \\left( \\dfrac{a}{p} \\right) = a^{\\frac{p-1}{2}} $$","title":"Legendre Symbol"},{"location":"Number-Theory-Notes.html#zeta-function","text":"$$ \\begin{split} \\sum_{n=1}^{\\infty} \\frac{1}{n^s} & = \\prod_{p \\text{ prime}}^{\\infty} ( \\frac{1}{1 - p^{-s}} ) \\end{split} $$ proof : $$ \\begin{align} & & \\sum_{i=1}^{\\infty} a^i & = S \\\\ & & \\sum_{i=0}^{\\infty} a^{i+1} & = \\sum_{i=1}^{\\infty} a^i \\\\ & & & = S a \\\\ & & & \\\\ & & S - S a & = 1 \\\\ \\to & & S & = \\frac{1}{1-a} \\end{align} $$ Write out the product of infinit series of primes, $p_i$: $$ \\begin{align} & ( 1 + p_0^{-s} + p_0^{-2s} + p_0^{-3s} + \\cdots ) \\\\ & ( 1 + p_1^{-s} + p_1^{-2s} + p_1^{-3s} + \\cdots ) \\\\ & \\cdots \\\\ & = \\prod_{p \\text{ prime}}^{\\infty} [ \\sum_{i=0}^{\\infty} \\frac{1}{p^{i s}} ] \\\\ & = \\prod_{p \\text{ prime}}^{\\infty} ( \\frac{1}{1-p^{-s}} ) \\end{align} $$ Any choice of terms in the product of infinite sums of primes will yield a value in the original $\\sum \\frac{1}{n^s}$.","title":"Zeta Function"},{"location":"Number-Theory-Notes.html#2019-01-10","text":"","title":"2019-01-10"},{"location":"PCB-Notes.html","text":"PCB Notes PCB Thickness Copper thickness is commonly expressed in oz , for example 1 oz Cu . This is shorthand for the height of copper if spread over a square foot surface. To calculate the height for 1 oz Cu , we need a few conversions: $ 1\\ \\mathrm{oz} = 0.0625\\ \\mathrm{lbs}$ $\\mathrm{Cu}\\ \\mathrm{density} = 8.96 \\frac{\\mathrm{g}}{\\mathrm{cm}^3} $ $ 1\\ \\mathrm{g} \\approx 0.00220462\\ \\mathrm{lbs} $ $ 1\\ \\mathrm{cm} \\approx 0.393701\\ \\mathrm{in} $ This implies: $\\mathrm{Cu}\\ \\mathrm{density} = \\frac{8.96 \\cdot 0.00220462\\ \\mathrm{lbs}}{(0.393701^3)\\ \\mathrm{in}^3} \\approx 0.324 \\frac{\\mathrm{lbs}}{\\mathrm{in}^3} $ $ h\\ \\mathrm{in} = \\frac{1\\ \\mathrm{oz}}{ 1\\ \\mathrm{ft}^2 } = \\frac{0.0625\\ \\mathrm{lbs}}{ 144\\ \\mathrm{in}^2 } \\approx .00134 $ Or, in general, $$ z\\ \\mathrm{oz}\\ \\mathrm{Cu} \\rightarrow z \\cdot 1.34\\ \\mathrm{mil} $$ Where $1\\ \\mathrm{mil} = \\frac{1}{1000}\\ \\mathrm{in}$. Decoupling Capacitors Either one 100nF or two, one of 0.1uF and another of 10uF Place as close to the chip as possible Place the decoupling capacitor across the power supply (3.3v or 5v) ( src ) SMD Sizes SMD mm inch 2920 7.4 x 5.1 0.29 x 0.20 2725 6.9 x 6.3 0.27 x 0.25 2512 6.3 x 3.2 0.25 x 0.125 2010 5.0 x 2.5 0.20 x 0.10 1825 4.5 x 6.4 0.18 x 0.25 1812 4.6 x 3.0 0.18 x 0.125 1806 4.5 x 1.6 0.18 x 0.06 1210 3.2 x 2.5 0.125 x 0.10 1206 3.0 x 1.5 0.12 x 0.06 1008 2.5 x 2.0 0.10 x 0.08 0805 2.0 x 1.3 0.08 x 0.05 0603 1.5 x 0.8 0.06 x 0.03 0402 1.0 x 0.5 0.04 x 0.02 0201 0.6 x 0.3 0.02 x 0.01 01005 0.4 x 0.2 0.016 x 0.008 Package Pitch (mm) SOIC 1.27 TSOP 0.5 SSOP 0.635 QSOP 0.635 VSOP 0.4, 0.5, 0.65 LQFP 1.4 PLCC 1.27 2018-02-03","title":"PCB Notes"},{"location":"PCB-Notes.html#pcb-notes","text":"","title":"PCB Notes"},{"location":"PCB-Notes.html#pcb-thickness","text":"Copper thickness is commonly expressed in oz , for example 1 oz Cu . This is shorthand for the height of copper if spread over a square foot surface. To calculate the height for 1 oz Cu , we need a few conversions: $ 1\\ \\mathrm{oz} = 0.0625\\ \\mathrm{lbs}$ $\\mathrm{Cu}\\ \\mathrm{density} = 8.96 \\frac{\\mathrm{g}}{\\mathrm{cm}^3} $ $ 1\\ \\mathrm{g} \\approx 0.00220462\\ \\mathrm{lbs} $ $ 1\\ \\mathrm{cm} \\approx 0.393701\\ \\mathrm{in} $ This implies: $\\mathrm{Cu}\\ \\mathrm{density} = \\frac{8.96 \\cdot 0.00220462\\ \\mathrm{lbs}}{(0.393701^3)\\ \\mathrm{in}^3} \\approx 0.324 \\frac{\\mathrm{lbs}}{\\mathrm{in}^3} $ $ h\\ \\mathrm{in} = \\frac{1\\ \\mathrm{oz}}{ 1\\ \\mathrm{ft}^2 } = \\frac{0.0625\\ \\mathrm{lbs}}{ 144\\ \\mathrm{in}^2 } \\approx .00134 $ Or, in general, $$ z\\ \\mathrm{oz}\\ \\mathrm{Cu} \\rightarrow z \\cdot 1.34\\ \\mathrm{mil} $$ Where $1\\ \\mathrm{mil} = \\frac{1}{1000}\\ \\mathrm{in}$.","title":"PCB Thickness"},{"location":"PCB-Notes.html#decoupling-capacitors","text":"Either one 100nF or two, one of 0.1uF and another of 10uF Place as close to the chip as possible Place the decoupling capacitor across the power supply (3.3v or 5v) ( src )","title":"Decoupling Capacitors"},{"location":"PCB-Notes.html#smd-sizes","text":"SMD mm inch 2920 7.4 x 5.1 0.29 x 0.20 2725 6.9 x 6.3 0.27 x 0.25 2512 6.3 x 3.2 0.25 x 0.125 2010 5.0 x 2.5 0.20 x 0.10 1825 4.5 x 6.4 0.18 x 0.25 1812 4.6 x 3.0 0.18 x 0.125 1806 4.5 x 1.6 0.18 x 0.06 1210 3.2 x 2.5 0.125 x 0.10 1206 3.0 x 1.5 0.12 x 0.06 1008 2.5 x 2.0 0.10 x 0.08 0805 2.0 x 1.3 0.08 x 0.05 0603 1.5 x 0.8 0.06 x 0.03 0402 1.0 x 0.5 0.04 x 0.02 0201 0.6 x 0.3 0.02 x 0.01 01005 0.4 x 0.2 0.016 x 0.008 Package Pitch (mm) SOIC 1.27 TSOP 0.5 SSOP 0.635 QSOP 0.635 VSOP 0.4, 0.5, 0.65 LQFP 1.4 PLCC 1.27","title":"SMD Sizes"},{"location":"PCB-Notes.html#2018-02-03","text":"","title":"2018-02-03"},{"location":"Permutation-Group.html","text":"Permutation Group Consider a group $G$ that acts on a set $X$. (Without loss of generality?) assume $X$ consists of $N$ array elements, each of which is $n$ long with each entry drawn from $m$ values. The permutation group element, $g \\in G$, acts on elements $x \\in X$ by permuting the elements $g \\cdot x \\in X$. As a canonical example, consider a $3 \\times 3$ grid with two colors and the group symmetries of a square: Call two elements of $X$ similar, $x \\sim _ G y$ if $\\exists \\ g \\in G \\ s.t. y = g \\cdot x$. Call the orbit of an element $x$, $O _ x = { y : g \\in G, y = g \\cdot x \\in X }$. Call the stabilizer of an $x$, $S _ x = { g : g \\in G, g \\cdot x = x }$. Claim: $\\sim _ G$ is an equivalence relation $$ \\begin{array}{ll} \\varepsilon \\in G & \\to \\varepsilon \\cdot x = x \\ & x \\sim _ G x \\ x \\sim _ G y, & \\to g \\cdot x = y \\ & \\to x = g^{-1} y \\ & \\to y \\sim _ G x \\ x \\sim _ G y, & y \\sim _ G z \\ & \\to g \\cdot x = y, \\ \\ h \\cdot y = z \\ & \\to g \\cdot x = h^{-1} z \\ & \\to h \\cdot g \\cdot x = z \\ & \\to x \\sim _ G z \\ \\end{array} $$ Claim: $S _ x \\le G$ $$ \\begin{array}{ll} \\varepsilon \\in S_x, & \\varepsilon \\cdot x = x \\ g \\in S_x \\to g^{-1} \\in S_x, & g \\cdot x = x \\ & \\to g^{-1} \\cdot g \\cdot x = g^{-1} x \\ & \\to x = g^{-1} x \\ \\end{array} $$ Claim: $|O _ x| \\times |S _ x| = |G|$ We must show $|O _ x| = | [ G : S _ x ] |$ $$ \\begin{array}{ll} g S _ x = h S _ x & \\iff h^{-1} g S _ x = S _ x \\ & \\iff h^{-1} g \\in S _ x \\ & \\iff h^{-1} g x = x \\ & \\iff g \\cdot x = h \\cdot x \\ \\end{array} $$ Claim: $\\frac{1}{|G|} \\sum _ {x \\in X} |S _ x|$ is the number of distinct orbits Suppose $k$ orbits, ${ O _ {x _ 0}, O _ {x _ 1}, O _ { x _ 2}, \\dots , O _ { x _ { k-1} } }$ $$ \\begin{array}{ll} \\sum _ {x \\in O _ { x _ j } } |S _ {x}| & = \\sum _ {x \\in O _ {x _ j} } \\frac{ |G| }{ |O _ x| } \\ & = \\sum _ {x \\in O _ {x _ j} } \\frac{ |G| }{ |O _ {x _ j}| } \\ & = |G| \\ \\frac{1}{|G|} \\sum _ {x \\in X} |S _ x| & = \\frac{1}{|G|} \\sum _ {j=0}^{k-1} \\sum _ {x \\in O _ {x _ j}} |S _ x| \\ & = \\frac{1}{|G|} \\sum _ {j=0}^{k-1} |G| \\ & = k \\end{array} $$ Burnside's Theorem (Frobenius) $$ \\begin{array}{l} \\text{Fix}(g) = { x \\in X | g \\cdot x = x } \\ \\sum _ { g \\in G } | \\text{Fix}(g) | = \\sum _ {x \\in X} | S _ x | \\end{array} $$ 2023-04-18","title":"Permutation Group"},{"location":"Permutation-Group.html#permutation-group","text":"Consider a group $G$ that acts on a set $X$. (Without loss of generality?) assume $X$ consists of $N$ array elements, each of which is $n$ long with each entry drawn from $m$ values. The permutation group element, $g \\in G$, acts on elements $x \\in X$ by permuting the elements $g \\cdot x \\in X$. As a canonical example, consider a $3 \\times 3$ grid with two colors and the group symmetries of a square: Call two elements of $X$ similar, $x \\sim _ G y$ if $\\exists \\ g \\in G \\ s.t. y = g \\cdot x$. Call the orbit of an element $x$, $O _ x = { y : g \\in G, y = g \\cdot x \\in X }$. Call the stabilizer of an $x$, $S _ x = { g : g \\in G, g \\cdot x = x }$. Claim: $\\sim _ G$ is an equivalence relation $$ \\begin{array}{ll} \\varepsilon \\in G & \\to \\varepsilon \\cdot x = x \\ & x \\sim _ G x \\ x \\sim _ G y, & \\to g \\cdot x = y \\ & \\to x = g^{-1} y \\ & \\to y \\sim _ G x \\ x \\sim _ G y, & y \\sim _ G z \\ & \\to g \\cdot x = y, \\ \\ h \\cdot y = z \\ & \\to g \\cdot x = h^{-1} z \\ & \\to h \\cdot g \\cdot x = z \\ & \\to x \\sim _ G z \\ \\end{array} $$ Claim: $S _ x \\le G$ $$ \\begin{array}{ll} \\varepsilon \\in S_x, & \\varepsilon \\cdot x = x \\ g \\in S_x \\to g^{-1} \\in S_x, & g \\cdot x = x \\ & \\to g^{-1} \\cdot g \\cdot x = g^{-1} x \\ & \\to x = g^{-1} x \\ \\end{array} $$ Claim: $|O _ x| \\times |S _ x| = |G|$ We must show $|O _ x| = | [ G : S _ x ] |$ $$ \\begin{array}{ll} g S _ x = h S _ x & \\iff h^{-1} g S _ x = S _ x \\ & \\iff h^{-1} g \\in S _ x \\ & \\iff h^{-1} g x = x \\ & \\iff g \\cdot x = h \\cdot x \\ \\end{array} $$ Claim: $\\frac{1}{|G|} \\sum _ {x \\in X} |S _ x|$ is the number of distinct orbits Suppose $k$ orbits, ${ O _ {x _ 0}, O _ {x _ 1}, O _ { x _ 2}, \\dots , O _ { x _ { k-1} } }$ $$ \\begin{array}{ll} \\sum _ {x \\in O _ { x _ j } } |S _ {x}| & = \\sum _ {x \\in O _ {x _ j} } \\frac{ |G| }{ |O _ x| } \\ & = \\sum _ {x \\in O _ {x _ j} } \\frac{ |G| }{ |O _ {x _ j}| } \\ & = |G| \\ \\frac{1}{|G|} \\sum _ {x \\in X} |S _ x| & = \\frac{1}{|G|} \\sum _ {j=0}^{k-1} \\sum _ {x \\in O _ {x _ j}} |S _ x| \\ & = \\frac{1}{|G|} \\sum _ {j=0}^{k-1} |G| \\ & = k \\end{array} $$","title":"Permutation Group"},{"location":"Permutation-Group.html#burnsides-theorem-frobenius","text":"$$ \\begin{array}{l} \\text{Fix}(g) = { x \\in X | g \\cdot x = x } \\ \\sum _ { g \\in G } | \\text{Fix}(g) | = \\sum _ {x \\in X} | S _ x | \\end{array} $$","title":"Burnside's Theorem (Frobenius)"},{"location":"Permutation-Group.html#2023-04-18","text":"","title":"2023-04-18"},{"location":"Probability-Definitions.html","text":"Probability Definitions Random Variable For input space $\\Omega$ and output space $G$, a random variable is a function that, for each \"event\" $\\omega \\in \\Omega$, assigns a probability and value $g(\\omega) \\in G$: $$ X = g(\\omega) \\text{, with probability } p _ { \\omega } $$ Shannon Entropy $$ H(p) = - \\sum_{x} p(x) \\lg p(x) $$ Conditional Entropy $$ H(X | Y) = \\sum _ {x,y} p(x,y) \\lg \\left( \\frac{p(x,y)}{p(x)} \\right) $$ Mutual Information $$ \\begin{array}{ll} I(X;Y) & = H(X) - H(X | Y) \\\\ & = D _ {KL} \\left[ \\ p(x , y) \\ || \\ p(x) \\cdot p(y) \\ \\right] \\\\ & = D _ {KL} \\left[ \\ p(x | y) \\ || \\ p(x) \\ \\right] \\\\ & = D _ {KL} \\left[ \\ p(y | x) \\ || \\ p(y) \\ \\right] \\\\ \\end{array} $$ Expectation on Transformed Random Variable $$ \\begin{array}{ll} E [ f ( X ) ] = \\sum _ { k } p _ k f( x _ k ) \\end{array} $$ Cross Entropy $$ H(p,q) = - \\sum_{x} p(x) \\lg q(x) $$ Kullback-Leilbler Divergence $$ \\begin{array}{ll} D_{KL} (p || q) & = - \\sum_{k} p(x) \\lg \\frac{q(x)}{p(x)} \\\\ & = - \\left( \\sum_{x} p(x) \\lg q(x) - p(x) \\lg p(x) \\right) \\end{array} $$ Maximum Likelihood Estimation todo 2020-06-12","title":"Probability Definitions"},{"location":"Probability-Definitions.html#probability-definitions","text":"","title":"Probability Definitions"},{"location":"Probability-Definitions.html#random-variable","text":"For input space $\\Omega$ and output space $G$, a random variable is a function that, for each \"event\" $\\omega \\in \\Omega$, assigns a probability and value $g(\\omega) \\in G$: $$ X = g(\\omega) \\text{, with probability } p _ { \\omega } $$","title":"Random Variable"},{"location":"Probability-Definitions.html#shannon-entropy","text":"$$ H(p) = - \\sum_{x} p(x) \\lg p(x) $$","title":"Shannon Entropy"},{"location":"Probability-Definitions.html#conditional-entropy","text":"$$ H(X | Y) = \\sum _ {x,y} p(x,y) \\lg \\left( \\frac{p(x,y)}{p(x)} \\right) $$","title":"Conditional Entropy"},{"location":"Probability-Definitions.html#mutual-information","text":"$$ \\begin{array}{ll} I(X;Y) & = H(X) - H(X | Y) \\\\ & = D _ {KL} \\left[ \\ p(x , y) \\ || \\ p(x) \\cdot p(y) \\ \\right] \\\\ & = D _ {KL} \\left[ \\ p(x | y) \\ || \\ p(x) \\ \\right] \\\\ & = D _ {KL} \\left[ \\ p(y | x) \\ || \\ p(y) \\ \\right] \\\\ \\end{array} $$","title":"Mutual Information"},{"location":"Probability-Definitions.html#expectation-on-transformed-random-variable","text":"$$ \\begin{array}{ll} E [ f ( X ) ] = \\sum _ { k } p _ k f( x _ k ) \\end{array} $$","title":"Expectation on Transformed Random Variable"},{"location":"Probability-Definitions.html#cross-entropy","text":"$$ H(p,q) = - \\sum_{x} p(x) \\lg q(x) $$","title":"Cross Entropy"},{"location":"Probability-Definitions.html#kullback-leilbler-divergence","text":"$$ \\begin{array}{ll} D_{KL} (p || q) & = - \\sum_{k} p(x) \\lg \\frac{q(x)}{p(x)} \\\\ & = - \\left( \\sum_{x} p(x) \\lg q(x) - p(x) \\lg p(x) \\right) \\end{array} $$","title":"Kullback-Leilbler Divergence"},{"location":"Probability-Definitions.html#maximum-likelihood-estimation","text":"todo","title":"Maximum Likelihood Estimation"},{"location":"Probability-Definitions.html#2020-06-12","text":"","title":"2020-06-12"},{"location":"Probability-Distribution-Generation-Notes.html","text":"Probability Distribution Generation Notes Sampling from a random variable, $X$, with a source uniform random variable $U$ (on $[0,1]$). $$ p_U(t) = \\begin{cases} 1, & & 0 < x < 1 \\ 0, & & \\text{otherwise} \\end{cases} $$ $p_X(t)$ is the distribution in question. $$ \\begin{split} \\text{cdf}_X(s) & = \\int_0^s \\text{pdf}_X(t) dt \\ & = y $$ If $\\text{cdf}_X(s)$ can be inverted, you get: $$ s = \\text{cdf}^{-1}_X{y} $$ 2018-12-19","title":"Probability Distribution Generation Notes"},{"location":"Probability-Distribution-Generation-Notes.html#probability-distribution-generation-notes","text":"Sampling from a random variable, $X$, with a source uniform random variable $U$ (on $[0,1]$). $$ p_U(t) = \\begin{cases} 1, & & 0 < x < 1 \\ 0, & & \\text{otherwise} \\end{cases} $$ $p_X(t)$ is the distribution in question. $$ \\begin{split} \\text{cdf}_X(s) & = \\int_0^s \\text{pdf}_X(t) dt \\ & = y $$ If $\\text{cdf}_X(s)$ can be inverted, you get: $$ s = \\text{cdf}^{-1}_X{y} $$","title":"Probability Distribution Generation Notes"},{"location":"Probability-Distribution-Generation-Notes.html#2018-12-19","text":"","title":"2018-12-19"},{"location":"Probability-Notes.html","text":"Probability Notes By convention: $$ E^n[X] \\stackrel{def}{=} (E[X])^n $$ Independence of Expectation (finite) Claim : $$ E[ \\sum_{k=0}^{n-1} X_k ] = \\sum_{k=0}^{n-1} E[X_k] $$ Proof : $$ \\begin{align} E[ X + Y ] & = \\int \\int (s + t) \\Pr\\{ X = s \\ \\& \\ Y = t \\} \\ ds \\ dt \\\\ & = \\int \\int s \\Pr \\{ X = s \\ \\& \\ Y = t \\} \\ ds \\ dt + \\int \\int t \\Pr \\{ X = s \\ \\& \\ Y = t \\} \\ ds \\ dt \\\\ & = \\int \\int s \\Pr \\{ X = s \\ \\& \\ Y = t \\} \\ dt \\ ds + \\int \\int t \\Pr \\{ X = s \\ \\& \\ Y = t \\} \\ ds \\ dt \\\\ & = \\int s \\Pr \\{ X = s \\} \\ ds + \\int t \\Pr \\{ Y = t \\} \\ dt \\\\ & = E[X] + E[Y] \\end{align} $$ Induction can be used to extend to the general case: $$ E[ \\sum_{k=0}^{n-1} X_k ] = \\sum_{k=0}^{n-1} E[X_k] $$ Bayes' Theorem $$ \\Pr\\{ A | B \\} = \\frac{ \\Pr\\{ A \\& B \\} }{ \\Pr\\{ B \\} } $$ $$ \\Pr\\{ B | A \\} = \\frac{ \\Pr\\{ A \\& B \\} }{ \\Pr\\{ A \\} } $$ $$ \\Pr\\{ A | B \\} = \\frac{ \\Pr\\{ B | A \\} \\Pr\\{ A \\} }{ \\Pr\\{ B \\} } $$ Variance $$ \\mathrm{Var}[X] \\stackrel{def}{=} E[(X - E[X])^2] = E[X^2] - (E[X])^2 $$ Moment Generating Functions $$ M_X(t) \\stackrel{def}{=} E[ e^{t X} ] = \\sum_{k=0}^{\\infty} \\frac{t^k E[X^k]}{k!} $$ If $X$ and $Y$ and independent random variables, then: $$ M_{X + Y}(t) = E[ e^{t(X + Y)} ] = E[ e^{tX} e^{tY} ] = M_X(t) \\cdot M_Y(t) $$ $$ \\begin{align} \\frac{d^n}{dt} M_X(t) & = \\frac{d^{(n)}}{dt} ( \\sum_{k=0}^{\\infty} \\frac{t^n E[X^n]}{k!} ) \\\\ & = \\sum_{k=n} \\frac{t^{k-n} E[X^k]}{(k-n)!} \\\\ \\to \\frac{d^n}{dt} M_X(0) & = E[X^n] \\end{align} $$ Characteristic Function $$ \\varphi_X(t) = E[ e^{itX} ] $$ Not all r.v.s have moment generating functions but all r.v.s have a characteristic function. If the moment generating function exists, then: $$ \\varphi_X(-it) = M_X(t) $$ Jensen's Inequality Claim : If $f(x)$ is a convex function, then: $$ E[f(X)] \\ge f(E[X]) $$ Proof : Taylor's theorem gives us: $$ \\exists\\ c : f(x) = f(\\mu) + f'(\\mu)(x - \\mu) + \\frac{f''(c)(x-\\mu)^2}{2} $$ Since $f(x)$ is concave, we know: $$ f(\\mu) + f'(\\mu)(x - \\mu) + \\frac{f''(c)(x-\\mu)^2}{2} \\ge f(\\mu) + f'(\\mu)(x-\\mu) $$ This gives us: $$ E[f(X)] \\ge E[ f(\\mu) + f'(\\mu)(X - \\mu) ] $$ Choose $ \\mu = E[X] $: $$ \\begin{align} E[ f(\\mu) + f'(\\mu)(X-\\mu) ] & = E[ f(E[X]) + f'(E[X])(X - E[X]) ] \\\\ & = E[ f( E[X] ) ] + f'(E[X])(E[X] - E[E[X]]) \\\\ & = f(E[X]) + 0 \\\\ \\end{align} $$ $$ \\to E[f(X)] \\ge f(E[X]) $$ Markov's Inequality Claim : $$ X \\ge 0, a > 0 $$ $$ \\Pr \\{ X \\ge a \\} \\le \\frac{E[X]}{a} $$ Proof : Since $X \\ge 0$ and $a > 0$: $$ \\begin{align} E[X] & = \\int_0^{\\infty} t\\ p_X(t) dt \\\\ & = \\int_0^{a} t\\ p_X(t) dt + \\int_a^{\\infty} t\\ p_X(t) dt \\\\ & \\ge \\int_{a}^{\\infty} t\\ p_X(t) dt \\\\ & \\ge \\int_{a}^{\\infty} a\\ p_X(t) dt \\\\ & = a \\int_{a}^{\\infty} p_X(t) dt \\\\ & = a \\Pr\\{ X \\ge a \\} \\\\ \\end{align} $$ $a > 0$, so we can divide: $$ \\to \\Pr\\{X \\ge a \\} \\le \\frac{E[X]}{a} $$ Chebyshev's Inequality Claim : $$ a > 0 $$ $$ \\Pr\\{|X - E[X]| \\ge a \\} \\le \\frac{ \\mathrm{Var}[X] }{a^2} $$ Proof : $$ \\begin{align} \\Pr\\{ |X - E[X]| \\ge a \\} & = \\Pr\\{ (X - E[X])^2 \\ge a^2 \\} \\\\ & \\le \\frac{E[ (X-E[X])^2 ]}{a^2} \\\\ & = \\frac{\\mathrm{Var}[X]}{a^2} \\end{align} $$ By Markov's and the definition of variance. Chernoff Bound $$ X \\ge 0, a > 0 $$ $$ \\Pr\\{ X \\ge a \\} = \\Pr\\{ e^{tX} \\ge e^{ta} \\} \\le \\frac{E[e^{tX}]}{e^{ta}} $$ $$ \\Pr\\{ X \\ge a \\} \\le \\min_{t>0} \\frac{E[e^{tX}]}{e^{ta}} $$ This can be seen by a straight forward application of Markov's inequality. The parameter $t$ can be chosen to taste. Generalized extreme value distribution (GEV) or Fisher Tippett Gnedenko theorem: $$ \\begin{align} X_0, X_1, \\cdots, X_{n-1} & \\ \\ \\ \\text{ i.i.d. RVs} \\\\ \\lim_{n \\to \\infty} P( \\frac{max(X_0, X_1, \\cdots, X_{n-1}) - b_n}{a_n} \\le x) & \\ \\ = G(x) \\\\ G_{\\gamma,a,b}(x) = \\exp( -(1 + (\\frac{x-b}{a})\\gamma)^{-\\frac{1}{\\gamma}}), \\ \\ \\ \\ & 1 + (\\frac{x-b}{a}) \\gamma > 0 \\end{align} $$ Where $G_{\\gamma,a,b}(x)$ above is the general form of the special cases of Gumbel, Frechet and the Weibull family of distributions. wiki 2018-08-04","title":"Probability Notes"},{"location":"Probability-Notes.html#probability-notes","text":"By convention: $$ E^n[X] \\stackrel{def}{=} (E[X])^n $$","title":"Probability Notes"},{"location":"Probability-Notes.html#independence-of-expectation-finite","text":"Claim : $$ E[ \\sum_{k=0}^{n-1} X_k ] = \\sum_{k=0}^{n-1} E[X_k] $$ Proof : $$ \\begin{align} E[ X + Y ] & = \\int \\int (s + t) \\Pr\\{ X = s \\ \\& \\ Y = t \\} \\ ds \\ dt \\\\ & = \\int \\int s \\Pr \\{ X = s \\ \\& \\ Y = t \\} \\ ds \\ dt + \\int \\int t \\Pr \\{ X = s \\ \\& \\ Y = t \\} \\ ds \\ dt \\\\ & = \\int \\int s \\Pr \\{ X = s \\ \\& \\ Y = t \\} \\ dt \\ ds + \\int \\int t \\Pr \\{ X = s \\ \\& \\ Y = t \\} \\ ds \\ dt \\\\ & = \\int s \\Pr \\{ X = s \\} \\ ds + \\int t \\Pr \\{ Y = t \\} \\ dt \\\\ & = E[X] + E[Y] \\end{align} $$ Induction can be used to extend to the general case: $$ E[ \\sum_{k=0}^{n-1} X_k ] = \\sum_{k=0}^{n-1} E[X_k] $$","title":"Independence of Expectation (finite)"},{"location":"Probability-Notes.html#bayes-theorem","text":"$$ \\Pr\\{ A | B \\} = \\frac{ \\Pr\\{ A \\& B \\} }{ \\Pr\\{ B \\} } $$ $$ \\Pr\\{ B | A \\} = \\frac{ \\Pr\\{ A \\& B \\} }{ \\Pr\\{ A \\} } $$ $$ \\Pr\\{ A | B \\} = \\frac{ \\Pr\\{ B | A \\} \\Pr\\{ A \\} }{ \\Pr\\{ B \\} } $$","title":"Bayes' Theorem"},{"location":"Probability-Notes.html#variance","text":"$$ \\mathrm{Var}[X] \\stackrel{def}{=} E[(X - E[X])^2] = E[X^2] - (E[X])^2 $$","title":"Variance"},{"location":"Probability-Notes.html#moment-generating-functions","text":"$$ M_X(t) \\stackrel{def}{=} E[ e^{t X} ] = \\sum_{k=0}^{\\infty} \\frac{t^k E[X^k]}{k!} $$ If $X$ and $Y$ and independent random variables, then: $$ M_{X + Y}(t) = E[ e^{t(X + Y)} ] = E[ e^{tX} e^{tY} ] = M_X(t) \\cdot M_Y(t) $$ $$ \\begin{align} \\frac{d^n}{dt} M_X(t) & = \\frac{d^{(n)}}{dt} ( \\sum_{k=0}^{\\infty} \\frac{t^n E[X^n]}{k!} ) \\\\ & = \\sum_{k=n} \\frac{t^{k-n} E[X^k]}{(k-n)!} \\\\ \\to \\frac{d^n}{dt} M_X(0) & = E[X^n] \\end{align} $$","title":"Moment Generating Functions"},{"location":"Probability-Notes.html#characteristic-function","text":"$$ \\varphi_X(t) = E[ e^{itX} ] $$ Not all r.v.s have moment generating functions but all r.v.s have a characteristic function. If the moment generating function exists, then: $$ \\varphi_X(-it) = M_X(t) $$","title":"Characteristic Function"},{"location":"Probability-Notes.html#jensens-inequality","text":"Claim : If $f(x)$ is a convex function, then: $$ E[f(X)] \\ge f(E[X]) $$ Proof : Taylor's theorem gives us: $$ \\exists\\ c : f(x) = f(\\mu) + f'(\\mu)(x - \\mu) + \\frac{f''(c)(x-\\mu)^2}{2} $$ Since $f(x)$ is concave, we know: $$ f(\\mu) + f'(\\mu)(x - \\mu) + \\frac{f''(c)(x-\\mu)^2}{2} \\ge f(\\mu) + f'(\\mu)(x-\\mu) $$ This gives us: $$ E[f(X)] \\ge E[ f(\\mu) + f'(\\mu)(X - \\mu) ] $$ Choose $ \\mu = E[X] $: $$ \\begin{align} E[ f(\\mu) + f'(\\mu)(X-\\mu) ] & = E[ f(E[X]) + f'(E[X])(X - E[X]) ] \\\\ & = E[ f( E[X] ) ] + f'(E[X])(E[X] - E[E[X]]) \\\\ & = f(E[X]) + 0 \\\\ \\end{align} $$ $$ \\to E[f(X)] \\ge f(E[X]) $$","title":"Jensen's Inequality"},{"location":"Probability-Notes.html#markovs-inequality","text":"Claim : $$ X \\ge 0, a > 0 $$ $$ \\Pr \\{ X \\ge a \\} \\le \\frac{E[X]}{a} $$ Proof : Since $X \\ge 0$ and $a > 0$: $$ \\begin{align} E[X] & = \\int_0^{\\infty} t\\ p_X(t) dt \\\\ & = \\int_0^{a} t\\ p_X(t) dt + \\int_a^{\\infty} t\\ p_X(t) dt \\\\ & \\ge \\int_{a}^{\\infty} t\\ p_X(t) dt \\\\ & \\ge \\int_{a}^{\\infty} a\\ p_X(t) dt \\\\ & = a \\int_{a}^{\\infty} p_X(t) dt \\\\ & = a \\Pr\\{ X \\ge a \\} \\\\ \\end{align} $$ $a > 0$, so we can divide: $$ \\to \\Pr\\{X \\ge a \\} \\le \\frac{E[X]}{a} $$","title":"Markov's Inequality"},{"location":"Probability-Notes.html#chebyshevs-inequality","text":"Claim : $$ a > 0 $$ $$ \\Pr\\{|X - E[X]| \\ge a \\} \\le \\frac{ \\mathrm{Var}[X] }{a^2} $$ Proof : $$ \\begin{align} \\Pr\\{ |X - E[X]| \\ge a \\} & = \\Pr\\{ (X - E[X])^2 \\ge a^2 \\} \\\\ & \\le \\frac{E[ (X-E[X])^2 ]}{a^2} \\\\ & = \\frac{\\mathrm{Var}[X]}{a^2} \\end{align} $$ By Markov's and the definition of variance.","title":"Chebyshev's Inequality"},{"location":"Probability-Notes.html#chernoff-bound","text":"$$ X \\ge 0, a > 0 $$ $$ \\Pr\\{ X \\ge a \\} = \\Pr\\{ e^{tX} \\ge e^{ta} \\} \\le \\frac{E[e^{tX}]}{e^{ta}} $$ $$ \\Pr\\{ X \\ge a \\} \\le \\min_{t>0} \\frac{E[e^{tX}]}{e^{ta}} $$ This can be seen by a straight forward application of Markov's inequality. The parameter $t$ can be chosen to taste. Generalized extreme value distribution (GEV) or Fisher Tippett Gnedenko theorem: $$ \\begin{align} X_0, X_1, \\cdots, X_{n-1} & \\ \\ \\ \\text{ i.i.d. RVs} \\\\ \\lim_{n \\to \\infty} P( \\frac{max(X_0, X_1, \\cdots, X_{n-1}) - b_n}{a_n} \\le x) & \\ \\ = G(x) \\\\ G_{\\gamma,a,b}(x) = \\exp( -(1 + (\\frac{x-b}{a})\\gamma)^{-\\frac{1}{\\gamma}}), \\ \\ \\ \\ & 1 + (\\frac{x-b}{a}) \\gamma > 0 \\end{align} $$ Where $G_{\\gamma,a,b}(x)$ above is the general form of the special cases of Gumbel, Frechet and the Weibull family of distributions. wiki","title":"Chernoff Bound"},{"location":"Probability-Notes.html#2018-08-04","text":"","title":"2018-08-04"},{"location":"Project-Organization.html","text":"Project Organization These are notes on \"best practices\" for Git project organization. Directory Structure File or Directory Description Misc src/ Source files. dist/ or bin/ Should remain empty in repo. Populated on compilation. tests/ Test suite. examples/ Example usage README.md Project description LICENSE License file for source .gitignore Git ignore file. Description src/ The source files of the project. dist/ The destination directory for compiled source files. This directory should remain empty in the main Git repo. tests/ The test suite to make sure the running code passes testing. examples/ Example usage of your program on small datasets (where applicable). README.md A description of the project. This should contain the following, preferably in this order: Screenshot Description of what the project is A \"quick start\" section Motivation for why the project exists How to compile (if applicable) How to run Example usage The license it's under, even if you have a LICENSE file. If there are multiple licenses then describe what each portion of the project falls under which license or where to find that information out. Credits (if applicable) References (if applicable) If a screenshot isn't appropriate for the project (say it's a simple command line program) then either a block of test with a sample run or an arbitrary picture should be used. If it's unclear what to use as a screenshot, use a free/libre licensed cat picture. LICENSE The license of the software. If there are multiple licenses, some options are to make a file per license used with some description either in the file or outside, as to which files in the project fall under which license or to concatenate all licenses into a single file. .gitignore The files for Git to ignore. Some common options are : *~ - ignore vi auto save files *.swp - ignore vi file lock files Language Dependent Files JavaScript package.json - dependencies for your npm package bower.json - front end dependencies for your JavaScript package C/C++ configure.ac Makefile.am configure Makefile.in References \"Maintaining an Open Source Project: Project Organization\" by Jacob Wenger 2017-08-05","title":"Project Organization"},{"location":"Project-Organization.html#project-organization","text":"These are notes on \"best practices\" for Git project organization.","title":"Project Organization"},{"location":"Project-Organization.html#directory-structure","text":"File or Directory Description Misc src/ Source files. dist/ or bin/ Should remain empty in repo. Populated on compilation. tests/ Test suite. examples/ Example usage README.md Project description LICENSE License file for source .gitignore Git ignore file.","title":"Directory Structure"},{"location":"Project-Organization.html#description","text":"","title":"Description"},{"location":"Project-Organization.html#src","text":"The source files of the project.","title":"src/"},{"location":"Project-Organization.html#dist","text":"The destination directory for compiled source files. This directory should remain empty in the main Git repo.","title":"dist/"},{"location":"Project-Organization.html#tests","text":"The test suite to make sure the running code passes testing.","title":"tests/"},{"location":"Project-Organization.html#examples","text":"Example usage of your program on small datasets (where applicable).","title":"examples/"},{"location":"Project-Organization.html#readmemd","text":"A description of the project. This should contain the following, preferably in this order: Screenshot Description of what the project is A \"quick start\" section Motivation for why the project exists How to compile (if applicable) How to run Example usage The license it's under, even if you have a LICENSE file. If there are multiple licenses then describe what each portion of the project falls under which license or where to find that information out. Credits (if applicable) References (if applicable) If a screenshot isn't appropriate for the project (say it's a simple command line program) then either a block of test with a sample run or an arbitrary picture should be used. If it's unclear what to use as a screenshot, use a free/libre licensed cat picture.","title":"README.md"},{"location":"Project-Organization.html#license","text":"The license of the software. If there are multiple licenses, some options are to make a file per license used with some description either in the file or outside, as to which files in the project fall under which license or to concatenate all licenses into a single file.","title":"LICENSE"},{"location":"Project-Organization.html#gitignore","text":"The files for Git to ignore. Some common options are : *~ - ignore vi auto save files *.swp - ignore vi file lock files","title":".gitignore"},{"location":"Project-Organization.html#language-dependent-files","text":"","title":"Language Dependent Files"},{"location":"Project-Organization.html#javascript","text":"package.json - dependencies for your npm package bower.json - front end dependencies for your JavaScript package","title":"JavaScript"},{"location":"Project-Organization.html#cc","text":"configure.ac Makefile.am configure Makefile.in","title":"C/C++"},{"location":"Project-Organization.html#references","text":"\"Maintaining an Open Source Project: Project Organization\" by Jacob Wenger","title":"References"},{"location":"Project-Organization.html#2017-08-05","text":"","title":"2017-08-05"},{"location":"Puzzle-Ribbon-Tiles.html","text":"Consider an $n \\times n$ grid tiled by ribbon tiles of length $n$, where a ribbon tile of length $n$ is defined to be a contiguous path with only 'right' or 'up' moves. Here is the problem statement: The puzzle asks for a straight forward proof that there are $n!$ configuration of these ribbon tiles. Here is one proof: Rotate the grid 45 degrees so that the first ribbon starts at the bottom diamond and all ribbons have a \"left\" and \"right\" choice if they can be extended. The first position has only one option for a ribbon start. The next two positions have a choice of either being extended into by the first ribbon or starting a new ribbon. At each subsequent layer below the midpoint of the rotated grid, there will be one open slot with all other ribbon tiles occupying the other positions. Past the midpoint, all choices will be forced until the grid is completely filled. Since at each height, $h$, below the midpoint of the rotated grid there are $h$ choices to start a new ribbon, there are $n!$ different tilings. This puzzle is credited to Chris Moore but seen in \"Handbook of Enumerative Combinatorics\" by Miklos Bona. Personally seen at tw 2023-03-31","title":"Puzzle Ribbon Tiles"},{"location":"Puzzle-Ribbon-Tiles.html#2023-03-31","text":"","title":"2023-03-31"},{"location":"Python-Regex-Cheatsheet.html","text":"Python Regex Cheatsheet import re ... # match only beginning of line # re.match(pat, s) # match anywhere # re.search(pat, s) Match a Real ^\\s*([-]?(\\d+|\\d+\\.\\d+|\\.\\d+|\\d+\\.))\\s*$ import re v = ['1', '-5', '0', '.1', '-.7', '1.2', '-3.4', '-10.', '91.'] x = [' a', 'b', 'c'] for _v in v: m = re.search('^\\s*([-]?(\\d+|\\d+\\.\\d+|\\.\\d+|\\d+\\.))\\s*$', _v) if m: print \"matched:\", m.group(1) for _x in x: m = re.search('^\\s*([-]?(\\d+|\\d+\\.\\d+|\\.\\d+|\\d+\\.))\\s*$', _x) if m: print \"matched??\", m.group(1) Match an Integer ^\\s*([-]?\\d+)\\s*$ import re v = ['1', '-5', '0', '.1', '-.7', '1.2', '-3.4', '-10.', '91.'] x = [' a', 'b', 'c'] for _v in v: m = re.search('^\\s*([-]?\\d+)\\s*$', _v) if m: print \"matched:\", m.group(1) for _x in x: m = re.search('^\\s*([-]?\\d+)\\s*$', _x) if m: print \"matched??\", m.group(1) 2020-04-18","title":"Python Regex Cheatsheet"},{"location":"Python-Regex-Cheatsheet.html#python-regex-cheatsheet","text":"import re ... # match only beginning of line # re.match(pat, s) # match anywhere # re.search(pat, s)","title":"Python Regex Cheatsheet"},{"location":"Python-Regex-Cheatsheet.html#match-a-real","text":"^\\s*([-]?(\\d+|\\d+\\.\\d+|\\.\\d+|\\d+\\.))\\s*$ import re v = ['1', '-5', '0', '.1', '-.7', '1.2', '-3.4', '-10.', '91.'] x = [' a', 'b', 'c'] for _v in v: m = re.search('^\\s*([-]?(\\d+|\\d+\\.\\d+|\\.\\d+|\\d+\\.))\\s*$', _v) if m: print \"matched:\", m.group(1) for _x in x: m = re.search('^\\s*([-]?(\\d+|\\d+\\.\\d+|\\.\\d+|\\d+\\.))\\s*$', _x) if m: print \"matched??\", m.group(1)","title":"Match a Real"},{"location":"Python-Regex-Cheatsheet.html#match-an-integer","text":"^\\s*([-]?\\d+)\\s*$ import re v = ['1', '-5', '0', '.1', '-.7', '1.2', '-3.4', '-10.', '91.'] x = [' a', 'b', 'c'] for _v in v: m = re.search('^\\s*([-]?\\d+)\\s*$', _v) if m: print \"matched:\", m.group(1) for _x in x: m = re.search('^\\s*([-]?\\d+)\\s*$', _x) if m: print \"matched??\", m.group(1)","title":"Match an Integer"},{"location":"Python-Regex-Cheatsheet.html#2020-04-18","text":"","title":"2020-04-18"},{"location":"Real-Analysis-In-A-Nutshell.html","text":"Real Analysis in a Nutshell A sequence of numbers $$( x_0, x_1, x_2, \\dots )$$ fits the Cauchy cirterion if: $$ \\forall n, \\ \\exists m, \\ s.t. |x_j - x_k| \\le \\frac{1}{n} \\ if j,k \\ge m $$ If a sequence of numbers fits the Cauchy criterion, it is a Cauchy sequence . 2019-09-24","title":"Real Analysis In A Nutshell"},{"location":"Real-Analysis-In-A-Nutshell.html#real-analysis-in-a-nutshell","text":"A sequence of numbers $$( x_0, x_1, x_2, \\dots )$$ fits the Cauchy cirterion if: $$ \\forall n, \\ \\exists m, \\ s.t. |x_j - x_k| \\le \\frac{1}{n} \\ if j,k \\ge m $$ If a sequence of numbers fits the Cauchy criterion, it is a Cauchy sequence .","title":"Real Analysis in a Nutshell"},{"location":"Real-Analysis-In-A-Nutshell.html#2019-09-24","text":"","title":"2019-09-24"},{"location":"Replica-Simulated-Annealing.html","text":"Replicated Simulated Annealing $$ \\begin{array}{cl} y \\propto \\frac{1}{T} \\ \\ \\ & \\beta, \\gamma \\in \\mathbb{R} \\ \\sigma \\in { \\sigma_0, \\sigma_1, \\cdots, \\sigma_{N-1} } & S = { q_0, q_1, \\cdots, q_{M-1} }^N \\ P( \\sigma ; \\beta , y, \\gamma ) & = (\\frac{1}{Z(\\beta,y,\\gamma) } ) \\cdot e^{y \\Phi(\\sigma,\\beta,\\gamma)} \\ \\Phi(\\sigma,\\beta,\\gamma) & = \\ln( \\sum_{\\sigma'} e^{-\\beta E(\\sigma') - \\gamma d(\\sigma,\\sigma')}) \\ Z(y,\\beta,\\gamma) & = \\sum _ {\\sigma \\in S} \\exp( y \\Phi _ {\\beta, \\gamma} ( \\sigma ) ) \\end{array} $$ In the above, $y$ is formally inverse temperature ($\\frac{1}{T}$), $\\Phi(\\cdots)$ is the local free entropy and $\\sigma$ is a configuration of $N$ cells, each of which can take on one of $M$ values, with $E(\\cdot)$ being the energy function of a particular configuration. The distance function, $d(\\cdot,\\cdot)$, should be monotonically increasing. If we take $y \\ge 2, y \\in \\mathbb{Z} _ { + }$, then $$ \\begin{array}{cl} Z(y,\\beta,\\gamma) & = \\sum _ { \\sigma \\in S } \\exp( y \\Phi _ { \\beta, \\gamma } ( \\sigma ) ) \\ & = \\sum _ { \\sigma \\in S } \\exp( \\sum _ { a=1 } ^ {y} \\Phi _ { \\beta, \\gamma } ( \\sigma ) ) \\ & = \\sum _ { \\sigma \\in S } \\prod _ {a=1} ^ y \\sum _ { \\sigma^a } \\exp( - \\beta E( \\sigma^a ) - \\gamma d( \\sigma, \\sigma^a ) ) \\end{array} $$ Define: TO BE CONTINUED... References Remarks on RSA Limits of SA on hard inference 2023-03-18","title":"Replica Simulated Annealing"},{"location":"Replica-Simulated-Annealing.html#replicated-simulated-annealing","text":"$$ \\begin{array}{cl} y \\propto \\frac{1}{T} \\ \\ \\ & \\beta, \\gamma \\in \\mathbb{R} \\ \\sigma \\in { \\sigma_0, \\sigma_1, \\cdots, \\sigma_{N-1} } & S = { q_0, q_1, \\cdots, q_{M-1} }^N \\ P( \\sigma ; \\beta , y, \\gamma ) & = (\\frac{1}{Z(\\beta,y,\\gamma) } ) \\cdot e^{y \\Phi(\\sigma,\\beta,\\gamma)} \\ \\Phi(\\sigma,\\beta,\\gamma) & = \\ln( \\sum_{\\sigma'} e^{-\\beta E(\\sigma') - \\gamma d(\\sigma,\\sigma')}) \\ Z(y,\\beta,\\gamma) & = \\sum _ {\\sigma \\in S} \\exp( y \\Phi _ {\\beta, \\gamma} ( \\sigma ) ) \\end{array} $$ In the above, $y$ is formally inverse temperature ($\\frac{1}{T}$), $\\Phi(\\cdots)$ is the local free entropy and $\\sigma$ is a configuration of $N$ cells, each of which can take on one of $M$ values, with $E(\\cdot)$ being the energy function of a particular configuration. The distance function, $d(\\cdot,\\cdot)$, should be monotonically increasing. If we take $y \\ge 2, y \\in \\mathbb{Z} _ { + }$, then $$ \\begin{array}{cl} Z(y,\\beta,\\gamma) & = \\sum _ { \\sigma \\in S } \\exp( y \\Phi _ { \\beta, \\gamma } ( \\sigma ) ) \\ & = \\sum _ { \\sigma \\in S } \\exp( \\sum _ { a=1 } ^ {y} \\Phi _ { \\beta, \\gamma } ( \\sigma ) ) \\ & = \\sum _ { \\sigma \\in S } \\prod _ {a=1} ^ y \\sum _ { \\sigma^a } \\exp( - \\beta E( \\sigma^a ) - \\gamma d( \\sigma, \\sigma^a ) ) \\end{array} $$ Define: TO BE CONTINUED...","title":"Replicated Simulated Annealing"},{"location":"Replica-Simulated-Annealing.html#references","text":"Remarks on RSA Limits of SA on hard inference","title":"References"},{"location":"Replica-Simulated-Annealing.html#2023-03-18","text":"","title":"2023-03-18"},{"location":"SSH-Recipes.html","text":"SSH Recipes Command Description ssh -L local:local.host:remote remote.host Tunnel port local on local.host to remote port on remote.host ssh -R remote:local.host:local remote.host Tunnel port remote on remote.host to local port on local.host Example Description ssh -L 8080:localhost:80 user@remote.host Send port 8080 connections on localhost to port 80 on remote.host ssh -R 2222:localhost:22 user@remote.host Send port 2222 connections on remote.host to port 22 locally (ssh redirection from remote to local) Command Description ssh -i key -D local -f -C -q -N user@remote.host Open an ssh tunnel with SOCKSv5 proxy enabled ( -D socks, -f fork, -C compress, -q quiet, -N no command) Example Description ssh -i ~/.ssh/priv.key -D 8080 -f -C -q -N user@remote.host Send localhost port 8080 out through remote.host Browser should have a Configure Proxy Access somewhere which should be set to SOCKS v5 , localhost / 127.0.0.1 as host and the appropriate port ( 8080 ). 2019-09-24","title":"SSH Recipes"},{"location":"SSH-Recipes.html#ssh-recipes","text":"Command Description ssh -L local:local.host:remote remote.host Tunnel port local on local.host to remote port on remote.host ssh -R remote:local.host:local remote.host Tunnel port remote on remote.host to local port on local.host Example Description ssh -L 8080:localhost:80 user@remote.host Send port 8080 connections on localhost to port 80 on remote.host ssh -R 2222:localhost:22 user@remote.host Send port 2222 connections on remote.host to port 22 locally (ssh redirection from remote to local) Command Description ssh -i key -D local -f -C -q -N user@remote.host Open an ssh tunnel with SOCKSv5 proxy enabled ( -D socks, -f fork, -C compress, -q quiet, -N no command) Example Description ssh -i ~/.ssh/priv.key -D 8080 -f -C -q -N user@remote.host Send localhost port 8080 out through remote.host Browser should have a Configure Proxy Access somewhere which should be set to SOCKS v5 , localhost / 127.0.0.1 as host and the appropriate port ( 8080 ).","title":"SSH Recipes"},{"location":"SSH-Recipes.html#2019-09-24","text":"","title":"2019-09-24"},{"location":"Screenshots-Screencasts-Animated-Gifs.html","text":"Screencasts I've found kazam to work very well. $ sudo apt-get install kazam $ kazam Screenshots Gimp File->Create->Screenshot ImageMagick $ import -window root screenshot.png Animated Gifs ImageMagick $ convert -delay 1 -layers optimize inp*.png anim.gif Quick and dirty way to create animated Gifs from a window $ winid=`xwininfo | grep -o 'Window id: [^ ]* ' | cut -f3 -d' '` ; echo $winid Click on the window in question and make sure the portion of the window you want to record is exposed. $ for x in {1..10} do import -window $winid out$x.png sleep 0.1 done Once the out{1..10}.png files are created, coalesce them into an animated Gif: $ convert -delay 1 -layers optimize out*.png anim.gif Using ImageMagick is sometimes slow. Using kazam (and only capturing a window) will create an mp4 file that can be exploded: $ ffmpeg -i inp.mp4 pic%03d.jpg $ for x in `ls pic*.jpg` do mogrify -crop 1000x700+0+70 $x done $ convert -layers optimize pic*.jpg out.gif Where mogrify alters the image file in place and -crop crops the top pixels (to get rid of the tabs and URL if it's a web browser, say). Sometimes ImageMagick has a lot of issues when trying to create an animated Gif, especially if there are many frames. Instead, you can use ffmpeg directly (see SO ): $ palette=\"/tmp/palette.png\" $ filters=\"fps=15,scale=320:-1:flags=lanczos\" $ ffmpeg -i input.mp4 -vf \"$filters,palettegen\" -y $palette $ ffmpeg -i input.mp4 -i $palette -lavfi \"$filters [x]; [x][1:v] paletteuse\" -y output.gif ffmpeg can apparently also directly create (large) animated Gifs: $ ffmpeg -i input.mp4 large_output.gif To reduce the large_output.gif , gifsicle can be used: $ gifsicle -O1 --loop large_output.gif > slim_output.gif though gifsicle looks to have some problems compressing well. To take a sub range of pictures from gifsicle , you can do something like: $ gifsicle -U inp.gif '#50-73' > out50-73.gif Where the # specifies the frame range and the -U (unoptimize) option is needed to get rid of artifacts that appear to happen when selecting from a mid range of frames. recommended workflow capture with kazam use the above script to convert from .mp4 to animated Gif with ffmpeg 2015-11-01","title":"Screenshots Screencasts Animated Gifs"},{"location":"Screenshots-Screencasts-Animated-Gifs.html#screencasts","text":"I've found kazam to work very well. $ sudo apt-get install kazam $ kazam","title":"Screencasts"},{"location":"Screenshots-Screencasts-Animated-Gifs.html#screenshots","text":"","title":"Screenshots"},{"location":"Screenshots-Screencasts-Animated-Gifs.html#gimp","text":"File->Create->Screenshot","title":"Gimp"},{"location":"Screenshots-Screencasts-Animated-Gifs.html#imagemagick","text":"$ import -window root screenshot.png","title":"ImageMagick"},{"location":"Screenshots-Screencasts-Animated-Gifs.html#animated-gifs","text":"","title":"Animated Gifs"},{"location":"Screenshots-Screencasts-Animated-Gifs.html#imagemagick_1","text":"$ convert -delay 1 -layers optimize inp*.png anim.gif","title":"ImageMagick"},{"location":"Screenshots-Screencasts-Animated-Gifs.html#quick-and-dirty-way-to-create-animated-gifs-from-a-window","text":"$ winid=`xwininfo | grep -o 'Window id: [^ ]* ' | cut -f3 -d' '` ; echo $winid Click on the window in question and make sure the portion of the window you want to record is exposed. $ for x in {1..10} do import -window $winid out$x.png sleep 0.1 done Once the out{1..10}.png files are created, coalesce them into an animated Gif: $ convert -delay 1 -layers optimize out*.png anim.gif Using ImageMagick is sometimes slow. Using kazam (and only capturing a window) will create an mp4 file that can be exploded: $ ffmpeg -i inp.mp4 pic%03d.jpg $ for x in `ls pic*.jpg` do mogrify -crop 1000x700+0+70 $x done $ convert -layers optimize pic*.jpg out.gif Where mogrify alters the image file in place and -crop crops the top pixels (to get rid of the tabs and URL if it's a web browser, say). Sometimes ImageMagick has a lot of issues when trying to create an animated Gif, especially if there are many frames. Instead, you can use ffmpeg directly (see SO ): $ palette=\"/tmp/palette.png\" $ filters=\"fps=15,scale=320:-1:flags=lanczos\" $ ffmpeg -i input.mp4 -vf \"$filters,palettegen\" -y $palette $ ffmpeg -i input.mp4 -i $palette -lavfi \"$filters [x]; [x][1:v] paletteuse\" -y output.gif ffmpeg can apparently also directly create (large) animated Gifs: $ ffmpeg -i input.mp4 large_output.gif To reduce the large_output.gif , gifsicle can be used: $ gifsicle -O1 --loop large_output.gif > slim_output.gif though gifsicle looks to have some problems compressing well. To take a sub range of pictures from gifsicle , you can do something like: $ gifsicle -U inp.gif '#50-73' > out50-73.gif Where the # specifies the frame range and the -U (unoptimize) option is needed to get rid of artifacts that appear to happen when selecting from a mid range of frames. recommended workflow capture with kazam use the above script to convert from .mp4 to animated Gif with ffmpeg","title":"Quick and dirty way to create animated Gifs from a window"},{"location":"Screenshots-Screencasts-Animated-Gifs.html#2015-11-01","text":"","title":"2015-11-01"},{"location":"Shannon-Entropy.html","text":"Shannon Entropy Claude E. Shannon's book, \"The Mathematical Theory of Communication\", is very accessible. The main points about how entropy is defined and derived along with the \"Fundamental Theorem for a Discrete Channel With Noise\" is digested below. Entropy Entropy can be defined as the number of bits it takes to describe a system. Given $n$ symbols, each occurring with probability $p_k$ for $k \\in (0, 1, \\dots, n-1)$, we ask how many configurations are there for a very long message, say of $T$ transmitted symbols. For the sake of clarity, we assume $T$ large and $T \\cdot p_k \\cdot n$ is integral. The number of ways to arrange $T \\cdot n$ elements comprised of $n$ symbols each occurring with $T \\cdot p_k \\cdot n$ frequency is the multinomial: $$ { T \\cdot n \\choose (T \\cdot p_0 \\cdot n), (T \\cdot p_1 \\cdot n), \\dots, (T \\cdot p_{n-1} \\cdot n) } $$ $$ = \\frac{(T \\cdot n)!}{\\prod_{k=0}^{n-1} (T \\cdot p_k \\cdot n)!} $$ If we concern ourselves with the bits it takes to represent the total number of configurations, we find (where $\\lg(\\cdot) = \\log_2(\\cdot)$): $$ \\lg( \\frac{(T \\cdot n)!}{\\prod_{k=0}^{n-1} (T \\cdot p_k \\cdot n)!} ) $$ $$ = \\lg( (T \\cdot n)! ) - \\sum_{k=0}^{n-1} \\lg( (T \\cdot p_k \\cdot n)! ) $$ $$ \\approx (T \\cdot n) lg( T \\cdot n ) - (T \\cdot n) - \\sum_{k=0}^{n-1} [ (T \\cdot p_k \\cdot n) \\lg(T \\cdot p_k \\cdot n) - (T \\cdot p_k \\cdot n) ] $$ By definition, $\\sum_{k=0}^{n-1} p_k = 1$, we can reduce to find: $$ = - T \\sum_{k=0}^{n-1} p_k \\lg(p_k) $$ We define $H$ to be our entropy, the average number of bits needed to represent our system. Since the above is the total number of bits needed, we divide by $T$ to find the average: $$ H = - \\sum_{k=0}^{n-1} p_k \\lg(p_k) $$ Transmission Over a Noisy Channel If we transmit $H$ bits per symbol over a noisy line and assume each symbol's error over the line is independent, label the number of bits, whole or partial, that succumb to error as $r$. That is, of the $H$ bits per symbol, $r$ are 'eaten' by noise in the channel. Call the channel capacity $C = H - r$. This is the number of useful bits that remain after we take away the noise from the number of bits needed to encode symbols. As above, consider a long message of $T$ transmitted symbols. First allocate some bits for error correction and choose $S$ such that: $$ S < C = H - r $$ Further $$ S = C - \\eta = H - r - \\eta $$ Where the number of error correcting bits is just shy of $T \\cdot \\eta$. Choose codewords in the source representation so that there are $2^{T \\cdot S}$ codewords that sit in $2^{T \\cdot H}$ total configurations. Sent messages will be from the restricted set of codewords and has probability: $$ \\frac{2^{T \\cdot S}}{2^{T \\cdot H}} = 2^{T \\cdot (S - H)} $$ A received message of $T \\cdot H$ bits long will have $T \\cdot r$ corrupted by error. The number of possible source configurations that could have sent the received message is: $$ 2^{T \\cdot r} $$ The probability that there is another codeword in the $2^{T \\cdot r}$ number of theoretical sent messages, aside from the source codeword, is the probability that none of the other codewords are hit: $$ [ 1 - 2^{ T \\cdot (S - H) } ]^{ 2^{T \\cdot r} } $$ $$ = [ 1 - \\frac{2^{ -T \\cdot \\eta}}{2^{T \\cdot r}} ]^{2^{T \\cdot r}} $$ As $T$ becomes large: $$ \\approx e^{ -2^{ -T \\cdot \\eta } } $$ $$ = 1 - 2^{ -T \\cdot \\eta } + O( 2^{-2 \\cdot T \\cdot \\eta} ) $$ $$ \\approx 1 - 2^{ -T \\cdot \\eta } $$ Which approaches 0. So the chance of our transmitted encoded codeword being mistaken for another codeword is vanishingly small. As long as we choose $S$ to be less than the channel capacity $C$ and the message is long enough ($T$ is big enough) we have a low chance of a source codeword colliding after transmission with a channel error rate of $r$. 2017-06-12","title":"Shannon Entropy"},{"location":"Shannon-Entropy.html#shannon-entropy","text":"Claude E. Shannon's book, \"The Mathematical Theory of Communication\", is very accessible. The main points about how entropy is defined and derived along with the \"Fundamental Theorem for a Discrete Channel With Noise\" is digested below.","title":"Shannon Entropy"},{"location":"Shannon-Entropy.html#entropy","text":"Entropy can be defined as the number of bits it takes to describe a system. Given $n$ symbols, each occurring with probability $p_k$ for $k \\in (0, 1, \\dots, n-1)$, we ask how many configurations are there for a very long message, say of $T$ transmitted symbols. For the sake of clarity, we assume $T$ large and $T \\cdot p_k \\cdot n$ is integral. The number of ways to arrange $T \\cdot n$ elements comprised of $n$ symbols each occurring with $T \\cdot p_k \\cdot n$ frequency is the multinomial: $$ { T \\cdot n \\choose (T \\cdot p_0 \\cdot n), (T \\cdot p_1 \\cdot n), \\dots, (T \\cdot p_{n-1} \\cdot n) } $$ $$ = \\frac{(T \\cdot n)!}{\\prod_{k=0}^{n-1} (T \\cdot p_k \\cdot n)!} $$ If we concern ourselves with the bits it takes to represent the total number of configurations, we find (where $\\lg(\\cdot) = \\log_2(\\cdot)$): $$ \\lg( \\frac{(T \\cdot n)!}{\\prod_{k=0}^{n-1} (T \\cdot p_k \\cdot n)!} ) $$ $$ = \\lg( (T \\cdot n)! ) - \\sum_{k=0}^{n-1} \\lg( (T \\cdot p_k \\cdot n)! ) $$ $$ \\approx (T \\cdot n) lg( T \\cdot n ) - (T \\cdot n) - \\sum_{k=0}^{n-1} [ (T \\cdot p_k \\cdot n) \\lg(T \\cdot p_k \\cdot n) - (T \\cdot p_k \\cdot n) ] $$ By definition, $\\sum_{k=0}^{n-1} p_k = 1$, we can reduce to find: $$ = - T \\sum_{k=0}^{n-1} p_k \\lg(p_k) $$ We define $H$ to be our entropy, the average number of bits needed to represent our system. Since the above is the total number of bits needed, we divide by $T$ to find the average: $$ H = - \\sum_{k=0}^{n-1} p_k \\lg(p_k) $$","title":"Entropy"},{"location":"Shannon-Entropy.html#transmission-over-a-noisy-channel","text":"If we transmit $H$ bits per symbol over a noisy line and assume each symbol's error over the line is independent, label the number of bits, whole or partial, that succumb to error as $r$. That is, of the $H$ bits per symbol, $r$ are 'eaten' by noise in the channel. Call the channel capacity $C = H - r$. This is the number of useful bits that remain after we take away the noise from the number of bits needed to encode symbols. As above, consider a long message of $T$ transmitted symbols. First allocate some bits for error correction and choose $S$ such that: $$ S < C = H - r $$ Further $$ S = C - \\eta = H - r - \\eta $$ Where the number of error correcting bits is just shy of $T \\cdot \\eta$. Choose codewords in the source representation so that there are $2^{T \\cdot S}$ codewords that sit in $2^{T \\cdot H}$ total configurations. Sent messages will be from the restricted set of codewords and has probability: $$ \\frac{2^{T \\cdot S}}{2^{T \\cdot H}} = 2^{T \\cdot (S - H)} $$ A received message of $T \\cdot H$ bits long will have $T \\cdot r$ corrupted by error. The number of possible source configurations that could have sent the received message is: $$ 2^{T \\cdot r} $$ The probability that there is another codeword in the $2^{T \\cdot r}$ number of theoretical sent messages, aside from the source codeword, is the probability that none of the other codewords are hit: $$ [ 1 - 2^{ T \\cdot (S - H) } ]^{ 2^{T \\cdot r} } $$ $$ = [ 1 - \\frac{2^{ -T \\cdot \\eta}}{2^{T \\cdot r}} ]^{2^{T \\cdot r}} $$ As $T$ becomes large: $$ \\approx e^{ -2^{ -T \\cdot \\eta } } $$ $$ = 1 - 2^{ -T \\cdot \\eta } + O( 2^{-2 \\cdot T \\cdot \\eta} ) $$ $$ \\approx 1 - 2^{ -T \\cdot \\eta } $$ Which approaches 0. So the chance of our transmitted encoded codeword being mistaken for another codeword is vanishingly small. As long as we choose $S$ to be less than the channel capacity $C$ and the message is long enough ($T$ is big enough) we have a low chance of a source codeword colliding after transmission with a channel error rate of $r$.","title":"Transmission Over a Noisy Channel"},{"location":"Shannon-Entropy.html#2017-06-12","text":"","title":"2017-06-12"},{"location":"Simple-Inequalities.html","text":"Simple Inequalities $$ \\begin{align} x_0 \\le x \\le x_1 &, \\ \\ \\ x_0 , x, x_1 \\in \\mathbb{R} \\ f(x_0) \\ge g(x_0), & \\ \\ \\ f'(x) \\ge g'(x) \\ \\to & \\ \\ \\ f(x) \\ge g(x) \\end{align} $$ For (real), bounded, continuous functions of one variable, explicit formulas for the error term of the remainder on the Taylor series can be used to get bounds on functions. Let $f: \\mathbb{R} \\to \\mathbb{R}$ be $n+1$ differentiable over a closed interval between $x_0$ to $x$, then: $$ \\begin{array}{l} \\exists x _ , & x _ 0 \\le x _ \\le x , \\ f(x) & = [ {\\sum} _ { k=0 } ^ { n } \\frac { f ^ { (k) }( x_0 ) } { k! } (x-x_0) ^ { k } ] + \\frac { f ^ { (n+1) } ( x _ * ) } { (n+1)! } (x-x_0) ^ { n+1 } \\ \\end{array} $$ Where the last term is the Lagrange form of the mean value form of the remainder ( src ). By restricting to an interval where you know the remainder term is of one sign, you can get explicit inequalities ( mo ). Claim : $0 \\le x \\le 1 , 1 - x \\le e ^ { -x }$ $$ \\begin{array}{llr} e ^ { -x } & = 1 - x e ^ { x _ } \\ \\forall x _ , & 0 \\ge x _ \\ge 1, \\ & 0 \\ge e ^ { -x _ } \\le 1 \\ \\to & -e ^ { -x _ } \\ge -1 \\ \\to & -x e ^ { -x _ } \\ge -x & (x \\ge 0) \\ \\to & 1 - x e ^ { -x _ * } \\ge 1 - x \\ \\to & e ^ { -x } \\ge 1 - x \\ \\to & 1 - x \\le e ^ { -x } \\end{array} $$ Claim : $\\frac { \\pi } { 2 } \\ge x \\ge 0, x \\ge sin(x)$ $$ \\begin{array}{llr} \\sin(x) & = x - \\frac { x ^ 3 } { 3! } \\cos( x _ ) \\ \\forall x _ , & 0 \\le x _ \\le \\frac { \\pi } { 2 }, \\ & 0 \\le \\cos( x _ ) \\le 1 \\ \\to & \\frac { x ^ 3 } { 3! } \\cos( x _ ) \\ge 0 & (\\frac { \\pi } { 2 } \\ge x \\ge 0) \\ \\to & -\\frac { x ^ 3 } { 3! } \\cos( x _ ) \\le 0 \\ \\to & x -\\frac { x ^ 3 } { 3! } \\cos( x _ * ) \\le x \\ \\to & \\sin( x ) \\le x \\ \\to & x \\ge \\sin( x ) \\ \\end{array} $$ Claim : $0 < x < 1, \\ln(1+x) < x < -\\ln(1-x)$ ... $$ $$ \\begin{array}{llr} \\end{array} $$ $$ \\begin{array}{llr} n \\ge k & \\to & -n \\le -k \\ & \\to & nk - n \\le nk - k \\ & \\to & n(k - 1) \\le k(n - 1) \\ & \\to & \\frac{n}{k} \\le \\frac{n - 1}{k-1} \\ & \\to & \\frac{n}{k} \\le \\frac{ n - 1}{k-1} \\ & \\to & ( \\frac { n } { k } ) ^ k \\le {\\prod} ^ { k-1 } _ { j=0 } \\frac { n-j } { k-j } \\ & \\to & (\\frac{n}{k})^k \\le { n \\choose k } \\ \\end{array} $$ 2021-03-15","title":"Simple Inequalities"},{"location":"Simple-Inequalities.html#simple-inequalities","text":"$$ \\begin{align} x_0 \\le x \\le x_1 &, \\ \\ \\ x_0 , x, x_1 \\in \\mathbb{R} \\ f(x_0) \\ge g(x_0), & \\ \\ \\ f'(x) \\ge g'(x) \\ \\to & \\ \\ \\ f(x) \\ge g(x) \\end{align} $$ For (real), bounded, continuous functions of one variable, explicit formulas for the error term of the remainder on the Taylor series can be used to get bounds on functions. Let $f: \\mathbb{R} \\to \\mathbb{R}$ be $n+1$ differentiable over a closed interval between $x_0$ to $x$, then: $$ \\begin{array}{l} \\exists x _ , & x _ 0 \\le x _ \\le x , \\ f(x) & = [ {\\sum} _ { k=0 } ^ { n } \\frac { f ^ { (k) }( x_0 ) } { k! } (x-x_0) ^ { k } ] + \\frac { f ^ { (n+1) } ( x _ * ) } { (n+1)! } (x-x_0) ^ { n+1 } \\ \\end{array} $$ Where the last term is the Lagrange form of the mean value form of the remainder ( src ). By restricting to an interval where you know the remainder term is of one sign, you can get explicit inequalities ( mo ). Claim : $0 \\le x \\le 1 , 1 - x \\le e ^ { -x }$ $$ \\begin{array}{llr} e ^ { -x } & = 1 - x e ^ { x _ } \\ \\forall x _ , & 0 \\ge x _ \\ge 1, \\ & 0 \\ge e ^ { -x _ } \\le 1 \\ \\to & -e ^ { -x _ } \\ge -1 \\ \\to & -x e ^ { -x _ } \\ge -x & (x \\ge 0) \\ \\to & 1 - x e ^ { -x _ * } \\ge 1 - x \\ \\to & e ^ { -x } \\ge 1 - x \\ \\to & 1 - x \\le e ^ { -x } \\end{array} $$ Claim : $\\frac { \\pi } { 2 } \\ge x \\ge 0, x \\ge sin(x)$ $$ \\begin{array}{llr} \\sin(x) & = x - \\frac { x ^ 3 } { 3! } \\cos( x _ ) \\ \\forall x _ , & 0 \\le x _ \\le \\frac { \\pi } { 2 }, \\ & 0 \\le \\cos( x _ ) \\le 1 \\ \\to & \\frac { x ^ 3 } { 3! } \\cos( x _ ) \\ge 0 & (\\frac { \\pi } { 2 } \\ge x \\ge 0) \\ \\to & -\\frac { x ^ 3 } { 3! } \\cos( x _ ) \\le 0 \\ \\to & x -\\frac { x ^ 3 } { 3! } \\cos( x _ * ) \\le x \\ \\to & \\sin( x ) \\le x \\ \\to & x \\ge \\sin( x ) \\ \\end{array} $$ Claim : $0 < x < 1, \\ln(1+x) < x < -\\ln(1-x)$ ... $$ $$ \\begin{array}{llr} \\end{array} $$ $$ \\begin{array}{llr} n \\ge k & \\to & -n \\le -k \\ & \\to & nk - n \\le nk - k \\ & \\to & n(k - 1) \\le k(n - 1) \\ & \\to & \\frac{n}{k} \\le \\frac{n - 1}{k-1} \\ & \\to & \\frac{n}{k} \\le \\frac{ n - 1}{k-1} \\ & \\to & ( \\frac { n } { k } ) ^ k \\le {\\prod} ^ { k-1 } _ { j=0 } \\frac { n-j } { k-j } \\ & \\to & (\\frac{n}{k})^k \\le { n \\choose k } \\ \\end{array} $$","title":"Simple Inequalities"},{"location":"Simple-Inequalities.html#2021-03-15","text":"","title":"2021-03-15"},{"location":"Simple-Sum.html","text":"Simple Sums $$ 0 < p < 1, p \\in \\mathbb{R}$$ $$ \\begin{align} \\sum_{k=0}^{\\infty} p^k = \\frac{1}{1-p} \\end{align} $$ Proof: $$ \\begin{align} S & = \\sum_{k=0}^{\\infty} p^k \\end{align} $$ $$ \\begin{align} \\sum_{k=0}^{\\infty} p^k & = 1 + \\sum_{k=1}^{\\infty} p^k \\end{align} $$ $$ \\begin{align} p S &= \\sum_{k=0}^{\\infty} p^{k+1} \\\\ &= \\sum_{k=1}^{\\infty} p^k \\\\ \\end{align} $$ $$ \\begin{align} S - p S &= 1 \\\\ S &= \\frac{1}{1-p} \\\\ \\end{align} $$ $$ \\begin{align} \\sum_{k=0}^{s-1} p^k = \\frac{1-p^s}{1-p} \\\\ \\sum_{k=s}^{\\infty} p^k = \\frac{p^s}{1-p} \\end{align} $$ Proof: $$ \\begin{align} \\sum_{k=0}^{s-1} p^k &= \\sum_{k=0}^{\\infty} p^k - \\sum_{k=s}^{\\infty} p^k \\\\ &= \\sum_{k=0}^{\\infty} p^k - p^s \\sum_{k=0}^{\\infty} p^k \\\\ &= \\frac{1}{1-p} - \\frac{p^s}{1-p} \\\\ &= \\frac{1 - p^s}{1-p} \\\\ \\end{align} $$ $$ \\begin{align} \\sum_{k=s}^{\\infty} p^k &= p^s \\sum_{k=0}^{\\infty} p^k \\\\ &= \\frac{p^s}{1-p} \\\\ \\end{align} $$ $$ \\begin{align} \\sum_{k=0}^{\\infty} k p^k = \\frac{p}{(1-p)^2} \\end{align} $$ Proof: $$ \\begin{align} S' &= \\sum_{k=0}^{\\infty} k p^k \\\\ p S' &= \\sum_{k=0}^{\\infty} k p^{k+1} \\\\ &= \\sum_{k=1}^{\\infty} (k - 1) p^{k} \\\\ &= \\sum_{k=1} k p^k - \\sum_{k=1} p^k \\\\ &= S' - \\frac{p}{1-p} \\\\ p S' - S' &= - \\frac{p}{1-p} \\\\ S' (p - 1) &= - \\frac{p}{1-p} \\\\ S' (1 - p) &= \\frac{p}{1-p} \\\\ S' &= \\frac{p}{(1-p)^2} \\\\ \\end{align} $$ $$ \\begin{align} \\end{align} $$ 2018-05-25","title":"Simple Sum"},{"location":"Simple-Sum.html#simple-sums","text":"$$ 0 < p < 1, p \\in \\mathbb{R}$$ $$ \\begin{align} \\sum_{k=0}^{\\infty} p^k = \\frac{1}{1-p} \\end{align} $$ Proof: $$ \\begin{align} S & = \\sum_{k=0}^{\\infty} p^k \\end{align} $$ $$ \\begin{align} \\sum_{k=0}^{\\infty} p^k & = 1 + \\sum_{k=1}^{\\infty} p^k \\end{align} $$ $$ \\begin{align} p S &= \\sum_{k=0}^{\\infty} p^{k+1} \\\\ &= \\sum_{k=1}^{\\infty} p^k \\\\ \\end{align} $$ $$ \\begin{align} S - p S &= 1 \\\\ S &= \\frac{1}{1-p} \\\\ \\end{align} $$ $$ \\begin{align} \\sum_{k=0}^{s-1} p^k = \\frac{1-p^s}{1-p} \\\\ \\sum_{k=s}^{\\infty} p^k = \\frac{p^s}{1-p} \\end{align} $$ Proof: $$ \\begin{align} \\sum_{k=0}^{s-1} p^k &= \\sum_{k=0}^{\\infty} p^k - \\sum_{k=s}^{\\infty} p^k \\\\ &= \\sum_{k=0}^{\\infty} p^k - p^s \\sum_{k=0}^{\\infty} p^k \\\\ &= \\frac{1}{1-p} - \\frac{p^s}{1-p} \\\\ &= \\frac{1 - p^s}{1-p} \\\\ \\end{align} $$ $$ \\begin{align} \\sum_{k=s}^{\\infty} p^k &= p^s \\sum_{k=0}^{\\infty} p^k \\\\ &= \\frac{p^s}{1-p} \\\\ \\end{align} $$ $$ \\begin{align} \\sum_{k=0}^{\\infty} k p^k = \\frac{p}{(1-p)^2} \\end{align} $$ Proof: $$ \\begin{align} S' &= \\sum_{k=0}^{\\infty} k p^k \\\\ p S' &= \\sum_{k=0}^{\\infty} k p^{k+1} \\\\ &= \\sum_{k=1}^{\\infty} (k - 1) p^{k} \\\\ &= \\sum_{k=1} k p^k - \\sum_{k=1} p^k \\\\ &= S' - \\frac{p}{1-p} \\\\ p S' - S' &= - \\frac{p}{1-p} \\\\ S' (p - 1) &= - \\frac{p}{1-p} \\\\ S' (1 - p) &= \\frac{p}{1-p} \\\\ S' &= \\frac{p}{(1-p)^2} \\\\ \\end{align} $$ $$ \\begin{align} \\end{align} $$","title":"Simple Sums"},{"location":"Simple-Sum.html#2018-05-25","text":"","title":"2018-05-25"},{"location":"Singularity-Estimation.html","text":"Singularity Estimation The term \"singularity\" was coined as an homage to a black hole singularity, where our theories of relativity break down. Though hard to pinpoint what the criteria are for the technological singularity, I believe I've heard Kurzweil say it's when there's the computation power of a small village, 10,000 to 100,000 human level AIs. As a \"back of the envelope\" calculation, we can estimate what the rough time frame for this could be. For this estimate, I will use the human brain's storage capacity. It's assumed that computation on what's stored goes hand in hand. The other factors that will be assumed to follow easily are power consumption, computation speed and whatever else goes with general purpose computation. The storage capacity is a nice metric as there's a sentiment that, in general, simple algorithms on large amounts of data win out over complex algorithms on small amounts of data. Often times the difference between code and data is semantic but the underlying sentiment is the main motivation for focusing on storage capacity over anything else. The time estimate is as follows: Estimate the human brain storage capacity Estimate the cost of current storage Assume Moore's Law holds for storage capacity and holds for the next couple of decades Assume the \"singularity\" will happen when storage capacity is the same as the human brain and costs around $1000 Extrapolate to find when that price point is hit. The cost of $1000 is chosen under the assumption that true general purpose AI will only happen when the economics of storage reach a point where amateurs and lay-people have access to the computing power. In the past, the personal computer became accessible at around the $1000 price point. The human brain is estimated to be 2.5 petabytes . The entropy of the human brain can be very roughly estimated by taking the average number of human brain neurons, 100 billion , and multiplying by the average number of neuron connections, 7000 . l(100*10^9 * 7000)/l(10) 14.84509804001425683076 Where a petabyte is 10^15 . As of today ( 2019-09-24 ) storage costs are about $20 / terabyte . Assuming a generalized Moore's Law holds by halving the cost every 18 months : $$ \\frac{$1000}{2.5 P} = \\frac{$20}{1 T} \\cdot \\frac{1}{2}^{y \\cdot \\frac{18}{12}} $$ or in about 7.5 years. This puts the a rough timeline on the singularity slated at the year 2027 . Often times these estimates can be off by a factor of a decade or so but if there are underlying exponential processes involved in terms of computing power per dollar, this means waiting some linear amount of time will result in an exponential increase in computer power. In other words, even if one or some of the parameters are off by an order of magnitude, this will only skew the time frame linearly. I find it interesting that the year 2027 is very close to what many of the cyberpunk authors of the 1980s and 1990s estimated as the time their stories. Reference The Singularity of AI and Biotech is Near, Futurist Ray Kurzweil Technological singularity Wikipedia: List of animals by number of neurons Wikipedia: Neuron Connectivity 2019-09-24","title":"Singularity Estimation"},{"location":"Singularity-Estimation.html#singularity-estimation","text":"The term \"singularity\" was coined as an homage to a black hole singularity, where our theories of relativity break down. Though hard to pinpoint what the criteria are for the technological singularity, I believe I've heard Kurzweil say it's when there's the computation power of a small village, 10,000 to 100,000 human level AIs. As a \"back of the envelope\" calculation, we can estimate what the rough time frame for this could be. For this estimate, I will use the human brain's storage capacity. It's assumed that computation on what's stored goes hand in hand. The other factors that will be assumed to follow easily are power consumption, computation speed and whatever else goes with general purpose computation. The storage capacity is a nice metric as there's a sentiment that, in general, simple algorithms on large amounts of data win out over complex algorithms on small amounts of data. Often times the difference between code and data is semantic but the underlying sentiment is the main motivation for focusing on storage capacity over anything else. The time estimate is as follows: Estimate the human brain storage capacity Estimate the cost of current storage Assume Moore's Law holds for storage capacity and holds for the next couple of decades Assume the \"singularity\" will happen when storage capacity is the same as the human brain and costs around $1000 Extrapolate to find when that price point is hit. The cost of $1000 is chosen under the assumption that true general purpose AI will only happen when the economics of storage reach a point where amateurs and lay-people have access to the computing power. In the past, the personal computer became accessible at around the $1000 price point. The human brain is estimated to be 2.5 petabytes . The entropy of the human brain can be very roughly estimated by taking the average number of human brain neurons, 100 billion , and multiplying by the average number of neuron connections, 7000 . l(100*10^9 * 7000)/l(10) 14.84509804001425683076 Where a petabyte is 10^15 . As of today ( 2019-09-24 ) storage costs are about $20 / terabyte . Assuming a generalized Moore's Law holds by halving the cost every 18 months : $$ \\frac{$1000}{2.5 P} = \\frac{$20}{1 T} \\cdot \\frac{1}{2}^{y \\cdot \\frac{18}{12}} $$ or in about 7.5 years. This puts the a rough timeline on the singularity slated at the year 2027 . Often times these estimates can be off by a factor of a decade or so but if there are underlying exponential processes involved in terms of computing power per dollar, this means waiting some linear amount of time will result in an exponential increase in computer power. In other words, even if one or some of the parameters are off by an order of magnitude, this will only skew the time frame linearly. I find it interesting that the year 2027 is very close to what many of the cyberpunk authors of the 1980s and 1990s estimated as the time their stories.","title":"Singularity Estimation"},{"location":"Singularity-Estimation.html#reference","text":"The Singularity of AI and Biotech is Near, Futurist Ray Kurzweil Technological singularity Wikipedia: List of animals by number of neurons Wikipedia: Neuron Connectivity","title":"Reference"},{"location":"Singularity-Estimation.html#2019-09-24","text":"","title":"2019-09-24"},{"location":"Socio-Economic-Definitions.html","text":"Socio-Economic Definitions Autocracy A form of government in which unlimited power is held by a single individual ( src ) Capitalism (politics) A socio-economic system based on private ownership of resources or capital. (economics) An economic system based on private ownership of the means of production and their operation for profit. (politics, economic liberalism) A socio-economic system based on private property rights, including the private ownership of resources or capital, with economic decisions made largely through the operation of a market unregulated by the state. (economics, economic liberalism) An economic system based on the abstraction of resources into the form of privately owned capital, with economic decisions made largely through the operation of a market unregulated by the state. ( src ) Communism (socio-economics) A socio-economic system based on common ownership of the means of production and the absence of social classes, money and a state. ( src , Wiktionary ) Direct Democracy A form of democracy in which people decide on policy initiatives directly. ( src ) Fascism A form of ideology characterized by centralized, totalitarian governance, strong regimentation of the economy and of society, and repression of criticism or opposition, by violence or otherwise. ( src ) Fiefdom Any organization in the control of a dominant individual. ( src ) Feudalism A hierarchical social system, reinforced by religion, based on personal ownership of resources and fealty between a lord and subject. ( src ) Marxism A method of socioeconomic analysis that uses a materialist interpretation of historical development, better known as historical materialism, to understand class relations and social conflict as well as a dialectical perspective to view social transformation. The socialist and communist theory that advocates a capture of state power by the working or lower class with an ultimate goal of the institution of communism. ( src wiktionary ) Neoliberalism An ideology that espouses ideas associated with economic liberalism, free market capitalism with a focus on privatization, deregulation, globalization, free trade, austerity and reduction in government spending to increase the role of the private sector in the economy and society. ( src ) Oligarchy A government run by only a few, often wealthy, individuals. ( src ) Representative Democracy A type of democracy founded on the principle of elected officials representing groups of people. ( src ) Social Democracy A policy that advocates economic and social interventions to promote social justice through liberal-democratic polity and a capitalist-oriented mixed economy. ( src ) Socialism (socio-economics) Any economic or political theory advocating for collective or governmental ownership and administration of the means of production and distribution of goods ( src ) Related Definitions Debate A discussion of opposing views ( src ) Dialectic A discourse between two or more people holding different points of view about a subject but both parties wishing to establish the truth through reasoned methods of argumentation. A formal system of reasoning that arrives at a truth by the exchange of logical arguments Note that in the context of Marxism, dialectic discussion (dialectical method, dialectics) serves as a discourse between two parties to mutually come to truth, as opposed to a didactic discussion, where one party is teaching the other or a general debate, where both parties could be discussing to voice their views without any intention of changing them. ( src wiktionary ) Dictatorship of the Proletariat A state of affairs in which the proletariat holds political power. The temporary period following the fall of capitalism characterized by a struggle to achieve a classless, stateless and moneyless communist society ( src wiktionary ) Didactic A method of instruction with the intent to teach or demonstrate. ( src ) Historical Materialism A Marxist methodological approach to the study of society, economics, and history, looking for the causes of developments and changes in human society in the means by which humans collectively produce the necessities of life. ( src ) Materialism A form of philosophical monism that holds that matter is the fundamental substance in nature, and that all things, including mental states and consciousness, are results of material interactions. ( src ) Monism A concept that there exists only a single reality of which the physical world is an aspect of. Specifically, monism denies the existence of a mind body duality or other aspects of duality. ( src ) Proletariat The working class or lower class. ( src ) 2020-11-02","title":"Socio Economic Definitions"},{"location":"Socio-Economic-Definitions.html#socio-economic-definitions","text":"Autocracy A form of government in which unlimited power is held by a single individual ( src ) Capitalism (politics) A socio-economic system based on private ownership of resources or capital. (economics) An economic system based on private ownership of the means of production and their operation for profit. (politics, economic liberalism) A socio-economic system based on private property rights, including the private ownership of resources or capital, with economic decisions made largely through the operation of a market unregulated by the state. (economics, economic liberalism) An economic system based on the abstraction of resources into the form of privately owned capital, with economic decisions made largely through the operation of a market unregulated by the state. ( src ) Communism (socio-economics) A socio-economic system based on common ownership of the means of production and the absence of social classes, money and a state. ( src , Wiktionary ) Direct Democracy A form of democracy in which people decide on policy initiatives directly. ( src ) Fascism A form of ideology characterized by centralized, totalitarian governance, strong regimentation of the economy and of society, and repression of criticism or opposition, by violence or otherwise. ( src ) Fiefdom Any organization in the control of a dominant individual. ( src ) Feudalism A hierarchical social system, reinforced by religion, based on personal ownership of resources and fealty between a lord and subject. ( src ) Marxism A method of socioeconomic analysis that uses a materialist interpretation of historical development, better known as historical materialism, to understand class relations and social conflict as well as a dialectical perspective to view social transformation. The socialist and communist theory that advocates a capture of state power by the working or lower class with an ultimate goal of the institution of communism. ( src wiktionary ) Neoliberalism An ideology that espouses ideas associated with economic liberalism, free market capitalism with a focus on privatization, deregulation, globalization, free trade, austerity and reduction in government spending to increase the role of the private sector in the economy and society. ( src ) Oligarchy A government run by only a few, often wealthy, individuals. ( src ) Representative Democracy A type of democracy founded on the principle of elected officials representing groups of people. ( src ) Social Democracy A policy that advocates economic and social interventions to promote social justice through liberal-democratic polity and a capitalist-oriented mixed economy. ( src ) Socialism (socio-economics) Any economic or political theory advocating for collective or governmental ownership and administration of the means of production and distribution of goods ( src )","title":"Socio-Economic Definitions"},{"location":"Socio-Economic-Definitions.html#related-definitions","text":"Debate A discussion of opposing views ( src ) Dialectic A discourse between two or more people holding different points of view about a subject but both parties wishing to establish the truth through reasoned methods of argumentation. A formal system of reasoning that arrives at a truth by the exchange of logical arguments Note that in the context of Marxism, dialectic discussion (dialectical method, dialectics) serves as a discourse between two parties to mutually come to truth, as opposed to a didactic discussion, where one party is teaching the other or a general debate, where both parties could be discussing to voice their views without any intention of changing them. ( src wiktionary ) Dictatorship of the Proletariat A state of affairs in which the proletariat holds political power. The temporary period following the fall of capitalism characterized by a struggle to achieve a classless, stateless and moneyless communist society ( src wiktionary ) Didactic A method of instruction with the intent to teach or demonstrate. ( src ) Historical Materialism A Marxist methodological approach to the study of society, economics, and history, looking for the causes of developments and changes in human society in the means by which humans collectively produce the necessities of life. ( src ) Materialism A form of philosophical monism that holds that matter is the fundamental substance in nature, and that all things, including mental states and consciousness, are results of material interactions. ( src ) Monism A concept that there exists only a single reality of which the physical world is an aspect of. Specifically, monism denies the existence of a mind body duality or other aspects of duality. ( src ) Proletariat The working class or lower class. ( src )","title":"Related Definitions"},{"location":"Socio-Economic-Definitions.html#2020-11-02","text":"","title":"2020-11-02"},{"location":"Software-Architechture-Notes.html","text":"Software Architecture Notes Microservices Microservervice Anti-Patterns The advice is as follows: Use boring technology Greedily create micro-service architecture instead of starting with one when not necessary Monolithic architecture is fine to start out Use proxy schemas Use a proxy server to a DB, say, to keep the API consistent Use queues Create a queue service which buffers messages to then be ferried to the destination Smooths out spikes Allows to work around shortcomings of end service (DB, say?) Use discovery tools Don't hard code addresses in code Use a service to discover the end point Use rate limitation Probably good to have both client side and server side detection to rate limit Use debugging tools early Monitoring of the architecture is necessary for smooth operation Issue becomes sifting through the data but better to have the data to sift through in the first place than not at all","title":"Software Architechture Notes"},{"location":"Software-Architechture-Notes.html#software-architecture-notes","text":"","title":"Software Architecture Notes"},{"location":"Software-Architechture-Notes.html#microservices","text":"Microservervice Anti-Patterns The advice is as follows: Use boring technology Greedily create micro-service architecture instead of starting with one when not necessary Monolithic architecture is fine to start out Use proxy schemas Use a proxy server to a DB, say, to keep the API consistent Use queues Create a queue service which buffers messages to then be ferried to the destination Smooths out spikes Allows to work around shortcomings of end service (DB, say?) Use discovery tools Don't hard code addresses in code Use a service to discover the end point Use rate limitation Probably good to have both client side and server side detection to rate limit Use debugging tools early Monitoring of the architecture is necessary for smooth operation Issue becomes sifting through the data but better to have the data to sift through in the first place than not at all","title":"Microservices"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html","text":"Statistical Mechanics for Computer Scientists These are notes on statistical mechanics concepts with a focus on interpreting them from the perspective of a computer scientist. These should be considered personal opinions and, therefore, might be completely misleading or outright wrong. Entropy Entropy can be considered \"the number of bits that it takes to describe a system\". That is if a system has $N$ possible states, each occurring with probability $p_i$, then the number of bits to describe the system is: $$ S _ { * } = - \\sum_{i=0}^{N-1} p_i \\cdot \\lg( p_i ) $$ With $\\lg(\\cdot) = \\frac{ \\ln(\\cdot) }{ \\ln(2) }$. Boltzmann Distribution The state $i$ is often called a \"microstate\". If we have a set of microstates and start out with assigning each of them energies, rather than probabilities, under suitable conditions, we can derive a probability for each microstate. If we assume each microstate has an energy, $E_i$, attached to it, we can write down some equations: $$ \\begin{array}{ll} 1 = & \\sum_{i} p_i \\\\ E = & \\sum_{i} p_i E_i \\\\ S = & - \\sum_{i} p_i \\ln(p_i) \\end{array} $$ Where we use $S_{*}$ to differentiate between the entropy defined with $\\lg(\\cdot)$ instead of $\\ln(\\cdot)$ For the derivations below, it's easier to work in natural logarithm ( $\\ln (\\cdot )$ ) rather than the logarithm base 2 ( $\\lg(\\cdot)$ ) in addition to what is used in the physics literature. The choice of base for the logarithm should only contribute a constant factor and shouldn't take away from the broader ideas. In the above, we make a few assumptions: Each of the microstate energies, $E_i$, is fixed and unchanging We impose the constraint that the average energy, $E$, is fixed The $p_i$ form a probability distribution In other words find the maximum entropy, $S$, subject to the constraints of $E_i$ chosen/fixed and an average fixed energy, $E$. So, we want to maximize $S$ by varying each of the individual $p_i$'s. We can use the method of Lagrange multipliers by using the two equations above as the constraints: $$ \\begin{align} \\vec{p} & = ( p_0, p_i, \\cdots, p_{N-1} ) \\\\ L( \\vec{p}, \\alpha, \\beta ) & = S - \\alpha [ (\\sum_{i} p_i) - 1 ] - \\beta [ (\\sum_{i} p_i E_i) - E ] \\\\ & = - \\sum_{i} p_i \\ln(p_i) - \\alpha [ (\\sum_{i} p_i) - 1 ] - \\beta [ (\\sum_{i} p_i E_i) - E ] \\\\ \\frac{\\partial}{\\partial p_i} L = & -ln(p_i) - 1 - \\alpha - \\beta E_i = 0 \\\\ \\to \\ \\ & p_i = e^{-(1+\\alpha)} e^{-\\beta E_i} \\end{align} $$ We can now define temperature: $$ T = \\frac{1}{\\beta} $$ And using one of the constraints, we can rewrite equations to get rid of the $\\alpha$ term: $$ \\begin{align} \\sum_{i} p_i & = 1 \\\\ \\to \\ \\ & \\sum_{i} e^{ -\\beta E_i } = e^{1 + \\alpha} \\\\ \\to \\ \\ & \\sum_{i} e^{ \\frac{E_i}{T} } = Z(T) \\\\ \\to \\ \\ & Z(T) = e^{1 + \\alpha} \\end{align} $$ Which gives us: $$ p_i = \\frac{1}{Z(T)} e^{ -\\frac{E_i}{T} } $$ Adding a term, $\\kappa$, to $T$ and rewriting the probability as: $$ p_i \\propto e^{ -\\frac{E_i}{\\kappa T} } $$ Is called a Boltzmann distribution. Another name is Gibbs distribution. $Z(T)$ is often called the partition function and acts as a renormalization constant. Because of the partition function's ( $Z(T)$ ) relation to temperature and energy, among other derived quantities, interrogating the partition function through the use of derivatives of different variables can produce information about the underlying system. Kullback-Leibler Divergence We want to talk about free energy but we will need the idea of the Kullback-Leibler divergence first before providing intuition about the free energy definition. Consider an optimal encoding of sending $n$ symbols over a channel with the $i$'th symbol occurring with probability $p_i$. We can write the entropy of the distribution $p(\\cdot)$ as: $$ S_p = - \\sum_{i}^{n-1} p_i \\ln(p_i) $$ Let's say we introduce another distribution $q(\\cdot)$ that we will use to find an encoding/decoding method on the symbols. If the symbols are transmitted at the rate of $p_i$ but we're using $q_i$ to encode/decode them, we end up with (proportionally) $\\lg(q_i)$ bits per symbols instead of (proportionally) $\\lg(p_i)$ bits per symbol. We can write down the entropy of receiving these symbols with probability distribution $p_i$ but using $q_i$ to encode them as: $$ S_q = - \\sum_{i}^{n-1} p_i \\ln(q_i) $$ The difference, $S_q - S_p$ is how \"bad\" the $q_i$ encoding is in terms of how many extra bits we waste using the $q_i$ encoding. If we introduce more notation: $$ \\begin{align} S_q - S_p & = - [ \\sum _ i p _ i \\ln(q _ i) - \\sum _ i p _ i \\ln(p _ i) ] \\\\ \\to D_{KL}(p || q) & \\stackrel{\\text{def}}{=} \\sum _ i p _ i \\ln( \\frac{p _ i}{q _ i} ) \\end{align} $$ Which is called the Kullback-Leibler Divergence. Another way to write this is: $$ D_{KL}(p || q) = H(p,q) - H(p) $$ Where $H(p,q)$ is called the \"cross entropy\": $$ \\begin{align} H(p) & = - \\sum_{i} p_i \\ln(p_i) \\\\ H(p,q) & = - \\sum_{i} p_i \\ln(q_i) \\end{align} $$ Helmholtz Free Energy Helmholtz free energy is defined as the average energy minus the entropy: $$ \\begin{align} F_H & = U - TS \\\\ & = \\sum_{i} p_i E_i + T \\sum_{i} p_i \\ln(p_i) \\end{align} $$ Under equilibrium (?) recall $$ \\begin{align} \\ \\ & p_i = \\frac{e^{ -\\frac{E_i}{T} } }{Z} \\\\ \\to \\ \\ & E_i = -T \\ln(p_i) - T \\ln(Z) \\\\ \\end{align} $$ Shuffling around, we find: $$ \\begin{align} F_H & = U - TS \\\\ & = \\sum_{i} p_i E_i + T \\sum_{i} p_i \\ln(p_i) \\\\ & = - T \\sum_{i} p_i \\ln(p_i) - T \\ln(Z) \\sum_{i} p_i + T \\sum_{i} p_i \\ln(p_i) \\\\ & = - T \\ln(Z) \\end{align} $$ Relating the log of the partition function (number of bits to describe the number of states), modified by temperature, to the average energy minus the entropy. For the sake of clarity: $$ \\begin{align} F_H & = U - TS \\\\ F_H & = -T \\ln(Z) \\\\ \\end{align} $$ Gibbs Free Energy If, instead we have a \"trial\" probability distribution $q_i$ but keep the energies of the microstates, $E_i$, untouched, we get the Gibbs free energy: $$ \\begin{align} F_G & = \\sum_{i} q_i E_i - T S_q \\\\ & = \\sum_{i} q_i E_i + T \\sum_{i} q_i \\ln(q_i) \\end{align} $$ Rearranging: $$ \\begin{align} F_G & = \\sum_{i} q_i E_i + T \\sum_{i} q_i \\ln(q_i) \\\\ & = -T \\sum_{i} q_i \\ln(p_i) - T \\ln(Z) + T \\sum_{i} q_i \\ln(q_i) \\\\ & = T \\sum_{i} q_i \\ln( \\frac{q_i}{p_i} ) - T \\ln(Z) \\\\ \\end{align} $$ $$ F_G = T D_{KL}( q || p ) + F_H $$ Relating Gibbs free energy to Helmholtz free energy by a factor of the \"divergence\" of the probability distributions. Appendix Lagrange Multipliers Statement without proof. $$ \\begin{align} & f,g \\in C^1 & \\\\ & f: \\mathbb{R}^n \\mapsto \\mathbb{R} & \\\\ & g: \\mathbb{R}^n \\mapsto \\mathbb{R}^m & \\ \\ \\ (m < n) \\\\ & D h(x) = [ \\frac{\\partial h_j}{\\partial x_k} ] & \\end{align} $$ $$ \\begin{align} \\text{ maximize: } & f(x) \\\\ \\text{ subject to: } & g(x)=0 \\\\ \\to \\ \\ & x ^ * \\text{ optimal } \\\\ & \\exists \\lambda ^ * \\in \\mathbb{ R } ^ m \\\\ \\text{ s.t. } \\ & D f(x ^ { * }) = { \\lambda ^ { * } } ^ { \\intercal } D g(x ^ { * }) \\end{align} $$ In other words, subject to the constrained surface $g$, the maximum point on $f$ is achieved when when the gradient of $f$ is equal and opposite to the constraint surface, $g$. Derivation of Entropy See Shannon Entropy but briefly recreated here for completeness. Consider $n$ symbols, each occurring with probability $p_k$ for $k \\in (0,1,, \\dots , n-1)$. If a system is comprised of $T$ symbols, where each is assumed to be independent of each other, and if $T$ large, then $T \\cdot p_k \\cdot n$ is approximately integral and we can express the number of ways of arranging $T$ symbols as: $$ { T \\cdot n \\choose (T \\cdot p_0 \\cdot n), (T \\cdot p_1 \\cdot n), \\dots, (T \\cdot p_{n-1} \\cdot n) } $$ $$ = \\frac{(T \\cdot n)!}{ {\\prod} _ { k=0 } ^ { n-1 } (T \\cdot p _ k \\cdot n)!} $$ The number of bits to describe the number of configurations is (with $\\lg(\\cdot) = \\log_2(\\cdot)$ ): $$ \\lg( \\frac{(T \\cdot n)!}{ {\\prod} _ { k=0 } ^ { n-1 } (T \\cdot p _ k \\cdot n)!} ) $$ Which, after some algebra, reduces to: $$ = - T \\sum _ { k=0 } ^ { n-1 } p _ k \\lg(p _ k) $$ Define the entropy, $S$, to be the average number of bits needed to represent our system at a particular point in time (that is, the average number of bits per symbol), we find: $$ S = - \\sum _ { k=0 } ^ { n-1 } p _ k \\lg(p _ k) $$ Metropolis Hastings Algorithm In this context, we sometimes want to sample from the Boltzmann distribution distribution. Call a state of the system $s_t$ with the energy of the state $E_{s_t}$. We call the transition probability: $$ P( s_t \\to s_{t+1} ) = \\begin{cases} e^{- \\beta ( E_{s_t} - E_{s_{t+1}} ) } & , & E_{s_{t+1}} > E_{s_t} \\\\ 1 & , & E_{s_{t+1}} \\le E_{s_t} \\\\ \\end{cases} $$ Recall $\\beta = \\frac{1}{T}$. Call the number of possible transition states from one state to the other $N(\\cdot)$: $$ \\begin{array}{ll} N( s_t \\to s_{t+1} ) & \\propto N_{s_t} e^{ -\\beta ( E_{s_{t+1}} - E_{s_t} ) } \\\\ N( s_{t+1} \\to s_t ) & \\propto N_{s_{t+1}} \\end{array} $$ If we assume: $$ N_{s_t} > 0, N_{s_{t+1}} > 0 \\\\ N( s_t \\to s_{t+1} ) = N( s_{t+1} \\to s_t ) $$ Then: $$ \\begin{array}{ll} & N( s_t \\to s_{t+1} ) - N( s_{t+1} \\to s_t ) = N_{s_t} e^{ -\\beta ( E_{s_{t+1}} - E_{s_t} ) } - N_{s_{t+1}} \\\\ \\to & 0 = N_{s_t} e^{ -\\beta ( E_{s_{t+1}} - E_{s_t} ) } - N_{s_{t+1}} \\\\ \\to & 0 = N_{s_t} ( \\frac{ e^{ -\\beta E_{s_{t+1}} }}{ e^{ - \\beta E_{s_t} } } - \\frac{ N_{s_{t+1}} }{ N_{s_t} } ) \\\\ \\to & 0 = \\frac{ e^{ -\\beta E_{s_{t+1}} }}{ e^{ - \\beta E_{s_t} } } - \\frac{ N_{s_{t+1}} }{ N_{s_t} } \\\\ \\to & \\frac{ N_{s_{t+1}} }{ N_{s_t} } = \\frac{ e^{ -\\beta E_{s_{t+1}} }}{ e^{ - \\beta E_{s_t} } } \\\\ \\to & N_{s_{t+1}} e^{ - \\beta E_{s_t} } = N_{s_t} e^{ -\\beta E_{s_{t+1}} } \\\\ \\end{array} $$ Which is a detailed balance condition ensuring it be an ergodic process. References susskind Gibbs free energy Kullback-Leibler Divergence Lagrange Multipliers Stationary Distribution Stationary Ergodic Process Ergodic Process Detailed Balance Complexity and Criticality by K. Christensen and N. Moloney 2023-11-12","title":"Statistical Mechanics For Computer Scientists"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#statistical-mechanics-for-computer-scientists","text":"These are notes on statistical mechanics concepts with a focus on interpreting them from the perspective of a computer scientist. These should be considered personal opinions and, therefore, might be completely misleading or outright wrong.","title":"Statistical Mechanics for Computer Scientists"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#entropy","text":"Entropy can be considered \"the number of bits that it takes to describe a system\". That is if a system has $N$ possible states, each occurring with probability $p_i$, then the number of bits to describe the system is: $$ S _ { * } = - \\sum_{i=0}^{N-1} p_i \\cdot \\lg( p_i ) $$ With $\\lg(\\cdot) = \\frac{ \\ln(\\cdot) }{ \\ln(2) }$.","title":"Entropy"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#boltzmann-distribution","text":"The state $i$ is often called a \"microstate\". If we have a set of microstates and start out with assigning each of them energies, rather than probabilities, under suitable conditions, we can derive a probability for each microstate. If we assume each microstate has an energy, $E_i$, attached to it, we can write down some equations: $$ \\begin{array}{ll} 1 = & \\sum_{i} p_i \\\\ E = & \\sum_{i} p_i E_i \\\\ S = & - \\sum_{i} p_i \\ln(p_i) \\end{array} $$ Where we use $S_{*}$ to differentiate between the entropy defined with $\\lg(\\cdot)$ instead of $\\ln(\\cdot)$ For the derivations below, it's easier to work in natural logarithm ( $\\ln (\\cdot )$ ) rather than the logarithm base 2 ( $\\lg(\\cdot)$ ) in addition to what is used in the physics literature. The choice of base for the logarithm should only contribute a constant factor and shouldn't take away from the broader ideas. In the above, we make a few assumptions: Each of the microstate energies, $E_i$, is fixed and unchanging We impose the constraint that the average energy, $E$, is fixed The $p_i$ form a probability distribution In other words find the maximum entropy, $S$, subject to the constraints of $E_i$ chosen/fixed and an average fixed energy, $E$. So, we want to maximize $S$ by varying each of the individual $p_i$'s. We can use the method of Lagrange multipliers by using the two equations above as the constraints: $$ \\begin{align} \\vec{p} & = ( p_0, p_i, \\cdots, p_{N-1} ) \\\\ L( \\vec{p}, \\alpha, \\beta ) & = S - \\alpha [ (\\sum_{i} p_i) - 1 ] - \\beta [ (\\sum_{i} p_i E_i) - E ] \\\\ & = - \\sum_{i} p_i \\ln(p_i) - \\alpha [ (\\sum_{i} p_i) - 1 ] - \\beta [ (\\sum_{i} p_i E_i) - E ] \\\\ \\frac{\\partial}{\\partial p_i} L = & -ln(p_i) - 1 - \\alpha - \\beta E_i = 0 \\\\ \\to \\ \\ & p_i = e^{-(1+\\alpha)} e^{-\\beta E_i} \\end{align} $$ We can now define temperature: $$ T = \\frac{1}{\\beta} $$ And using one of the constraints, we can rewrite equations to get rid of the $\\alpha$ term: $$ \\begin{align} \\sum_{i} p_i & = 1 \\\\ \\to \\ \\ & \\sum_{i} e^{ -\\beta E_i } = e^{1 + \\alpha} \\\\ \\to \\ \\ & \\sum_{i} e^{ \\frac{E_i}{T} } = Z(T) \\\\ \\to \\ \\ & Z(T) = e^{1 + \\alpha} \\end{align} $$ Which gives us: $$ p_i = \\frac{1}{Z(T)} e^{ -\\frac{E_i}{T} } $$ Adding a term, $\\kappa$, to $T$ and rewriting the probability as: $$ p_i \\propto e^{ -\\frac{E_i}{\\kappa T} } $$ Is called a Boltzmann distribution. Another name is Gibbs distribution. $Z(T)$ is often called the partition function and acts as a renormalization constant. Because of the partition function's ( $Z(T)$ ) relation to temperature and energy, among other derived quantities, interrogating the partition function through the use of derivatives of different variables can produce information about the underlying system.","title":"Boltzmann Distribution"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#kullback-leibler-divergence","text":"We want to talk about free energy but we will need the idea of the Kullback-Leibler divergence first before providing intuition about the free energy definition. Consider an optimal encoding of sending $n$ symbols over a channel with the $i$'th symbol occurring with probability $p_i$. We can write the entropy of the distribution $p(\\cdot)$ as: $$ S_p = - \\sum_{i}^{n-1} p_i \\ln(p_i) $$ Let's say we introduce another distribution $q(\\cdot)$ that we will use to find an encoding/decoding method on the symbols. If the symbols are transmitted at the rate of $p_i$ but we're using $q_i$ to encode/decode them, we end up with (proportionally) $\\lg(q_i)$ bits per symbols instead of (proportionally) $\\lg(p_i)$ bits per symbol. We can write down the entropy of receiving these symbols with probability distribution $p_i$ but using $q_i$ to encode them as: $$ S_q = - \\sum_{i}^{n-1} p_i \\ln(q_i) $$ The difference, $S_q - S_p$ is how \"bad\" the $q_i$ encoding is in terms of how many extra bits we waste using the $q_i$ encoding. If we introduce more notation: $$ \\begin{align} S_q - S_p & = - [ \\sum _ i p _ i \\ln(q _ i) - \\sum _ i p _ i \\ln(p _ i) ] \\\\ \\to D_{KL}(p || q) & \\stackrel{\\text{def}}{=} \\sum _ i p _ i \\ln( \\frac{p _ i}{q _ i} ) \\end{align} $$ Which is called the Kullback-Leibler Divergence. Another way to write this is: $$ D_{KL}(p || q) = H(p,q) - H(p) $$ Where $H(p,q)$ is called the \"cross entropy\": $$ \\begin{align} H(p) & = - \\sum_{i} p_i \\ln(p_i) \\\\ H(p,q) & = - \\sum_{i} p_i \\ln(q_i) \\end{align} $$","title":"Kullback-Leibler Divergence"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#helmholtz-free-energy","text":"Helmholtz free energy is defined as the average energy minus the entropy: $$ \\begin{align} F_H & = U - TS \\\\ & = \\sum_{i} p_i E_i + T \\sum_{i} p_i \\ln(p_i) \\end{align} $$ Under equilibrium (?) recall $$ \\begin{align} \\ \\ & p_i = \\frac{e^{ -\\frac{E_i}{T} } }{Z} \\\\ \\to \\ \\ & E_i = -T \\ln(p_i) - T \\ln(Z) \\\\ \\end{align} $$ Shuffling around, we find: $$ \\begin{align} F_H & = U - TS \\\\ & = \\sum_{i} p_i E_i + T \\sum_{i} p_i \\ln(p_i) \\\\ & = - T \\sum_{i} p_i \\ln(p_i) - T \\ln(Z) \\sum_{i} p_i + T \\sum_{i} p_i \\ln(p_i) \\\\ & = - T \\ln(Z) \\end{align} $$ Relating the log of the partition function (number of bits to describe the number of states), modified by temperature, to the average energy minus the entropy. For the sake of clarity: $$ \\begin{align} F_H & = U - TS \\\\ F_H & = -T \\ln(Z) \\\\ \\end{align} $$","title":"Helmholtz Free Energy"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#gibbs-free-energy","text":"If, instead we have a \"trial\" probability distribution $q_i$ but keep the energies of the microstates, $E_i$, untouched, we get the Gibbs free energy: $$ \\begin{align} F_G & = \\sum_{i} q_i E_i - T S_q \\\\ & = \\sum_{i} q_i E_i + T \\sum_{i} q_i \\ln(q_i) \\end{align} $$ Rearranging: $$ \\begin{align} F_G & = \\sum_{i} q_i E_i + T \\sum_{i} q_i \\ln(q_i) \\\\ & = -T \\sum_{i} q_i \\ln(p_i) - T \\ln(Z) + T \\sum_{i} q_i \\ln(q_i) \\\\ & = T \\sum_{i} q_i \\ln( \\frac{q_i}{p_i} ) - T \\ln(Z) \\\\ \\end{align} $$ $$ F_G = T D_{KL}( q || p ) + F_H $$ Relating Gibbs free energy to Helmholtz free energy by a factor of the \"divergence\" of the probability distributions.","title":"Gibbs Free Energy"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#appendix","text":"","title":"Appendix"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#lagrange-multipliers","text":"Statement without proof. $$ \\begin{align} & f,g \\in C^1 & \\\\ & f: \\mathbb{R}^n \\mapsto \\mathbb{R} & \\\\ & g: \\mathbb{R}^n \\mapsto \\mathbb{R}^m & \\ \\ \\ (m < n) \\\\ & D h(x) = [ \\frac{\\partial h_j}{\\partial x_k} ] & \\end{align} $$ $$ \\begin{align} \\text{ maximize: } & f(x) \\\\ \\text{ subject to: } & g(x)=0 \\\\ \\to \\ \\ & x ^ * \\text{ optimal } \\\\ & \\exists \\lambda ^ * \\in \\mathbb{ R } ^ m \\\\ \\text{ s.t. } \\ & D f(x ^ { * }) = { \\lambda ^ { * } } ^ { \\intercal } D g(x ^ { * }) \\end{align} $$ In other words, subject to the constrained surface $g$, the maximum point on $f$ is achieved when when the gradient of $f$ is equal and opposite to the constraint surface, $g$.","title":"Lagrange Multipliers"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#derivation-of-entropy","text":"See Shannon Entropy but briefly recreated here for completeness. Consider $n$ symbols, each occurring with probability $p_k$ for $k \\in (0,1,, \\dots , n-1)$. If a system is comprised of $T$ symbols, where each is assumed to be independent of each other, and if $T$ large, then $T \\cdot p_k \\cdot n$ is approximately integral and we can express the number of ways of arranging $T$ symbols as: $$ { T \\cdot n \\choose (T \\cdot p_0 \\cdot n), (T \\cdot p_1 \\cdot n), \\dots, (T \\cdot p_{n-1} \\cdot n) } $$ $$ = \\frac{(T \\cdot n)!}{ {\\prod} _ { k=0 } ^ { n-1 } (T \\cdot p _ k \\cdot n)!} $$ The number of bits to describe the number of configurations is (with $\\lg(\\cdot) = \\log_2(\\cdot)$ ): $$ \\lg( \\frac{(T \\cdot n)!}{ {\\prod} _ { k=0 } ^ { n-1 } (T \\cdot p _ k \\cdot n)!} ) $$ Which, after some algebra, reduces to: $$ = - T \\sum _ { k=0 } ^ { n-1 } p _ k \\lg(p _ k) $$ Define the entropy, $S$, to be the average number of bits needed to represent our system at a particular point in time (that is, the average number of bits per symbol), we find: $$ S = - \\sum _ { k=0 } ^ { n-1 } p _ k \\lg(p _ k) $$","title":"Derivation of Entropy"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#metropolis-hastings-algorithm","text":"In this context, we sometimes want to sample from the Boltzmann distribution distribution. Call a state of the system $s_t$ with the energy of the state $E_{s_t}$. We call the transition probability: $$ P( s_t \\to s_{t+1} ) = \\begin{cases} e^{- \\beta ( E_{s_t} - E_{s_{t+1}} ) } & , & E_{s_{t+1}} > E_{s_t} \\\\ 1 & , & E_{s_{t+1}} \\le E_{s_t} \\\\ \\end{cases} $$ Recall $\\beta = \\frac{1}{T}$. Call the number of possible transition states from one state to the other $N(\\cdot)$: $$ \\begin{array}{ll} N( s_t \\to s_{t+1} ) & \\propto N_{s_t} e^{ -\\beta ( E_{s_{t+1}} - E_{s_t} ) } \\\\ N( s_{t+1} \\to s_t ) & \\propto N_{s_{t+1}} \\end{array} $$ If we assume: $$ N_{s_t} > 0, N_{s_{t+1}} > 0 \\\\ N( s_t \\to s_{t+1} ) = N( s_{t+1} \\to s_t ) $$ Then: $$ \\begin{array}{ll} & N( s_t \\to s_{t+1} ) - N( s_{t+1} \\to s_t ) = N_{s_t} e^{ -\\beta ( E_{s_{t+1}} - E_{s_t} ) } - N_{s_{t+1}} \\\\ \\to & 0 = N_{s_t} e^{ -\\beta ( E_{s_{t+1}} - E_{s_t} ) } - N_{s_{t+1}} \\\\ \\to & 0 = N_{s_t} ( \\frac{ e^{ -\\beta E_{s_{t+1}} }}{ e^{ - \\beta E_{s_t} } } - \\frac{ N_{s_{t+1}} }{ N_{s_t} } ) \\\\ \\to & 0 = \\frac{ e^{ -\\beta E_{s_{t+1}} }}{ e^{ - \\beta E_{s_t} } } - \\frac{ N_{s_{t+1}} }{ N_{s_t} } \\\\ \\to & \\frac{ N_{s_{t+1}} }{ N_{s_t} } = \\frac{ e^{ -\\beta E_{s_{t+1}} }}{ e^{ - \\beta E_{s_t} } } \\\\ \\to & N_{s_{t+1}} e^{ - \\beta E_{s_t} } = N_{s_t} e^{ -\\beta E_{s_{t+1}} } \\\\ \\end{array} $$ Which is a detailed balance condition ensuring it be an ergodic process.","title":"Metropolis Hastings Algorithm"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#references","text":"susskind Gibbs free energy Kullback-Leibler Divergence Lagrange Multipliers Stationary Distribution Stationary Ergodic Process Ergodic Process Detailed Balance Complexity and Criticality by K. Christensen and N. Moloney","title":"References"},{"location":"Statistical-Mechanics-For-Computer-Scientists.html#2023-11-12","text":"","title":"2023-11-12"},{"location":"Story-Motifs.html","text":"Story Motifs Act Name Description 1 Intro (optional) An introductory sequence that often doesn't have anything to do with the main adventure directly to set tone or pacing of movie 1 Normal World Introduction to the status quo and the world the main character lives in 1 Inciting Event Event that triggers journey 1 Call to Adventure The hero is called on to start an adventure 1 Initial Refusal and Reluctant Acceptance Hero initially refuses teh call but eventually capitulates 2 Gaining Confidence The hero starts to discover their new abilities 2 Subplot A sub plot is started that does not directly impact the adventure but complements the hero or adventure 2 A Battle Lost The hero loses a battle without losing the war 2 Disaster Things progressively get worse 2 Despair The goal looks hopeless 3 Reversal Something happens to make the goal within reach (with the path often coming from the Subplot ) 3 Climax The final battle is fought and won or lost depending on whether the story is a tragedy or not 3 New Normal The new normal is established, either by a successful or unsuccessful campaign of the hero, with the world being better or worse Examples Matrix 1.Intro Agents attack on Trinity and her narrow escape 1.Normal Neo in front of his computer and later going to his day job 1.Event Agents come to interrogate Neo at his work and ensuing chase and capture 1.Call Neo is told he's \"the one\" 1.Refusal But Neo doesn't believe it... 2.Confidence Neo learns Kung Fu 2.Subplot Oracle tells Neo he needs to make a choice, Trinity as a love interest (via Oracle), Cypher is a spy 2.Loss Agents attack the group 2.Disaster Morphius is taken, most of the crew is murdered 2.Despair Morphius is captured and being interregated 3.Reversal Neo and Trinity decide to attack 3.Climax Neo defeats agents 3.New Neo leaves a message for the computers Not all fits neatly into this structure and there's a lot of variation, including how the subplots weave into the main hero's journely and adventure (which maybe is why Matrix is so well written). There might be some disagreement over what the sub plot is (or even if there's just one) and how it feeds into the reversal. Also, a lot of the events are not done in order. It seems like the basic story structure is still valid, though. Blade Runner 1.Intro Leon is interviewed and kills Blade Runner 1.Normal Deckard ordering noodles 1.Event Gaaf calls on Deckard to come to the police station (?) 1.Call Deckard is asked to hunt replicants 1.Refusal Deckard initially refuses but accepts after being threatened by police cheif and hearing about the attack on the other Blade Runner 2.Confidence Deckard interviews Rachael and discovers she's a replicant 2.Subplot Deckards romance with Rachael 2.Loss Deckard confronts and kills Zora 2.Disaster Deckards shakes (?) 2.Despair (?) 3.Reversal Deckard tracks Priss down to Sebastian's apartment 3.Climax Deckard and Roy fight 3.New Deckard runs off with Rachael I'm not sure this format exactly works with this theme as there's no real inciting event, disaster or despair scenario for Deckard but there are many aspects that fit. It should also be noted that Roy Batty could be considered the main character. It could be considered that the first act of Roy's journey is only eluded to with the rest of the movie being Roy's journey. Roy Batty's journey 1.Intro Leon is interviewed and kills Blade Runner 1.Normal (?) 1.Event (?) 1.Call (?) 1.Refusal (?) 2.Confidence Escape or confrontation with Hannibal Chew 2.Subplot Priss and Roy make friends with Sebastian 2.Loss Deckard confronts Zora 2.Disaster Zora and Leon are murdered 2.Despair Roy and Priss despair (\"Then we're stupid and we'll die...\") 3.Reversal Roy uses Sebastian to get a meeting with Tyrell 3.Climax Deckard and Roy fight 3.New Roy dies It seems that Loss and Disaster go hand in hand so maybe they shouldn't really be differentiated. My Fair Lady (movie) 1.Intro High society leaving opera 1.Normal Eliza Doolitle selling flowers 1.Event Henry and Eliza meet outside of the Opera house 1.Call Eliza comes to Henry's house and asks for lessons 1.Refusal Henry initially refuses but accepts after Hugh offers a wager to Henry to teach Eliza 2.Confidence Eliza passes at the ball 2.Subplot Freddie is smitten with Eliza and is set up as a love interest 2.Loss Eliza is worried about what she'll do now that the experiment and bet is over 2.Disaster Eliza leaves 2.Despair Eliza and Henry are distrought over their separation 3.Reversal Eliza is accepted by others (Pickering, Freddie and Henry's mother) 3.Climax Eliza and Henry negotiate. Eliza leaves for Freddie 3.New Henry misses Eliza and then she returns I'm not sure how well this movie follows the format. One interpretation is that the horse race is the start of act 2 instead of the ball but it could be thought of as the horse race being a small sub act 1 stuffed at the end of act 1. It's also not clear what the reversal is or if it can be cleanly separated. Maybe it's that there are really two main characters, Henry and Eliza, where Henry's initial journey is to raise Eliza up and morphs into a fight to keep Eliza whereas Eliza's journey is to improve her life. Notes Romantic Comedies (City Formula) Supporting characters to love interest are not viable options for dating (married, gay, etc.) External force brings pair to reluctantly work together Initial refusal of emotional attachment until (usually silent) acceptance that they feel for each other Conflict happens where usually one party makes a mis-step and drives the love interest apart, often as a direct result of the external force that brought them together in the first place putting pressure on them \"Defeat\" stage where characters wallow in their loss \"Realization\" where the character that made the mis-step comes to their senses, apologizes, makes it right and asks for forgiveness Acceptance where characters end up together 2019-01-06","title":"Story Motifs"},{"location":"Story-Motifs.html#story-motifs","text":"Act Name Description 1 Intro (optional) An introductory sequence that often doesn't have anything to do with the main adventure directly to set tone or pacing of movie 1 Normal World Introduction to the status quo and the world the main character lives in 1 Inciting Event Event that triggers journey 1 Call to Adventure The hero is called on to start an adventure 1 Initial Refusal and Reluctant Acceptance Hero initially refuses teh call but eventually capitulates 2 Gaining Confidence The hero starts to discover their new abilities 2 Subplot A sub plot is started that does not directly impact the adventure but complements the hero or adventure 2 A Battle Lost The hero loses a battle without losing the war 2 Disaster Things progressively get worse 2 Despair The goal looks hopeless 3 Reversal Something happens to make the goal within reach (with the path often coming from the Subplot ) 3 Climax The final battle is fought and won or lost depending on whether the story is a tragedy or not 3 New Normal The new normal is established, either by a successful or unsuccessful campaign of the hero, with the world being better or worse","title":"Story Motifs"},{"location":"Story-Motifs.html#examples","text":"","title":"Examples"},{"location":"Story-Motifs.html#matrix","text":"1.Intro Agents attack on Trinity and her narrow escape 1.Normal Neo in front of his computer and later going to his day job 1.Event Agents come to interrogate Neo at his work and ensuing chase and capture 1.Call Neo is told he's \"the one\" 1.Refusal But Neo doesn't believe it... 2.Confidence Neo learns Kung Fu 2.Subplot Oracle tells Neo he needs to make a choice, Trinity as a love interest (via Oracle), Cypher is a spy 2.Loss Agents attack the group 2.Disaster Morphius is taken, most of the crew is murdered 2.Despair Morphius is captured and being interregated 3.Reversal Neo and Trinity decide to attack 3.Climax Neo defeats agents 3.New Neo leaves a message for the computers Not all fits neatly into this structure and there's a lot of variation, including how the subplots weave into the main hero's journely and adventure (which maybe is why Matrix is so well written). There might be some disagreement over what the sub plot is (or even if there's just one) and how it feeds into the reversal. Also, a lot of the events are not done in order. It seems like the basic story structure is still valid, though.","title":"Matrix"},{"location":"Story-Motifs.html#blade-runner","text":"1.Intro Leon is interviewed and kills Blade Runner 1.Normal Deckard ordering noodles 1.Event Gaaf calls on Deckard to come to the police station (?) 1.Call Deckard is asked to hunt replicants 1.Refusal Deckard initially refuses but accepts after being threatened by police cheif and hearing about the attack on the other Blade Runner 2.Confidence Deckard interviews Rachael and discovers she's a replicant 2.Subplot Deckards romance with Rachael 2.Loss Deckard confronts and kills Zora 2.Disaster Deckards shakes (?) 2.Despair (?) 3.Reversal Deckard tracks Priss down to Sebastian's apartment 3.Climax Deckard and Roy fight 3.New Deckard runs off with Rachael I'm not sure this format exactly works with this theme as there's no real inciting event, disaster or despair scenario for Deckard but there are many aspects that fit. It should also be noted that Roy Batty could be considered the main character. It could be considered that the first act of Roy's journey is only eluded to with the rest of the movie being Roy's journey. Roy Batty's journey 1.Intro Leon is interviewed and kills Blade Runner 1.Normal (?) 1.Event (?) 1.Call (?) 1.Refusal (?) 2.Confidence Escape or confrontation with Hannibal Chew 2.Subplot Priss and Roy make friends with Sebastian 2.Loss Deckard confronts Zora 2.Disaster Zora and Leon are murdered 2.Despair Roy and Priss despair (\"Then we're stupid and we'll die...\") 3.Reversal Roy uses Sebastian to get a meeting with Tyrell 3.Climax Deckard and Roy fight 3.New Roy dies It seems that Loss and Disaster go hand in hand so maybe they shouldn't really be differentiated.","title":"Blade Runner"},{"location":"Story-Motifs.html#my-fair-lady-movie","text":"1.Intro High society leaving opera 1.Normal Eliza Doolitle selling flowers 1.Event Henry and Eliza meet outside of the Opera house 1.Call Eliza comes to Henry's house and asks for lessons 1.Refusal Henry initially refuses but accepts after Hugh offers a wager to Henry to teach Eliza 2.Confidence Eliza passes at the ball 2.Subplot Freddie is smitten with Eliza and is set up as a love interest 2.Loss Eliza is worried about what she'll do now that the experiment and bet is over 2.Disaster Eliza leaves 2.Despair Eliza and Henry are distrought over their separation 3.Reversal Eliza is accepted by others (Pickering, Freddie and Henry's mother) 3.Climax Eliza and Henry negotiate. Eliza leaves for Freddie 3.New Henry misses Eliza and then she returns I'm not sure how well this movie follows the format. One interpretation is that the horse race is the start of act 2 instead of the ball but it could be thought of as the horse race being a small sub act 1 stuffed at the end of act 1. It's also not clear what the reversal is or if it can be cleanly separated. Maybe it's that there are really two main characters, Henry and Eliza, where Henry's initial journey is to raise Eliza up and morphs into a fight to keep Eliza whereas Eliza's journey is to improve her life.","title":"My Fair Lady (movie)"},{"location":"Story-Motifs.html#notes","text":"","title":"Notes"},{"location":"Story-Motifs.html#romantic-comedies-city-formula","text":"Supporting characters to love interest are not viable options for dating (married, gay, etc.) External force brings pair to reluctantly work together Initial refusal of emotional attachment until (usually silent) acceptance that they feel for each other Conflict happens where usually one party makes a mis-step and drives the love interest apart, often as a direct result of the external force that brought them together in the first place putting pressure on them \"Defeat\" stage where characters wallow in their loss \"Realization\" where the character that made the mis-step comes to their senses, apologizes, makes it right and asks for forgiveness Acceptance where characters end up together","title":"Romantic Comedies (City Formula)"},{"location":"Story-Motifs.html#2019-01-06","text":"","title":"2019-01-06"},{"location":"Sum-of-Square-Roots.html","text":"Sum of Square Roots Notes The sum of square roots problem is: $$ \\begin{align} n, m \\in \\mathbb{Z} \\ a_j, b_k \\in \\mathbb{Z} \\ \\sum_j^{n-1} \\sqrt a_j =? \\sum_k^{m-1} \\sqrt b_k \\end{align} $$ In other words, is checking the sum of square roots in NP? $$ u \\in \\mathbb{Z} $$ $$ \\begin{align} \\frac{\\int_{-\\pi}^{\\pi} e^{i u \\theta} d\\theta}{2 \\pi} = \\begin{cases} 1 & u \\ne 0 \\ 0 & u = 0 \\end{cases} \\end{align} $$ For $ a_k \\in \\mathbb{Z} $, $$ \\begin{align} \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} \\prod_k^{n-1} ( e^{-a_k i \\theta} + e^{a_k i \\theta} ) d\\theta = \\begin{cases} 0 & \\text{no partition} \\ !0 & \\text{at least one partition} \\end{cases} \\end{align} $$ The irrationality measure is defined to be $$ $$","title":"Sum of Square Roots"},{"location":"Sum-of-Square-Roots.html#sum-of-square-roots-notes","text":"The sum of square roots problem is: $$ \\begin{align} n, m \\in \\mathbb{Z} \\ a_j, b_k \\in \\mathbb{Z} \\ \\sum_j^{n-1} \\sqrt a_j =? \\sum_k^{m-1} \\sqrt b_k \\end{align} $$ In other words, is checking the sum of square roots in NP? $$ u \\in \\mathbb{Z} $$ $$ \\begin{align} \\frac{\\int_{-\\pi}^{\\pi} e^{i u \\theta} d\\theta}{2 \\pi} = \\begin{cases} 1 & u \\ne 0 \\ 0 & u = 0 \\end{cases} \\end{align} $$ For $ a_k \\in \\mathbb{Z} $, $$ \\begin{align} \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} \\prod_k^{n-1} ( e^{-a_k i \\theta} + e^{a_k i \\theta} ) d\\theta = \\begin{cases} 0 & \\text{no partition} \\ !0 & \\text{at least one partition} \\end{cases} \\end{align} $$ The irrationality measure is defined to be $$ $$","title":"Sum of Square Roots Notes"},{"location":"TEMPLATE.html","text":"TITLE [TEXT] YYYY-MM-DD","title":"TEMPLATE"},{"location":"TEMPLATE.html#title","text":"[TEXT]","title":"TITLE"},{"location":"TEMPLATE.html#yyyy-mm-dd","text":"","title":"YYYY-MM-DD"},{"location":"Textile-Cheat-Sheet.html","text":"Textile Cheat Sheet Header (h1) h1. Header (h2) h2. Header (h3) h3. link (aligned) @\"text\":http://url@, @!>pic.png!@ picture link @!http://url!@ code block <pre> </pre> bold **bold** bold *bold* italic _italic_ literal @literal@ paragraph alignment @p<.@, @p=.@, @p>.@ paragraph indent @p(.@, @p((.@, ..., @p().@, ..., @p)).@, @p).@ dot list @*@, @**@, ... numbered list @#@, @##@, ... block quote @bq.@ Tables |_. attribute list |_. ... | | val | ue | |\\2. spans two cols | | col 1 | col 2 | |/3. spans 3 rows | a | | b | | c | References Textile Quick Reference A Textile Reference 2015-10-20","title":"Textile Cheat Sheet"},{"location":"Textile-Cheat-Sheet.html#textile-cheat-sheet","text":"Header (h1) h1. Header (h2) h2. Header (h3) h3. link (aligned) @\"text\":http://url@, @!>pic.png!@ picture link @!http://url!@ code block <pre> </pre> bold **bold** bold *bold* italic _italic_ literal @literal@ paragraph alignment @p<.@, @p=.@, @p>.@ paragraph indent @p(.@, @p((.@, ..., @p().@, ..., @p)).@, @p).@ dot list @*@, @**@, ... numbered list @#@, @##@, ... block quote @bq.@","title":"Textile Cheat Sheet"},{"location":"Textile-Cheat-Sheet.html#tables","text":"|_. attribute list |_. ... | | val | ue | |\\2. spans two cols | | col 1 | col 2 | |/3. spans 3 rows | a | | b | | c |","title":"Tables"},{"location":"Textile-Cheat-Sheet.html#references","text":"Textile Quick Reference A Textile Reference","title":"References"},{"location":"Textile-Cheat-Sheet.html#2015-10-20","text":"","title":"2015-10-20"},{"location":"Unix-y-notes.html","text":"Remove highlights from less search ESC u Jump to line in less ng - Jump to line n relative to top of file nG - Jump to line n relative to bottom of file Gzip without timestamps By default, gzip keeps timestamp information for the file you're compressing. This is problematic when you want reproducibility. $ gzip -n inp.txt This will create a file inp.txt.gz that, when uncompressed, will create a file with the current system timestamp. As far as I know, bgzip does not keep file timestamp information. Read lines in a bash script http://stackoverflow.com/questions/10929453/bash-scripting-read-file-line-by-line #!/bin/bash while IFS='' read -r line || [[ -n \"$line\" ]]; do echo $line done < <( echo -e \"this small script\\nreads multiple\\n lines\" ) IFS='' (or IFS=) prevents leading/trailing whitespace from being trimmed. -r prevents backslash escapes from being interpreted. || [[ -n $line ]] prevents the last line from being ignored if it doesn't end with a \\n (since read returns a non-zero exit code when it encounters EOF). sometimes you can just get away with this: #!/bin/bash while read line ; do echo $line done < <( echo -e \"this small script\\nreads multiple\\n lines\" ) Diff two streams $ diff <( echo -e \"stream\\na\" ) <( echo -e \"stream\\nb\" ) 2c2 < a --- > b Find all files ending in .md and do an ls -l $ find . -type f -name '*.md' -exec ls -l {} \\; Put pairs of lines on their own line $ echo -e 'a\\nb\\nc\\nd\\ne\\nf' | paste - - | tr '\\t' ' ' a b c d e f Differences, overlaps in two files $ comm <( echo -e 'c\\na\\nb\\nd' | sort ) <( echo -e 'e\\nb\\nd' | sort ) a b c d e Print formatted columns $ echo -e \"column_0\\tcol1\\na\\tbbbb\\n\" column_0 col1 a bbbb $ echo -e \"column_0\\tcol1\\na\\tbbbb\\n\" | column -t column_0 col1 a bbbb bgzip $ bgzip -i inp.txt $ bgzip --stdout --offset 100 --size 32 inp.txt.gz parallel #!/bin/bash function process { z=$1 time ( echo sleeping $z && sleep $z && echo waking up \"($z)\" ) } export -f process time echo -e '1\\n3\\n4' | parallel --max-procs 2 process {} sleeping 1 waking up (1) real 0m1.002s user 0m0.000s sys 0m0.000s sleeping 3 waking up (3) real 0m3.002s user 0m0.000s sys 0m0.000s sleeping 4 waking up (4) real 0m4.003s user 0m0.000s sys 0m0.000s real 0m5.603s user 0m0.140s sys 0m0.072s xargs (parallel) #!/bin/bash function process { z=$1 time ( echo sleeping $z && sleep $z && echo waking up \"($z)\" ) } export -f process time echo -e '1\\n3\\n4' | xargs -n 1 -P 2 -I{} bash -c 'process {}' sleeping 1 sleeping 3 waking up (1) real 0m1.003s user 0m0.000s sys 0m0.000s sleeping 4 waking up (3) real 0m3.002s user 0m0.000s sys 0m0.000s waking up (4) real 0m4.003s user 0m0.000s sys 0m0.000s real 0m5.019s user 0m0.000s sys 0m0.000s sort on multiple fields $ echo -e \"5,cats,meow\\n7,cute,mew\\n2,cats,mewmew\\n10,cats,meowmeowmeow\\n2,cute,4you\\n8,cute,4ever\" 5,cats,meow 7,cute,mew 2,cats,mewmew 10,cats,meowmeowmeow 2,cute,4you 8,cute,4ever $ echo -e \"5,cats,meow\\n7,cute,mew\\n2,cats,mewmew\\n10,cats,meowmeowmeow\\n2,cute,4you\\n8,cute,4ever\" | \\ sort -k2,2 -k1,1nr -t, 10,cats,meowmeowmeow 5,cats,meow 2,cats,mewmew 8,cute,4ever 7,cute,mew 2,cute,4you -t field -k<start>,<stop><opt> key start and stop position along with opt (in the above n for numeric, r for reverse) 2017-05-14","title":"Unix y notes"},{"location":"Unix-y-notes.html#remove-highlights-from-less-search","text":"ESC u","title":"Remove highlights from less search"},{"location":"Unix-y-notes.html#jump-to-line-in-less","text":"ng - Jump to line n relative to top of file nG - Jump to line n relative to bottom of file","title":"Jump to line in less"},{"location":"Unix-y-notes.html#gzip-without-timestamps","text":"By default, gzip keeps timestamp information for the file you're compressing. This is problematic when you want reproducibility. $ gzip -n inp.txt This will create a file inp.txt.gz that, when uncompressed, will create a file with the current system timestamp. As far as I know, bgzip does not keep file timestamp information.","title":"Gzip without timestamps"},{"location":"Unix-y-notes.html#read-lines-in-a-bash-script","text":"http://stackoverflow.com/questions/10929453/bash-scripting-read-file-line-by-line #!/bin/bash while IFS='' read -r line || [[ -n \"$line\" ]]; do echo $line done < <( echo -e \"this small script\\nreads multiple\\n lines\" ) IFS='' (or IFS=) prevents leading/trailing whitespace from being trimmed. -r prevents backslash escapes from being interpreted. || [[ -n $line ]] prevents the last line from being ignored if it doesn't end with a \\n (since read returns a non-zero exit code when it encounters EOF). sometimes you can just get away with this: #!/bin/bash while read line ; do echo $line done < <( echo -e \"this small script\\nreads multiple\\n lines\" )","title":"Read lines in a bash script"},{"location":"Unix-y-notes.html#diff-two-streams","text":"$ diff <( echo -e \"stream\\na\" ) <( echo -e \"stream\\nb\" ) 2c2 < a --- > b","title":"Diff two streams"},{"location":"Unix-y-notes.html#find-all-files-ending-in-md-and-do-an-ls-l","text":"$ find . -type f -name '*.md' -exec ls -l {} \\;","title":"Find all files ending in .md and do an ls -l"},{"location":"Unix-y-notes.html#put-pairs-of-lines-on-their-own-line","text":"$ echo -e 'a\\nb\\nc\\nd\\ne\\nf' | paste - - | tr '\\t' ' ' a b c d e f","title":"Put pairs of lines on their own line"},{"location":"Unix-y-notes.html#differences-overlaps-in-two-files","text":"$ comm <( echo -e 'c\\na\\nb\\nd' | sort ) <( echo -e 'e\\nb\\nd' | sort ) a b c d e","title":"Differences, overlaps in two files"},{"location":"Unix-y-notes.html#print-formatted-columns","text":"$ echo -e \"column_0\\tcol1\\na\\tbbbb\\n\" column_0 col1 a bbbb $ echo -e \"column_0\\tcol1\\na\\tbbbb\\n\" | column -t column_0 col1 a bbbb","title":"Print formatted columns"},{"location":"Unix-y-notes.html#bgzip","text":"$ bgzip -i inp.txt $ bgzip --stdout --offset 100 --size 32 inp.txt.gz","title":"bgzip"},{"location":"Unix-y-notes.html#parallel","text":"#!/bin/bash function process { z=$1 time ( echo sleeping $z && sleep $z && echo waking up \"($z)\" ) } export -f process time echo -e '1\\n3\\n4' | parallel --max-procs 2 process {} sleeping 1 waking up (1) real 0m1.002s user 0m0.000s sys 0m0.000s sleeping 3 waking up (3) real 0m3.002s user 0m0.000s sys 0m0.000s sleeping 4 waking up (4) real 0m4.003s user 0m0.000s sys 0m0.000s real 0m5.603s user 0m0.140s sys 0m0.072s","title":"parallel"},{"location":"Unix-y-notes.html#xargs-parallel","text":"#!/bin/bash function process { z=$1 time ( echo sleeping $z && sleep $z && echo waking up \"($z)\" ) } export -f process time echo -e '1\\n3\\n4' | xargs -n 1 -P 2 -I{} bash -c 'process {}' sleeping 1 sleeping 3 waking up (1) real 0m1.003s user 0m0.000s sys 0m0.000s sleeping 4 waking up (3) real 0m3.002s user 0m0.000s sys 0m0.000s waking up (4) real 0m4.003s user 0m0.000s sys 0m0.000s real 0m5.019s user 0m0.000s sys 0m0.000s","title":"xargs (parallel)"},{"location":"Unix-y-notes.html#sort-on-multiple-fields","text":"$ echo -e \"5,cats,meow\\n7,cute,mew\\n2,cats,mewmew\\n10,cats,meowmeowmeow\\n2,cute,4you\\n8,cute,4ever\" 5,cats,meow 7,cute,mew 2,cats,mewmew 10,cats,meowmeowmeow 2,cute,4you 8,cute,4ever $ echo -e \"5,cats,meow\\n7,cute,mew\\n2,cats,mewmew\\n10,cats,meowmeowmeow\\n2,cute,4you\\n8,cute,4ever\" | \\ sort -k2,2 -k1,1nr -t, 10,cats,meowmeowmeow 5,cats,meow 2,cats,mewmew 8,cute,4ever 7,cute,mew 2,cute,4you -t field -k<start>,<stop><opt> key start and stop position along with opt (in the above n for numeric, r for reverse)","title":"sort on multiple fields"},{"location":"Unix-y-notes.html#2017-05-14","text":"","title":"2017-05-14"},{"location":"Useful-Tables.html","text":"Useful Tables Nato Phonetic Alphabet alfa bravo charlie delta echo foxtrot golf hotel india juliett kilo lima mike november oscar papa quebec romeo sierra tango uniform victor whisky xray yankee zulu Metric prefixes Name Symbol Base 10 Decimal quetta Q $10^{30}$ 1000000000000000000000000000000 ronna R $10^{27}$ 1000000000000000000000000000 yotta Y $10^{24}$ 1000000000000000000000000 zetta Z $10^{21}$ 1000000000000000000000 exa E $10^{18}$ 1000000000000000000 peta P $10^{15}$ 1000000000000000 tera T $10^{12}$ 1000000000000 giga G $10^9$ 1000000000 mega M $10^6$ 1000000 kilo k $10^3$ 1000 hecto h $10^2$ 100 deca da $10^1$ 10 $10^0$ 1 deci d $10^{\u22121}$ 0.1 centi c $10^{\u22122}$ 0.01 milli m $10^{\u22123}$ 0.001 micro $\\mu$ $10^{\u22126}$ 0.000001 nano n $10^{\u22129}$ 0.000000001 pico p $10^{\u221212}$ 0.000000000001 femto f $10^{\u221215}$ 0.000000000000001 atto a $10^{\u221218}$ 0.000000000000000001 zepto z $10^{\u221221}$ 0.000000000000000000001 yocto y $10^{\u221224}$ 0.000000000000000000000001 ronto r $10^{\u221227}$ 0.000000000000000000000000001 quecto q $10^{\u221230}$ 0.000000000000000000000000000001 Electromagnetic Spectrum 2022-08-09","title":"Useful Tables"},{"location":"Useful-Tables.html#useful-tables","text":"","title":"Useful Tables"},{"location":"Useful-Tables.html#nato-phonetic-alphabet","text":"alfa bravo charlie delta echo foxtrot golf hotel india juliett kilo lima mike november oscar papa quebec romeo sierra tango uniform victor whisky xray yankee zulu","title":"Nato Phonetic Alphabet"},{"location":"Useful-Tables.html#metric-prefixes","text":"Name Symbol Base 10 Decimal quetta Q $10^{30}$ 1000000000000000000000000000000 ronna R $10^{27}$ 1000000000000000000000000000 yotta Y $10^{24}$ 1000000000000000000000000 zetta Z $10^{21}$ 1000000000000000000000 exa E $10^{18}$ 1000000000000000000 peta P $10^{15}$ 1000000000000000 tera T $10^{12}$ 1000000000000 giga G $10^9$ 1000000000 mega M $10^6$ 1000000 kilo k $10^3$ 1000 hecto h $10^2$ 100 deca da $10^1$ 10 $10^0$ 1 deci d $10^{\u22121}$ 0.1 centi c $10^{\u22122}$ 0.01 milli m $10^{\u22123}$ 0.001 micro $\\mu$ $10^{\u22126}$ 0.000001 nano n $10^{\u22129}$ 0.000000001 pico p $10^{\u221212}$ 0.000000000001 femto f $10^{\u221215}$ 0.000000000000001 atto a $10^{\u221218}$ 0.000000000000000001 zepto z $10^{\u221221}$ 0.000000000000000000001 yocto y $10^{\u221224}$ 0.000000000000000000000001 ronto r $10^{\u221227}$ 0.000000000000000000000000001 quecto q $10^{\u221230}$ 0.000000000000000000000000000001","title":"Metric prefixes"},{"location":"Useful-Tables.html#electromagnetic-spectrum","text":"","title":"Electromagnetic Spectrum"},{"location":"Useful-Tables.html#2022-08-09","text":"","title":"2022-08-09"},{"location":"World-Economy.html","text":"World Economy Market capitalization is the outstanding shares times the share price. As of 2014, the estimated total market capitalization of the world's domestic companies is in the $64 trillion range. (64 * 10^12). As of today (Dec. 3, 2017) 16,718,425 BTC * $11,404.50 = 190665277912.50 or around $190B. For comparison, Amazon has a market capitalization now of $560B. If we assume Bitcoin grows to 10% of the total global market capitalization, that would put it at about $5T. Assuming all 21M bitcoins get mined, that puts the total worth of each Bitcoin at $5T / 21M or approximately $238k/BTC . From the same world bank chart as above, the USA is at around $27T with China at second with $7T, Japan at third with $4.9T. This means the USA is 3x more than the next highest and around 8x the third highest. 2017-12-03","title":"World Economy"},{"location":"World-Economy.html#world-economy","text":"Market capitalization is the outstanding shares times the share price. As of 2014, the estimated total market capitalization of the world's domestic companies is in the $64 trillion range. (64 * 10^12). As of today (Dec. 3, 2017) 16,718,425 BTC * $11,404.50 = 190665277912.50 or around $190B. For comparison, Amazon has a market capitalization now of $560B. If we assume Bitcoin grows to 10% of the total global market capitalization, that would put it at about $5T. Assuming all 21M bitcoins get mined, that puts the total worth of each Bitcoin at $5T / 21M or approximately $238k/BTC . From the same world bank chart as above, the USA is at around $27T with China at second with $7T, Japan at third with $4.9T. This means the USA is 3x more than the next highest and around 8x the third highest.","title":"World Economy"},{"location":"World-Economy.html#2017-12-03","text":"","title":"2017-12-03"},{"location":"YouTube-Storage.html","text":"YouTube Storage Estimates Average video length 11.7 minutes ( statista ) About 13.325 B videos ( ref ) Average video quality is 1280x720p ( ref ) Average of 105Mb per minute of video for 1280x720p video ( ref ) This gives us an estimate of: $$ \\begin{array}{ll} & (11.7 \\text{ min}) \\cdot (13.325 \\cdot 10^9 \\text{ vid}) \\cdot ( 105 \\frac{ \\text{Mb} }{ \\text{ vid} \\cdot \\text{ min} }) \\ = & 16,369,762,500,000 \\text{ Mb} \\ \\approx & 15 \\cdot 2^{40} \\text{ Mb} \\ = & 15 \\text{ Exabyte} \\end{array} $$ 2024-01-09","title":"YouTube Storage"},{"location":"YouTube-Storage.html#youtube-storage-estimates","text":"Average video length 11.7 minutes ( statista ) About 13.325 B videos ( ref ) Average video quality is 1280x720p ( ref ) Average of 105Mb per minute of video for 1280x720p video ( ref ) This gives us an estimate of: $$ \\begin{array}{ll} & (11.7 \\text{ min}) \\cdot (13.325 \\cdot 10^9 \\text{ vid}) \\cdot ( 105 \\frac{ \\text{Mb} }{ \\text{ vid} \\cdot \\text{ min} }) \\ = & 16,369,762,500,000 \\text{ Mb} \\ \\approx & 15 \\cdot 2^{40} \\text{ Mb} \\ = & 15 \\text{ Exabyte} \\end{array} $$","title":"YouTube Storage Estimates"},{"location":"YouTube-Storage.html#2024-01-09","text":"","title":"2024-01-09"},{"location":"ffmpeg-notes.html","text":"Video Cropping -qscale sets the compression level (?) (higher is more compression). crop is width, height, start x, start y. ffmpeg -i inp.mp4 -qscale 10 -filter:v \"crop=in_w:in_h-63:0:63\" out.mp4 2015-11-05","title":"Ffmpeg notes"},{"location":"ffmpeg-notes.html#video-cropping","text":"-qscale sets the compression level (?) (higher is more compression). crop is width, height, start x, start y. ffmpeg -i inp.mp4 -qscale 10 -filter:v \"crop=in_w:in_h-63:0:63\" out.mp4","title":"Video Cropping"},{"location":"ffmpeg-notes.html#2015-11-05","text":"","title":"2015-11-05"},{"location":"home.html","text":"dev Textile Cheat Sheet 2015-10-20 Image Resize 2015-10-27 Screenshots Screencasts Animated Gifs 2015-11-01 ffmpeg notes 2015-11-05 Unix-y notes 2017-05-14 lattice reduction 2015-11-30 GCode Conversion 2016-09-19 Git Rename Master 2016-09-21 MkDocs Quickstart 2016-09-28 GPG Notes 2017-02-09 Git Notes 2017-02-10 Shannon Entropy 2017-06-12 Enabling Server HTTPS 2017-06-12 BGZF Example 2017-06-12 C Project Template 2017-08-05 Project Organization 2017-08-05 Kelly Criterion 2017-08-18 File Naming Conventions 2017-08-18 Command Line Option Loose Standard 2017-12-25 PCB Notes 2018-02-03 Coding Style 2018-02-17 Energy Consumption Stats 2017-09-21 Simple Sum 2018-05-25 Fisher Yates Shuffle 2018-06-13 Halting Problem 2018-06-13 Assorted Small Probability Problems 2018-06-29 Probability Notes 2018-08-04 Amdahls Law 2018-09-03 Is It Really Open 2019-01-10 Number Theory Notes 2019-01-10 Arbitrary Binary Functions 2019-01-10 Empirical Laws 2019-03-18 Food CO2 Water 2019-04-02 SSH Recipes 2019-09-24 Future Predictions 2021-04-27 Diophantine Approximation 2020-05-22 GCode Common 2020-09-09 Misc Math 2021-08-30 Socio Economic Definitions 2020-11-02 Bitcoin Moon Math 2017-12-06 Energy Discussion 2021-04-24 AWK Cheatsheet 2021-10-01 Littlewood Polynomials Notes 2022-01-20 Useful Tables 2022-08-09 Belief Propagation 2022-08-16 Probability Defintions 2023-03-24 Ribbon Tile Puzzle (spoilers) 2023-04-02 Kullback-Leibler Divergence 2023-08-22 Statistical Mechanics for Computer Scientists 2023-11-12","title":"Home"},{"location":"home.html#dev","text":"Textile Cheat Sheet 2015-10-20 Image Resize 2015-10-27 Screenshots Screencasts Animated Gifs 2015-11-01 ffmpeg notes 2015-11-05 Unix-y notes 2017-05-14 lattice reduction 2015-11-30 GCode Conversion 2016-09-19 Git Rename Master 2016-09-21 MkDocs Quickstart 2016-09-28 GPG Notes 2017-02-09 Git Notes 2017-02-10 Shannon Entropy 2017-06-12 Enabling Server HTTPS 2017-06-12 BGZF Example 2017-06-12 C Project Template 2017-08-05 Project Organization 2017-08-05 Kelly Criterion 2017-08-18 File Naming Conventions 2017-08-18 Command Line Option Loose Standard 2017-12-25 PCB Notes 2018-02-03 Coding Style 2018-02-17 Energy Consumption Stats 2017-09-21 Simple Sum 2018-05-25 Fisher Yates Shuffle 2018-06-13 Halting Problem 2018-06-13 Assorted Small Probability Problems 2018-06-29 Probability Notes 2018-08-04 Amdahls Law 2018-09-03 Is It Really Open 2019-01-10 Number Theory Notes 2019-01-10 Arbitrary Binary Functions 2019-01-10 Empirical Laws 2019-03-18 Food CO2 Water 2019-04-02 SSH Recipes 2019-09-24 Future Predictions 2021-04-27 Diophantine Approximation 2020-05-22 GCode Common 2020-09-09 Misc Math 2021-08-30 Socio Economic Definitions 2020-11-02 Bitcoin Moon Math 2017-12-06 Energy Discussion 2021-04-24 AWK Cheatsheet 2021-10-01 Littlewood Polynomials Notes 2022-01-20 Useful Tables 2022-08-09 Belief Propagation 2022-08-16 Probability Defintions 2023-03-24 Ribbon Tile Puzzle (spoilers) 2023-04-02 Kullback-Leibler Divergence 2023-08-22 Statistical Mechanics for Computer Scientists 2023-11-12","title":"dev"},{"location":"lattice-reduction.html","text":"Lattice Reduction There seem to be two main methods of using lattice reduction techniques for other norms. The first looks to be to use a linear programming step in place of the 'weak reduction' step (the Gram-Schmidt reduction step). The second is embedding the base in a higher dimension with extra structure and giving bounds on how far it is from the different norm. References Other norms for Lattice reduction techniques (LLL, PSLQ)? Other norms for Lattice reduction techniques (LLL, PSLQ)? \"The generalized basis reduction algorithm\" by Laszlo Lovasz and Herbert Scarf \"Lattice Problems and Norm Embeddings\" by Oded Regev and Ricky Rosen \"Limits on the Hardness of Lattice Problems in lp Norms\" by Chris Peikert \"Lattice Basis Reduction in Infinity Norm\" by Vanya Ivanova (Bachelor Thesis) 2015-11-30","title":"Lattice reduction"},{"location":"lattice-reduction.html#lattice-reduction","text":"There seem to be two main methods of using lattice reduction techniques for other norms. The first looks to be to use a linear programming step in place of the 'weak reduction' step (the Gram-Schmidt reduction step). The second is embedding the base in a higher dimension with extra structure and giving bounds on how far it is from the different norm.","title":"Lattice Reduction"},{"location":"lattice-reduction.html#references","text":"Other norms for Lattice reduction techniques (LLL, PSLQ)? Other norms for Lattice reduction techniques (LLL, PSLQ)? \"The generalized basis reduction algorithm\" by Laszlo Lovasz and Herbert Scarf \"Lattice Problems and Norm Embeddings\" by Oded Regev and Ricky Rosen \"Limits on the Hardness of Lattice Problems in lp Norms\" by Chris Peikert \"Lattice Basis Reduction in Infinity Norm\" by Vanya Ivanova (Bachelor Thesis)","title":"References"},{"location":"lattice-reduction.html#2015-11-30","text":"","title":"2015-11-30"},{"location":"data/index.html","text":"Data Sources Gini Index World Bank, Development Research Group ( link )","title":"Index"},{"location":"data/index.html#data-sources","text":"","title":"Data Sources"},{"location":"data/index.html#gini-index","text":"World Bank, Development Research Group ( link )","title":"Gini Index"},{"location":"papers/index.html","text":"Papers","title":"Index"},{"location":"papers/index.html#papers","text":"","title":"Papers"}]}