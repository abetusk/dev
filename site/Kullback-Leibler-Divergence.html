<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>dev blog</title>

    <link rel="stylesheet" href="/dev/css/styles.css">
    <link rel="stylesheet" href="/dev/css/pygment_trac.css">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js", "TeX/AMSsymbols.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
      });
    </script>
    <script type="text/javascript" src="/dev/js/MathJax.js"></script>
    <!-- <script type="text/javascript" src="/dev/js/MathJax.js?config=TeX-AMS_HTML""></script> -->

    <script type="text/javascript" src="/dev/js/audience-minutes.js"></script>

    <meta name="viewport" content="width=device-width">
  </head>
  <body>

    <div class="wrapper">
      <header>
        <h1>Dev Blog</h1>

        <table>
          <body>
            <tr><td><a href="/dev/">./dev</a></td></tr>
           <tr>
              <td>
                <br /> <br /> <br />
                <p><small> Original theme by <a href="https://github.com/orderedlist">orderedlist</a> (CC-BY-SA)</small></p>

                <br />
                <p>
                Where applicable, all content is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC-BY-SA</a>.
                <br />

                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="/dev/img/cc-by-sa-80x15.png" /></a>
                </p>
              </td>
            </tr>
          </body>
        </table>

      </header>
      <section>

        <h1 id="kullback-leibler-divergence">Kullback-Leibler Divergence</h1>
<p>Here is a short discussion of the
<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a> that
helps give some motivation for what it is an where it comes from.</p>
<p>Given two (discrete) probability distribution functions, $p(\cdot)$ and $q(\cdot)$, the Kullback-Leibler divergence
is defined as:</p>
<p>$$
D_{KL}(p || q) = - \sum_{x} p(x) \ln(\frac{p(x)}{q(x)})
$$</p>
<p>The Kullback-Leibler divergence, in some intuitive sense, measures the difference
between probability distributions.</p>
<p>As motivation, consider a situation where you are providing internet service to
a business.
This business wants to compress their data but has, incorrectly, guessed at
a distribution $q(x)$ for the symbols it's trying to transmit.</p>
<p>If you've determined the true distribution to be $p(x)$ the question is, how much
is the gain you get by, say, charging the business for compressing their data with
the probability distribution $q(x)$ when you have knowledge and can use the true
distribution $p(x)$.</p>
<p>The business will encode it's messages with an average of</p>
<p>$$
-\sum_{x} p(x) \lg(q(x))
$$</p>
<p>bits, where the $\lg(q(x))$ is the encoded bit size of the token $x$ but will only
happen with the true underlying distribution $p(x)$.
If you take the messages and re-encode them properly, the average bit size will
be:</p>
<p>$$
-\sum_{x} p(x) \lg(p(x))
$$</p>
<p>Taking the difference:</p>
<p>$$
\begin{array}{cl}
 &amp; -\sum_{x} p(x) \lg(p(x)) - [-\sum_{x} p(x) \lg(q(x) ] \\
 \to &amp; -\sum_{x} p(x) \lg( \frac{p(x)}{q(x)})
\end{array}
$$</p>
<p>On average, $D_{KL}(p || q)$ bits per token are saved.
If the business is willing to pay rates based on the bits they see going out, namely
$-\sum_{x} p(x) \lg(q(x))$, but
you're paying only for $-\sum_{x} p(x) \lg(p(x))$ bits going out, then there's potential profit on the difference
to be made.</p>
<hr />
<p>The <a href="https://en.wikipedia.org/wiki/Cross-entropy">cross entropy</a> is
a concept closely connected to the Kullback-Leibler distribution.</p>
<p>The cross entropy is defined as:</p>
<p>$$
H(p,q) = -\sum_{x} p(x) \lg(q(x))
$$</p>
<p>The Kullback-Leibler distribution can be seen as the difference of the entropy
of the distribution $p(\cdot)$ and the cross entropy of $p(\cdot)$ and $q(\cdot)$:</p>
<p>$$
\begin{array}{cl}
D_{KL} &amp; = H(p) - H(p,q) \\
 &amp; = -\sum_{x} p(x) \lg(\frac{p(x)}{q(x)})
\end{array}
$$</p>
<p>In the above example, the cross entropy is how many bits the business is transmitting
under their fallacious assumption of $q(\cdot)$ as the distribution and the entropy
of $p(\cdot)$ is the "optimized" bits you transmit with knowledge of the underlying
distribution.</p>
<hr />
<p>When given a "guess" distribution, $q(x)$, one can compare how "bad" it is compared with
a "true" or underlying distribution, $p(x)$.</p>
<h6 id="2023-08-22">2023-08-22</h6>

      </section>

      <!--
      <footer>
        <p><small> Original theme by <a href="https://github.com/orderedlist">orderedlist</a> (CC-BY-SA)</small></p>
      </footer>
      -->

    </div>
    <script src="/dev/js/scale.fix.js"></script>
  </body
</html>